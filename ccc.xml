This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/rules/codacy.mdc
.env.example
.gitignore
CHANGELOG.md
database/schema/schema.sql
docs/phase2/CCC-S2-API.md
docs/phase2/CCC-S2-ARCHITECTURE.md
docs/phase2/CCC-S2-IMPLEMENTATION.md
docs/phase2/CCC-S2-MASTER.md
docs/phase2/CCC-S2-TESTING.md
docs/phase2/CCC-S2-USER-GUIDE.md
docs/phase2/README.md
examples/phase2_usage.py
LICENSE
proxy_server.py
README.md
requirements.txt
resonant_loop_lab.html
src/__init__.py
src/memory/__init__.py
src/memory/database.py
src/models/__init__.py
src/models/memory_models.py
src/services/__init__.py
src/services/memory_service.py
src/utils/__init__.py
src/utils/causal_memory_core.py
src/utils/context_analyzer.py
src/utils/encryption.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/codacy.mdc">
---
    description: Configuration for AI behavior when interacting with Codacy's MCP Server
    globs: 
    alwaysApply: true
---
---
# Codacy Rules
Configuration for AI behavior when interacting with Codacy's MCP Server

## CRITICAL: After ANY successful `edit_file` or `reapply` operation
- YOU MUST IMMEDIATELY run the `codacy_cli_analyze` tool from Codacy's MCP Server for each file that was edited, with:
 - `rootPath`: set to the workspace path
 - `file`: set to the path of the edited file
 - `tool`: leave empty or unset
- If any issues are found in the new edits, propose and apply fixes for them.
- > NOTE: Failure to follow this rule is considered a critical error.

## When you tried to run the `codacy_cli_analyze` tool and the Codacy CLI is not installed
- Ask the user 'Codacy CLI is not installed. Would you like me to install it now?'
- If the user responds with "yes", run the `codacy_cli_install` tool and then continue with the original task
- If the user responds with "no", instruct the user that they can disable automatic analysis in the extension settings
- Wait for the user to respond before proceeding with any other actions

## After every response
- If you made any file edits in this conversation, verify you ran `codacy_cli_analyze` tool from Codacy's MCP Server 

## When there are no Codacy MCP Server tools available, or the MCP Server is not reachable
- Suggest the user the following troubleshooting steps:
 - Try to reset the MCP on the extension
 - If the user is using VSCode, suggest them to review their Copilot > MCP settings in Github, under their organization or personal account. Refer them to Settings > Copilot > Enable MCP servers in Copilot. Suggested URL (https://github.com/settings/copilot/features) or https://github.com/organizations/{organization-name}/settings/copilot/features (This can only be done by their organization admins / owners)
- If none of the above steps work, suggest the user to contact Codacy support

## Trying to call a tool that needs a rootPath as a parameter
- Always use the standard, non-URL-encoded file system path

## CRITICAL: Dependencies and Security Checks
- IMMEDIATELY after ANY of these actions:
 - Running npm/yarn/pnpm install
 - Adding dependencies to package.json
 - Adding requirements to requirements.txt
 - Adding dependencies to pom.xml
 - Adding dependencies to build.gradle
 - Any other package manager operations
- You MUST run the `codacy_cli_analyze` tool with:
 - `rootPath`: set to the workspace path
 - `tool`: set to "trivy"
 - `file`: leave empty or unset
- If any vulnerabilities are found because of the newly added packages:
 - Stop all other operations
 - Propose and apply fixes for the security issues
 - Only continue with the original task after security issues are resolved
- EXAMPLE:
 - After: npm install react-markdown
 - Do: Run codacy_cli_analyze with trivy
 - Before: Continuing with any other tasks

## General
- Repeat the relevant steps for each modified file.
- "Propose fixes" means to both suggest and, if possible, automatically apply the fixes.
- You MUST NOT wait for the user to ask for analysis or remind you to run the tool.
- Do not run `codacy_cli_analyze` looking for changes in duplicated code or code complexity metrics.
- Do not run `codacy_cli_analyze` looking for changes in code coverage.
- Do not try to manually install Codacy CLI using either brew, npm, npx, or any other package manager.
- If the Codacy CLI is not installed, just run the `codacy_cli_analyze` tool from Codacy's MCP Server.
- When calling `codacy_cli_analyze`, only send provider, organization and repository if the project is a git repository.

## Whenever a call to a Codacy tool that uses `repository` or `organization` as a parameter returns a 404 error
- Offer to run the `codacy_setup_repository` tool to add the repository to Codacy
- If the user accepts, run the `codacy_setup_repository` tool
- Do not ever try to run the `codacy_setup_repository` tool on your own
- After setup, immediately retry the action that failed (only retry once)
---
</file>

<file path=".env.example">
# OpenAI API Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Override default OpenAI API base URL
# OPENAI_API_BASE=https://api.openai.com/v1

# Optional: Server configuration (defaults shown)
# HOST=127.0.0.1
# PORT=5111
</file>

<file path="database/schema/schema.sql">
-- CCC Stage 2 Database Schema
-- Version: 1.0
-- Author: Phase 2 Implementation

-- Sessions table
CREATE TABLE IF NOT EXISTS sessions (
    session_id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_preferences TEXT DEFAULT '{}',
    status TEXT DEFAULT 'active',
    CHECK (status IN ('active', 'archived', 'expired'))
);

-- Conversations table
CREATE TABLE IF NOT EXISTS conversations (
    conversation_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    directive TEXT NOT NULL,
    status TEXT DEFAULT 'active',
    context_summary TEXT,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE,
    CHECK (status IN ('active', 'completed', 'archived'))
);

-- Turns table
CREATE TABLE IF NOT EXISTS turns (
    turn_id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,
    turn_number INTEGER NOT NULL,
    agent TEXT NOT NULL,
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT DEFAULT '{}',
    FOREIGN KEY (conversation_id) REFERENCES conversations(conversation_id) ON DELETE CASCADE,
    CHECK (agent IN ('wykeve', 'beatrice', 'codey')),
    UNIQUE(conversation_id, turn_number)
);

-- Agent States table
CREATE TABLE IF NOT EXISTS agent_states (
    state_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    agent TEXT NOT NULL,
    state_data TEXT DEFAULT '{}',
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE,
    CHECK (agent IN ('wykeve', 'beatrice', 'codey')),
    UNIQUE(session_id, agent)
);

-- Context summaries table for efficient retrieval
CREATE TABLE IF NOT EXISTS context_summaries (
    summary_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    summary_text TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    conversation_count INTEGER DEFAULT 0,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_sessions_status ON sessions(status);
CREATE INDEX IF NOT EXISTS idx_sessions_last_active ON sessions(last_active);
CREATE INDEX IF NOT EXISTS idx_conversations_session ON conversations(session_id);
CREATE INDEX IF NOT EXISTS idx_conversations_status ON conversations(status);
CREATE INDEX IF NOT EXISTS idx_turns_conversation ON turns(conversation_id);
CREATE INDEX IF NOT EXISTS idx_turns_timestamp ON turns(timestamp);
CREATE INDEX IF NOT EXISTS idx_agent_states_session ON agent_states(session_id);
CREATE INDEX IF NOT EXISTS idx_context_summaries_session ON context_summaries(session_id);
</file>

<file path="docs/phase2/CCC-S2-API.md">
# CCC - Stage 2 API Specification

**Document ID**: CCC-S2-API  
**Version**: 1.0  
**Author**: Codey, The Executor  
**Reviewed by**: Beatrice, The Supervisor  
**Approved by**: Wykeve, Prime Architect  
**Date**: 2024  
**Dependencies**: CCC-S2-ARCHITECTURE.md

---

## API Overview

The Stage 2 API extends the CCC system with comprehensive memory and session management capabilities. All Stage 1 endpoints remain functional for backward compatibility, with new v2 endpoints providing enhanced memory-aware operations.

## Base Configuration

- **Base URL**: `http://127.0.0.1:5111`
- **API Version**: v2
- **Content-Type**: `application/json`
- **Authentication**: Session-based tokens
- **Rate Limiting**: 100 requests/minute per session

## Authentication

### Session Token Format
```json
{
  "session_token": "sess_1234567890abcdef",
  "expires_at": "2024-01-01T12:00:00Z",
  "permissions": ["read", "write", "memory"]
}
```

## API Endpoints

### 1. Session Management

#### Create Session
```http
POST /api/v2/sessions
Content-Type: application/json

{
  "user_preferences": {
    "memory_retention_days": 30,
    "context_depth": 10,
    "auto_summarize": true
  },
  "initial_context": "Optional context string"
}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "created_at": "2024-01-01T10:00:00Z",
  "token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...",
  "status": "active"
}
```

#### Get Session Details
```http
GET /api/v2/sessions/{session_id}
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "created_at": "2024-01-01T10:00:00Z",
  "last_active": "2024-01-01T12:30:00Z",
  "conversation_count": 5,
  "total_turns": 47,
  "user_preferences": {
    "memory_retention_days": 30,
    "context_depth": 10,
    "auto_summarize": true
  },
  "status": "active"
}
```

#### Update Session Preferences
```http
PUT /api/v2/sessions/{session_id}/preferences
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "memory_retention_days": 60,
  "context_depth": 15,
  "auto_summarize": false
}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "updated_preferences": {
    "memory_retention_days": 60,
    "context_depth": 15,
    "auto_summarize": false
  },
  "updated_at": "2024-01-01T12:35:00Z"
}
```

#### Archive Session
```http
POST /api/v2/sessions/{session_id}/archive
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "status": "archived",
  "archived_at": "2024-01-01T12:40:00Z",
  "conversations_archived": 5
}
```

### 2. Conversation Management

#### Start New Conversation
```http
POST /api/v2/conversations
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "session_id": "sess_1234567890abcdef",
  "directive": "Create a Python function to calculate Fibonacci numbers",
  "use_context": true,
  "context_depth": 5
}
```

**Response:**
```json
{
  "conversation_id": "conv_abcdef1234567890",
  "session_id": "sess_1234567890abcdef",
  "directive": "Create a Python function to calculate Fibonacci numbers",
  "created_at": "2024-01-01T12:45:00Z",
  "relevant_context": [
    {
      "conversation_id": "conv_previous123",
      "relevance_score": 0.85,
      "summary": "Previous discussion about recursive algorithms"
    }
  ]
}
```

#### Get Conversation History
```http
GET /api/v2/conversations/{conversation_id}
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "conversation_id": "conv_abcdef1234567890",
  "session_id": "sess_1234567890abcdef",
  "directive": "Create a Python function to calculate Fibonacci numbers",
  "created_at": "2024-01-01T12:45:00Z",
  "status": "completed",
  "turns": [
    {
      "turn_id": "turn_001",
      "turn_number": 1,
      "agent": "wykeve",
      "content": "Create a Python function to calculate Fibonacci numbers",
      "timestamp": "2024-01-01T12:45:00Z",
      "metadata": {
        "user_input": true
      }
    },
    {
      "turn_id": "turn_002", 
      "turn_number": 2,
      "agent": "beatrice",
      "content": "I'll analyze this directive for creating a Fibonacci function...",
      "timestamp": "2024-01-01T12:45:15Z",
      "metadata": {
        "model": "gpt-4",
        "temperature": 0.7,
        "response_time_ms": 1240
      }
    }
  ]
}
```

#### Add Conversation Turn
```http
POST /api/v2/conversations/{conversation_id}/turns
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "agent": "beatrice",
  "content": "Based on the directive, I recommend...",
  "metadata": {
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 1000
  }
}
```

**Response:**
```json
{
  "turn_id": "turn_003",
  "conversation_id": "conv_abcdef1234567890",
  "turn_number": 3,
  "agent": "beatrice",
  "content": "Based on the directive, I recommend...",
  "timestamp": "2024-01-01T12:46:00Z",
  "stored": true
}
```

### 3. Context Management

#### Get Session Context
```http
GET /api/v2/sessions/{session_id}/context
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "context_summary": "This session has focused on Python programming, particularly algorithms and data structures. Recent conversations have covered recursive functions, optimization techniques, and code clarity.",
  "key_topics": [
    "python programming",
    "algorithms",
    "recursion",
    "optimization"
  ],
  "agent_states": {
    "beatrice": {
      "preferred_analysis_style": "analytical",
      "focus_areas": ["code_quality", "performance"],
      "learning_patterns": [
        "User prefers detailed explanations",
        "Emphasis on best practices"
      ]
    },
    "codey": {
      "preferred_implementation_style": "clean_code",
      "recent_technologies": ["python", "algorithms"],
      "successful_patterns": [
        "Step-by-step implementation",
        "Clear variable naming"
      ]
    }
  },
  "recent_conversations": [
    {
      "conversation_id": "conv_recent001",
      "directive": "Explain bubble sort algorithm",
      "relevance_score": 0.75,
      "created_at": "2024-01-01T11:30:00Z"
    }
  ]
}
```

#### Search Context
```http
POST /api/v2/sessions/{session_id}/context/search
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "query": "python functions",
  "max_results": 5,
  "relevance_threshold": 0.6,
  "time_range": {
    "start": "2024-01-01T00:00:00Z",
    "end": "2024-01-01T23:59:59Z"
  }
}
```

**Response:**
```json
{
  "query": "python functions",
  "results": [
    {
      "conversation_id": "conv_match001",
      "turn_id": "turn_005",
      "relevance_score": 0.92,
      "snippet": "Here's a Python function that demonstrates...",
      "context": "Discussion about function design patterns",
      "timestamp": "2024-01-01T10:15:00Z"
    }
  ],
  "total_matches": 3,
  "search_time_ms": 45
}
```

#### Update Context Summary
```http
PUT /api/v2/sessions/{session_id}/context/summary
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "summary": "Updated context summary based on recent interactions",
  "key_topics": ["python", "algorithms", "web development"],
  "trigger": "manual_update"
}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "summary_updated": true,
  "updated_at": "2024-01-01T13:00:00Z",
  "previous_summary_archived": true
}
```

### 4. Enhanced Chat Completions

#### Memory-Enhanced Chat Completions
```http
POST /v2/chat/completions
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "session_id": "sess_1234567890abcdef",
  "conversation_id": "conv_abcdef1234567890",
  "model": "gpt-4",
  "messages": [
    {
      "role": "user",
      "content": "Create a Python function to calculate Fibonacci numbers"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "memory_options": {
    "use_context": true,
    "context_depth": 5,
    "include_agent_state": true,
    "auto_store_response": true
  }
}
```

**Response:**
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1704110400,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Based on your previous interest in recursive algorithms and clean code practices, here's an optimized Fibonacci function...",
        "agent": "beatrice"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 200,
    "total_tokens": 350
  },
  "memory_metadata": {
    "context_used": true,
    "relevant_conversations": 2,
    "agent_state_applied": true,
    "turn_stored": true,
    "turn_id": "turn_new001"
  }
}
```

### 5. Agent State Management

#### Get Agent State
```http
GET /api/v2/sessions/{session_id}/agents/{agent_name}/state
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "agent": "beatrice",
  "state": {
    "personality_traits": {
      "analytical_depth": 0.8,
      "detail_orientation": 0.9,
      "supportive_tone": 0.7
    },
    "learned_patterns": [
      {
        "pattern": "User prefers step-by-step explanations",
        "confidence": 0.85,
        "learned_from": 12
      }
    ],
    "expertise_areas": [
      "code_analysis",
      "quality_assurance", 
      "strategic_planning"
    ],
    "interaction_history": {
      "total_interactions": 47,
      "successful_outcomes": 42,
      "preferred_response_length": "detailed"
    }
  },
  "last_updated": "2024-01-01T12:55:00Z"
}
```

#### Update Agent State
```http
PUT /api/v2/sessions/{session_id}/agents/{agent_name}/state
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "state_updates": {
    "personality_traits": {
      "analytical_depth": 0.85
    },
    "new_learned_pattern": {
      "pattern": "User appreciates code comments",
      "confidence": 0.75
    }
  },
  "update_reason": "positive_feedback_received"
}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "agent": "beatrice",
  "state_updated": true,
  "updated_fields": ["personality_traits", "learned_patterns"],
  "updated_at": "2024-01-01T13:05:00Z"
}
```

### 6. Memory Analytics

#### Get Memory Statistics
```http
GET /api/v2/sessions/{session_id}/analytics
Authorization: Bearer {session_token}
```

**Response:**
```json
{
  "session_id": "sess_1234567890abcdef",
  "analytics": {
    "memory_usage": {
      "total_conversations": 15,
      "total_turns": 127,
      "storage_size_mb": 2.4,
      "oldest_conversation": "2024-01-01T08:00:00Z"
    },
    "interaction_patterns": {
      "average_conversation_length": 8.5,
      "most_active_agent": "beatrice",
      "preferred_topics": ["programming", "algorithms", "optimization"],
      "response_satisfaction": 0.89
    },
    "context_effectiveness": {
      "context_utilization_rate": 0.76,
      "relevance_accuracy": 0.82,
      "context_hit_rate": 0.71
    }
  }
}
```

### 7. Data Export and Management

#### Export Session Data
```http
POST /api/v2/sessions/{session_id}/export
Authorization: Bearer {session_token}
Content-Type: application/json

{
  "format": "json",
  "include_metadata": true,
  "include_agent_states": true,
  "time_range": {
    "start": "2024-01-01T00:00:00Z",
    "end": "2024-01-01T23:59:59Z"
  }
}
```

**Response:**
```json
{
  "export_id": "export_xyz789",
  "session_id": "sess_1234567890abcdef",
  "format": "json",
  "status": "processing",
  "estimated_completion": "2024-01-01T13:15:00Z",
  "download_url": "/api/v2/exports/export_xyz789/download"
}
```

## Error Responses

### Standard Error Format
```json
{
  "error": {
    "code": "MEMORY_SERVICE_ERROR",
    "message": "Failed to retrieve session context",
    "details": {
      "session_id": "sess_1234567890abcdef",
      "operation": "get_context",
      "timestamp": "2024-01-01T13:20:00Z"
    },
    "retry_after": 30
  }
}
```

### Common Error Codes

| Code | HTTP Status | Description |
|------|-------------|-------------|
| `SESSION_NOT_FOUND` | 404 | Session ID does not exist |
| `SESSION_EXPIRED` | 401 | Session token has expired |
| `CONVERSATION_NOT_FOUND` | 404 | Conversation ID does not exist |
| `MEMORY_SERVICE_ERROR` | 503 | Memory service temporarily unavailable |
| `CONTEXT_RETRIEVAL_FAILED` | 500 | Failed to retrieve context data |
| `INVALID_SESSION_TOKEN` | 401 | Session token is invalid or malformed |
| `RATE_LIMIT_EXCEEDED` | 429 | Too many requests for this session |
| `STORAGE_QUOTA_EXCEEDED` | 413 | Session storage limit reached |

## Rate Limiting

### Limits by Endpoint Type
- **Session Management**: 10 requests/minute
- **Conversation Operations**: 30 requests/minute
- **Context Retrieval**: 50 requests/minute
- **Chat Completions**: 20 requests/minute
- **Analytics**: 5 requests/minute

### Rate Limit Headers
```http
X-RateLimit-Limit: 50
X-RateLimit-Remaining: 47
X-RateLimit-Reset: 1704110460
X-RateLimit-Window: 60
```

## Backward Compatibility

### Stage 1 Endpoints
All Stage 1 endpoints remain fully functional:
- `GET /health` - Health check
- `POST /v1/chat/completions` - Basic chat completions

### Migration Strategy
- Stage 1 endpoints continue to work without modification
- New v2 endpoints provide enhanced functionality
- Clients can adopt memory features incrementally
- No breaking changes to existing implementations

## SDK Examples

### JavaScript SDK Usage
```javascript
// Initialize CCC Memory Client
const ccc = new CCCMemoryClient({
  baseUrl: 'http://127.0.0.1:5111',
  apiVersion: 'v2'
});

// Create session with memory
const session = await ccc.createSession({
  memory_retention_days: 30,
  context_depth: 10
});

// Start memory-enhanced conversation
const conversation = await ccc.startConversation({
  session_id: session.session_id,
  directive: "Create a Python function for Fibonacci numbers",
  use_context: true
});

// Execute covenant cycle with memory
const response = await ccc.enhancedChatCompletion({
  session_id: session.session_id,
  conversation_id: conversation.conversation_id,
  messages: [
    { role: "user", content: "Create a Python function for Fibonacci numbers" }
  ],
  memory_options: {
    use_context: true,
    include_agent_state: true
  }
});
```

### Python SDK Usage
```python
from ccc_memory_client import CCCMemoryClient

# Initialize client
client = CCCMemoryClient(
    base_url='http://127.0.0.1:5111',
    api_version='v2'
)

# Create session
session = await client.create_session(
    memory_retention_days=30,
    context_depth=10
)

# Enhanced covenant cycle
response = await client.enhanced_chat_completion(
    session_id=session['session_id'],
    messages=[
        {"role": "user", "content": "Create a Python function for Fibonacci numbers"}
    ],
    memory_options={
        "use_context": True,
        "include_agent_state": True
    }
)
```

---

## Conclusion

The Stage 2 API provides comprehensive memory and context management capabilities while maintaining full backward compatibility with Stage 1. The RESTful design ensures easy integration and scaling, while the memory features enable sophisticated, context-aware interactions that improve with each use.

---

*"A well-designed API is a conversation between human intention and machine capability."* - Wykeve, Prime Architect

**Document Status**: APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Review**: Upon API Implementation Completion
</file>

<file path="docs/phase2/CCC-S2-ARCHITECTURE.md">
# CCC - Stage 2 Architecture Document

**Document ID**: CCC-S2-ARCHITECTURE  
**Version**: 1.0  
**Author**: Beatrice, The Archivist  
**Reviewed by**: Codey, The Executor  
**Approved by**: Wykeve, Prime Architect  
**Date**: 2024  
**Dependencies**: CCC-S2-MASTER.md

---

## Architecture Overview

Stage 2 extends the CCC foundation with a comprehensive memory and persistence layer, transforming the stateless 3-turn cycle into a stateful, context-aware system. The architecture maintains the security and simplicity of Stage 1 while adding sophisticated memory management capabilities.

## System Components

### 1. Memory Persistence Layer

#### Database Schema
```sql
-- Sessions table
CREATE TABLE sessions (
    session_id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_preferences TEXT, -- JSON blob
    status TEXT DEFAULT 'active' -- active, archived, expired
);

-- Conversations table  
CREATE TABLE conversations (
    conversation_id TEXT PRIMARY KEY,
    session_id TEXT REFERENCES sessions(session_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    directive TEXT NOT NULL,
    status TEXT DEFAULT 'active' -- active, completed, archived
);

-- Turns table
CREATE TABLE turns (
    turn_id TEXT PRIMARY KEY,
    conversation_id TEXT REFERENCES conversations(conversation_id),
    turn_number INTEGER NOT NULL,
    agent TEXT NOT NULL, -- wykeve, beatrice, codey
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT, -- JSON blob with model, temperature, etc.
    UNIQUE(conversation_id, turn_number)
);

-- Agent states table
CREATE TABLE agent_states (
    state_id TEXT PRIMARY KEY,
    session_id TEXT REFERENCES sessions(session_id),
    agent TEXT NOT NULL, -- beatrice, codey
    state_data TEXT NOT NULL, -- JSON blob
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(session_id, agent)
);

-- Context summaries table
CREATE TABLE context_summaries (
    summary_id TEXT PRIMARY KEY,
    session_id TEXT REFERENCES sessions(session_id),
    summary_text TEXT NOT NULL,
    conversation_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Data Access Layer (DAL)
```python
class MemoryDAL:
    """Data Access Layer for CCC Memory Operations"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.connection_pool = self._init_connection_pool()
    
    # Session Management
    async def create_session(self, session_id: str) -> dict
    async def get_session(self, session_id: str) -> dict
    async def update_session_activity(self, session_id: str) -> bool
    async def archive_session(self, session_id: str) -> bool
    
    # Conversation Management  
    async def create_conversation(self, session_id: str, directive: str) -> str
    async def get_conversation_history(self, conversation_id: str) -> list
    async def add_turn(self, conversation_id: str, turn_data: dict) -> str
    
    # Context Management
    async def get_session_context(self, session_id: str) -> dict
    async def update_context_summary(self, session_id: str, summary: str) -> bool
    
    # Agent State Management
    async def get_agent_state(self, session_id: str, agent: str) -> dict
    async def update_agent_state(self, session_id: str, agent: str, state: dict) -> bool
```

### 2. Memory Service Layer

#### Core Memory Service
```python
class MemoryService:
    """High-level memory operations for CCC"""
    
    def __init__(self, dal: MemoryDAL):
        self.dal = dal
        self.context_analyzer = ContextAnalyzer()
        self.encryption_service = EncryptionService()
    
    async def initialize_session(self, user_id: str = None) -> str:
        """Create new session with optional user context"""
        
    async def store_conversation_turn(
        self, 
        session_id: str,
        conversation_id: str,
        agent: str,
        content: str,
        metadata: dict
    ) -> bool:
        """Store a single turn with encryption and validation"""
        
    async def get_relevant_context(
        self, 
        session_id: str,
        current_directive: str,
        max_context_turns: int = 10
    ) -> dict:
        """Retrieve contextually relevant conversation history"""
        
    async def update_agent_learning(
        self,
        session_id: str,
        agent: str,
        interaction_outcome: dict
    ) -> bool:
        """Update agent learning patterns based on interaction"""
        
    async def generate_context_summary(self, session_id: str) -> str:
        """Generate intelligent summary of session context"""
```

#### Context Analyzer
```python
class ContextAnalyzer:
    """Analyzes conversations for context extraction and relevance"""
    
    def __init__(self):
        self.similarity_threshold = 0.7
        self.max_context_age_hours = 24
    
    def extract_key_topics(self, conversation_text: str) -> list:
        """Extract main topics and themes from conversation"""
        
    def calculate_relevance_score(
        self, 
        current_directive: str,
        historical_conversation: dict
    ) -> float:
        """Calculate relevance score between current and past interactions"""
        
    def summarize_conversation_sequence(self, turns: list) -> str:
        """Create concise summary of conversation sequence"""
        
    def identify_learning_patterns(self, agent_history: list) -> dict:
        """Identify patterns in agent responses for learning"""
```

### 3. Enhanced Proxy Server

#### New Endpoints
```python
# Memory Management Endpoints
@app.route('/api/v2/sessions', methods=['POST'])
def create_session():
    """Create new memory session"""

@app.route('/api/v2/sessions/<session_id>', methods=['GET'])  
def get_session(session_id):
    """Retrieve session information and context"""

@app.route('/api/v2/sessions/<session_id>/context', methods=['GET'])
def get_session_context(session_id):
    """Get relevant context for current session"""

@app.route('/api/v2/conversations', methods=['POST'])
def start_conversation():
    """Start new conversation within session"""

@app.route('/api/v2/conversations/<conversation_id>/turns', methods=['POST'])
def add_conversation_turn(conversation_id):
    """Add turn to existing conversation"""

# Enhanced Chat Completions with Memory
@app.route('/v2/chat/completions', methods=['POST'])
def enhanced_chat_completions():
    """Chat completions with memory context injection"""
```

#### Memory-Enhanced Request Processing
```python
async def process_enhanced_request(request_data: dict) -> dict:
    """Process chat request with memory context"""
    
    session_id = request_data.get('session_id')
    if session_id:
        # Retrieve relevant context
        context = await memory_service.get_relevant_context(
            session_id, 
            request_data['messages'][-1]['content']
        )
        
        # Inject context into system prompt
        enhanced_messages = inject_memory_context(
            request_data['messages'],
            context
        )
        
        # Process with OpenAI
        response = await call_openai_api(enhanced_messages)
        
        # Store conversation turn
        await memory_service.store_conversation_turn(
            session_id,
            request_data.get('conversation_id'),
            determine_agent(request_data),
            response['content'],
            extract_metadata(request_data, response)
        )
        
        return response
    else:
        # Fallback to Stage 1 behavior
        return await process_standard_request(request_data)
```

### 4. Frontend Enhancements

#### Session Management UI
```html
<!-- Session Selection Panel -->
<div class="session-panel">
    <div class="session-header">
        <h3>Memory Sessions</h3>
        <button id="new-session-btn">New Session</button>
    </div>
    
    <div class="session-list">
        <!-- Dynamically populated session list -->
    </div>
    
    <div class="session-controls">
        <button id="export-session">Export</button>
        <button id="archive-session">Archive</button>
    </div>
</div>
```

#### Context Display Panel
```html
<!-- Context Awareness Panel -->
<div class="context-panel">
    <div class="context-header">
        <h4>Session Context</h4>
        <span class="context-indicator" id="context-status">●</span>
    </div>
    
    <div class="context-summary" id="context-summary">
        <!-- AI-generated context summary -->
    </div>
    
    <div class="related-conversations">
        <h5>Related Discussions</h5>
        <div id="related-list">
            <!-- Links to related conversations -->
        </div>
    </div>
</div>
```

#### Enhanced JavaScript Architecture
```javascript
class CCCMemoryManager {
    constructor() {
        this.currentSession = null;
        this.contextCache = new Map();
        this.sessionList = [];
    }
    
    async initializeSession(userPreferences = {}) {
        // Create new session with memory capabilities
    }
    
    async loadSession(sessionId) {
        // Load existing session and context
    }
    
    async enhancedCovenantCycle(directive) {
        // Execute cycle with memory context injection
        const context = await this.getRelevantContext(directive);
        return await this.executeWithContext(directive, context);
    }
    
    async updateContext(conversationData) {
        // Update local context cache and server state
    }
    
    renderContextPanel() {
        // Update UI with current context information
    }
}
```

## Data Flow Architecture

### Memory-Enhanced Request Flow
```
1. User Input → Frontend Session Manager
2. Session Manager → Context Retrieval (if session exists)
3. Frontend → Enhanced Proxy Server (/v2/chat/completions)
4. Proxy Server → Memory Service (context injection)
5. Enhanced Request → OpenAI API
6. OpenAI Response → Memory Service (storage)
7. Response + Context → Frontend
8. Frontend → UI Update + Context Display
```

### Session Lifecycle
```
Session Creation:
1. Generate unique session ID
2. Initialize database records
3. Create agent state objects
4. Return session token to frontend

Context Building:
1. Store each conversation turn
2. Analyze for key topics and patterns
3. Update agent learning states
4. Generate periodic context summaries

Session Management:
1. Track activity timestamps
2. Archive inactive sessions
3. Cleanup expired data
4. Maintain performance metrics
```

## Performance Optimization

### Database Optimization
- **Indexing Strategy**: Composite indexes on frequently queried columns
- **Connection Pooling**: Async connection pool for concurrent operations
- **Query Optimization**: Prepared statements and efficient JOINs
- **Data Archival**: Automated archival of old conversations

### Memory Management
- **Context Caching**: LRU cache for frequently accessed contexts
- **Lazy Loading**: Load context only when needed
- **Compression**: Compress stored conversation data
- **Cleanup Jobs**: Automated cleanup of temporary data

### API Response Optimization
- **Parallel Processing**: Concurrent context retrieval and API calls
- **Response Streaming**: Stream responses while storing to memory
- **Background Tasks**: Asynchronous context analysis and summarization
- **Circuit Breakers**: Fallback to Stage 1 behavior on memory service failure

## Security Architecture

### Data Encryption
```python
class EncryptionService:
    """Handles encryption of sensitive memory data"""
    
    def __init__(self, key_manager: KeyManager):
        self.key_manager = key_manager
        self.cipher_suite = Fernet(key_manager.get_memory_key())
    
    def encrypt_conversation_content(self, content: str) -> bytes:
        """Encrypt conversation content for storage"""
        
    def decrypt_conversation_content(self, encrypted_content: bytes) -> str:
        """Decrypt conversation content for retrieval"""
        
    def encrypt_agent_state(self, state_dict: dict) -> bytes:
        """Encrypt agent state data"""
```

### Session Security
- **Token-Based Authentication**: Secure session tokens for API access
- **Session Isolation**: Cryptographic separation of user data
- **Access Logging**: Complete audit trail of memory operations
- **Data Sanitization**: Input validation and XSS prevention

## Monitoring and Observability

### Key Metrics
- **Memory Operation Latency**: Track performance of database operations
- **Session Activity**: Monitor active sessions and usage patterns
- **Context Relevance**: Measure effectiveness of context matching
- **Storage Growth**: Track database size and growth rates

### Health Checks
```python
@app.route('/health/memory', methods=['GET'])
def memory_health_check():
    """Comprehensive memory system health check"""
    return {
        'database_connection': check_database_health(),
        'memory_service': check_memory_service_health(),
        'encryption_service': check_encryption_health(),
        'performance_metrics': get_performance_snapshot()
    }
```

## Error Handling and Recovery

### Graceful Degradation
- **Memory Service Failures**: Fallback to Stage 1 stateless behavior
- **Database Outages**: Temporary in-memory storage with recovery
- **Context Retrieval Errors**: Continue without context enhancement
- **Encryption Failures**: Secure failure modes with alerting

### Data Recovery
- **Backup Strategy**: Automated database backups with retention policy
- **Transaction Rollback**: Atomic operations with rollback capability
- **Corruption Detection**: Data integrity checks and repair procedures
- **Disaster Recovery**: Complete system restoration procedures

## Testing Strategy

### Unit Testing
- **Memory Service Tests**: Comprehensive testing of all memory operations
- **Database Tests**: Schema validation and data integrity tests
- **Encryption Tests**: Security validation of encryption/decryption
- **Context Analysis Tests**: Validation of context matching algorithms

### Integration Testing
- **End-to-End Flows**: Complete session lifecycle testing
- **Performance Testing**: Load testing with concurrent sessions
- **Security Testing**: Penetration testing of memory features
- **Compatibility Testing**: Stage 1 backward compatibility validation

---

## Conclusion

The Stage 2 architecture represents a sophisticated evolution of the CCC system, introducing comprehensive memory and context capabilities while maintaining the elegant simplicity and security of the original design. The modular architecture ensures that memory features enhance rather than complicate the core system, providing a solid foundation for future stages.

---

*"Architecture is not just about what we build, but how well it serves those who will build upon it."* - Wykeve, Prime Architect

**Document Status**: APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Review**: Upon Phase 2.1 Completion
</file>

<file path="docs/phase2/CCC-S2-IMPLEMENTATION.md">
# CCC - Stage 2 Implementation Guide

**Document ID**: CCC-S2-IMPLEMENTATION  
**Version**: 1.0  
**Author**: Codey, The Executor  
**Reviewed by**: Beatrice, The Supervisor  
**Approved by**: Wykeve, Prime Architect  
**Date**: 2024  
**Dependencies**: CCC-S2-MASTER.md, CCC-S2-ARCHITECTURE.md, CCC-S2-API.md

---

## Implementation Overview

This guide provides step-by-step instructions for implementing Stage 2 memory and context retention capabilities in the CCC system. Follow the phases sequentially to ensure proper integration and testing at each stage.

## Prerequisites

### System Requirements
- **Python**: 3.9+ with async/await support
- **Database**: SQLite 3.35+ (or PostgreSQL for production)
- **Memory**: Minimum 1GB RAM for development
- **Disk**: 5GB free space for database and logs
- **Network**: HTTP/HTTPS support for external API calls

### Development Environment
```bash
# Update requirements.txt
echo "aiosqlite>=0.17.0" >> requirements.txt
echo "cryptography>=3.4.8" >> requirements.txt
echo "pyjwt>=2.4.0" >> requirements.txt
echo "python-dateutil>=2.8.2" >> requirements.txt

# Install dependencies
pip install -r requirements.txt
```

### Directory Structure
```
CCC/
├── docs/phase2/           # Phase 2 documentation
├── src/                   # New source directory
│   ├── memory/           # Memory service components
│   ├── models/           # Data models
│   ├── services/         # Business logic services
│   └── utils/            # Utility functions
├── database/             # Database schema and migrations
├── tests/               # Test suite
└── config/              # Configuration files
```

## Phase 2.1: Core Memory Infrastructure

### Step 1: Create Directory Structure
```bash
cd /path/to/CCC
mkdir -p src/{memory,models,services,utils}
mkdir -p database/{schema,migrations}
mkdir -p tests/{unit,integration}
mkdir -p config
```

### Step 2: Database Schema Implementation

Create `database/schema/schema.sql`:
```sql
-- Sessions table
CREATE TABLE IF NOT EXISTS sessions (
    session_id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_preferences TEXT DEFAULT '{}',
    status TEXT DEFAULT 'active',
    CHECK (status IN ('active', 'archived', 'expired'))
);

-- Conversations table
CREATE TABLE IF NOT EXISTS conversations (
    conversation_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    directive TEXT NOT NULL,
    status TEXT DEFAULT 'active',
    context_summary TEXT,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE,
    CHECK (status IN ('active', 'completed', 'archived'))
);

-- Turns table
CREATE TABLE IF NOT EXISTS turns (
    turn_id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,
    turn_number INTEGER NOT NULL,
    agent TEXT NOT NULL,
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT DEFAULT '{}',
    FOREIGN KEY (conversation_id) REFERENCES conversations(conversation_id) ON DELETE CASCADE,
    CHECK (agent IN ('wykeve', 'beatrice', 'codey')),
    UNIQUE(conversation_id, turn_number)
);

-- Agent states table
CREATE TABLE IF NOT EXISTS agent_states (
    state_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    agent TEXT NOT NULL,
    state_data TEXT NOT NULL DEFAULT '{}',
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE,
    CHECK (agent IN ('beatrice', 'codey')),
    UNIQUE(session_id, agent)
);

-- Context summaries table
CREATE TABLE IF NOT EXISTS context_summaries (
    summary_id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    summary_text TEXT NOT NULL,
    conversation_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id) ON DELETE CASCADE
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_sessions_status ON sessions(status);
CREATE INDEX IF NOT EXISTS idx_sessions_last_active ON sessions(last_active);
CREATE INDEX IF NOT EXISTS idx_conversations_session ON conversations(session_id);
CREATE INDEX IF NOT EXISTS idx_conversations_status ON conversations(status);
CREATE INDEX IF NOT EXISTS idx_turns_conversation ON turns(conversation_id);
CREATE INDEX IF NOT EXISTS idx_turns_timestamp ON turns(timestamp);
CREATE INDEX IF NOT EXISTS idx_agent_states_session ON agent_states(session_id);
```

### Step 3: Data Models Implementation

Create `src/models/memory_models.py`:
```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any
import json
import uuid

@dataclass
class Session:
    session_id: str = field(default_factory=lambda: f"sess_{uuid.uuid4().hex}")
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_active: datetime = field(default_factory=datetime.utcnow)
    user_preferences: Dict[str, Any] = field(default_factory=dict)
    status: str = "active"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'session_id': self.session_id,
            'created_at': self.created_at.isoformat(),
            'last_active': self.last_active.isoformat(),
            'user_preferences': self.user_preferences,
            'status': self.status
        }

@dataclass
class Conversation:
    conversation_id: str = field(default_factory=lambda: f"conv_{uuid.uuid4().hex}")
    session_id: str = ""
    created_at: datetime = field(default_factory=datetime.utcnow)
    directive: str = ""
    status: str = "active"
    context_summary: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'conversation_id': self.conversation_id,
            'session_id': self.session_id,
            'created_at': self.created_at.isoformat(),
            'directive': self.directive,
            'status': self.status,
            'context_summary': self.context_summary
        }

@dataclass
class Turn:
    turn_id: str = field(default_factory=lambda: f"turn_{uuid.uuid4().hex}")
    conversation_id: str = ""
    turn_number: int = 0
    agent: str = ""
    content: str = ""
    timestamp: datetime = field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'turn_id': self.turn_id,
            'conversation_id': self.conversation_id,
            'turn_number': self.turn_number,
            'agent': self.agent,
            'content': self.content,
            'timestamp': self.timestamp.isoformat(),
            'metadata': self.metadata
        }

@dataclass
class AgentState:
    state_id: str = field(default_factory=lambda: f"state_{uuid.uuid4().hex}")
    session_id: str = ""
    agent: str = ""
    state_data: Dict[str, Any] = field(default_factory=dict)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'state_id': self.state_id,
            'session_id': self.session_id,
            'agent': self.agent,
            'state_data': self.state_data,
            'updated_at': self.updated_at.isoformat()
        }
```

### Step 4: Data Access Layer Implementation

Create `src/memory/database.py`:
```python
import aiosqlite
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager

from ..models.memory_models import Session, Conversation, Turn, AgentState

logger = logging.getLogger(__name__)

class MemoryDAL:
    """Data Access Layer for CCC Memory Operations"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._connection_pool_size = 10
    
    async def initialize_database(self):
        """Initialize database with schema"""
        async with aiosqlite.connect(self.db_path) as db:
            with open('database/schema/schema.sql', 'r') as f:
                schema = f.read()
            await db.executescript(schema)
            await db.commit()
            logger.info("Database initialized successfully")
    
    @asynccontextmanager
    async def get_connection(self):
        """Get database connection with proper error handling"""
        conn = None
        try:
            conn = await aiosqlite.connect(self.db_path)
            conn.row_factory = aiosqlite.Row
            yield conn
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            if conn:
                await conn.rollback()
            raise
        finally:
            if conn:
                await conn.close()
    
    # Session Management
    async def create_session(self, session: Session) -> bool:
        """Create new session"""
        async with self.get_connection() as conn:
            await conn.execute(
                """INSERT INTO sessions 
                   (session_id, created_at, last_active, user_preferences, status)
                   VALUES (?, ?, ?, ?, ?)""",
                (session.session_id, session.created_at, session.last_active,
                 json.dumps(session.user_preferences), session.status)
            )
            await conn.commit()
            return True
    
    async def get_session(self, session_id: str) -> Optional[Session]:
        """Retrieve session by ID"""
        async with self.get_connection() as conn:
            cursor = await conn.execute(
                "SELECT * FROM sessions WHERE session_id = ?",
                (session_id,)
            )
            row = await cursor.fetchone()
            if row:
                return Session(
                    session_id=row['session_id'],
                    created_at=datetime.fromisoformat(row['created_at']),
                    last_active=datetime.fromisoformat(row['last_active']),
                    user_preferences=json.loads(row['user_preferences']),
                    status=row['status']
                )
            return None
    
    async def update_session_activity(self, session_id: str) -> bool:
        """Update session last activity timestamp"""
        async with self.get_connection() as conn:
            cursor = await conn.execute(
                "UPDATE sessions SET last_active = ? WHERE session_id = ?",
                (datetime.utcnow(), session_id)
            )
            await conn.commit()
            return cursor.rowcount > 0
    
    # Conversation Management
    async def create_conversation(self, conversation: Conversation) -> bool:
        """Create new conversation"""
        async with self.get_connection() as conn:
            await conn.execute(
                """INSERT INTO conversations 
                   (conversation_id, session_id, created_at, directive, status, context_summary)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (conversation.conversation_id, conversation.session_id,
                 conversation.created_at, conversation.directive,
                 conversation.status, conversation.context_summary)
            )
            await conn.commit()
            return True
    
    async def get_conversation(self, conversation_id: str) -> Optional[Conversation]:
        """Retrieve conversation by ID"""
        async with self.get_connection() as conn:
            cursor = await conn.execute(
                "SELECT * FROM conversations WHERE conversation_id = ?",
                (conversation_id,)
            )
            row = await cursor.fetchone()
            if row:
                return Conversation(
                    conversation_id=row['conversation_id'],
                    session_id=row['session_id'],
                    created_at=datetime.fromisoformat(row['created_at']),
                    directive=row['directive'],
                    status=row['status'],
                    context_summary=row['context_summary']
                )
            return None
    
    async def add_turn(self, turn: Turn) -> bool:
        """Add turn to conversation"""
        async with self.get_connection() as conn:
            await conn.execute(
                """INSERT INTO turns 
                   (turn_id, conversation_id, turn_number, agent, content, timestamp, metadata)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (turn.turn_id, turn.conversation_id, turn.turn_number,
                 turn.agent, turn.content, turn.timestamp, json.dumps(turn.metadata))
            )
            await conn.commit()
            return True
    
    async def get_conversation_turns(self, conversation_id: str) -> List[Turn]:
        """Get all turns for a conversation"""
        async with self.get_connection() as conn:
            cursor = await conn.execute(
                "SELECT * FROM turns WHERE conversation_id = ? ORDER BY turn_number",
                (conversation_id,)
            )
            rows = await cursor.fetchall()
            return [
                Turn(
                    turn_id=row['turn_id'],
                    conversation_id=row['conversation_id'],
                    turn_number=row['turn_number'],
                    agent=row['agent'],
                    content=row['content'],
                    timestamp=datetime.fromisoformat(row['timestamp']),
                    metadata=json.loads(row['metadata'])
                )
                for row in rows
            ]
    
    # Agent State Management
    async def get_agent_state(self, session_id: str, agent: str) -> Optional[AgentState]:
        """Get agent state for session"""
        async with self.get_connection() as conn:
            cursor = await conn.execute(
                "SELECT * FROM agent_states WHERE session_id = ? AND agent = ?",
                (session_id, agent)
            )
            row = await cursor.fetchone()
            if row:
                return AgentState(
                    state_id=row['state_id'],
                    session_id=row['session_id'],
                    agent=row['agent'],
                    state_data=json.loads(row['state_data']),
                    updated_at=datetime.fromisoformat(row['updated_at'])
                )
            return None
    
    async def update_agent_state(self, agent_state: AgentState) -> bool:
        """Update or create agent state"""
        async with self.get_connection() as conn:
            await conn.execute(
                """INSERT OR REPLACE INTO agent_states 
                   (state_id, session_id, agent, state_data, updated_at)
                   VALUES (?, ?, ?, ?, ?)""",
                (agent_state.state_id, agent_state.session_id, agent_state.agent,
                 json.dumps(agent_state.state_data), agent_state.updated_at)
            )
            await conn.commit()
            return True
```

### Step 5: Memory Service Implementation

Create `src/services/memory_service.py`:
```python
import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta

from ..memory.database import MemoryDAL
from ..models.memory_models import Session, Conversation, Turn, AgentState
from ..utils.encryption import EncryptionService
from ..utils.context_analyzer import ContextAnalyzer

logger = logging.getLogger(__name__)

class MemoryService:
    """High-level memory operations for CCC"""
    
    def __init__(self, dal: MemoryDAL):
        self.dal = dal
        self.context_analyzer = ContextAnalyzer()
        self.encryption_service = EncryptionService()
        self._session_cache = {}
        self._cache_timeout = 300  # 5 minutes
    
    async def initialize_session(self, user_preferences: Dict[str, Any] = None) -> Session:
        """Create new session with optional user context"""
        session = Session(user_preferences=user_preferences or {})
        
        # Initialize default agent states
        beatrice_state = AgentState(
            session_id=session.session_id,
            agent="beatrice",
            state_data={
                "personality_traits": {
                    "analytical_depth": 0.8,
                    "detail_orientation": 0.9,
                    "supportive_tone": 0.7
                },
                "learned_patterns": [],
                "expertise_areas": ["code_analysis", "quality_assurance", "strategic_planning"],
                "interaction_history": {
                    "total_interactions": 0,
                    "successful_outcomes": 0,
                    "preferred_response_length": "detailed"
                }
            }
        )
        
        codey_state = AgentState(
            session_id=session.session_id,
            agent="codey",
            state_data={
                "personality_traits": {
                    "creativity_level": 0.8,
                    "implementation_focus": 0.9,
                    "detail_attention": 0.8
                },
                "execution_history": [],
                "preferred_approaches": [],
                "successful_patterns": []
            }
        )
        
        # Store in database
        await self.dal.create_session(session)
        await self.dal.update_agent_state(beatrice_state)
        await self.dal.update_agent_state(codey_state)
        
        # Cache session
        self._session_cache[session.session_id] = {
            'session': session,
            'cached_at': datetime.utcnow()
        }
        
        logger.info(f"Created new session: {session.session_id}")
        return session
    
    async def get_session(self, session_id: str) -> Optional[Session]:
        """Get session with caching"""
        # Check cache first
        cached = self._session_cache.get(session_id)
        if cached and (datetime.utcnow() - cached['cached_at']).seconds < self._cache_timeout:
            await self.dal.update_session_activity(session_id)
            return cached['session']
        
        # Load from database
        session = await self.dal.get_session(session_id)
        if session:
            self._session_cache[session_id] = {
                'session': session,
                'cached_at': datetime.utcnow()
            }
            await self.dal.update_session_activity(session_id)
        
        return session
    
    async def store_conversation_turn(
        self, 
        session_id: str,
        conversation_id: str,
        agent: str,
        content: str,
        metadata: Dict[str, Any] = None
    ) -> bool:
        """Store a single turn with encryption and validation"""
        try:
            # Get conversation to determine turn number
            conversation = await self.dal.get_conversation(conversation_id)
            if not conversation:
                logger.error(f"Conversation not found: {conversation_id}")
                return False
            
            # Get existing turns to determine next turn number
            existing_turns = await self.dal.get_conversation_turns(conversation_id)
            turn_number = max([t.turn_number for t in existing_turns], default=0) + 1
            
            # Encrypt sensitive content if configured
            encrypted_content = content
            if self.encryption_service.is_enabled():
                encrypted_content = self.encryption_service.encrypt_content(content)
            
            # Create and store turn
            turn = Turn(
                conversation_id=conversation_id,
                turn_number=turn_number,
                agent=agent,
                content=encrypted_content,
                metadata=metadata or {}
            )
            
            success = await self.dal.add_turn(turn)
            if success:
                # Update agent learning patterns asynchronously
                asyncio.create_task(
                    self._update_agent_learning(session_id, agent, content, metadata)
                )
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to store conversation turn: {e}")
            return False
    
    async def get_relevant_context(
        self, 
        session_id: str,
        current_directive: str,
        max_context_turns: int = 10
    ) -> Dict[str, Any]:
        """Retrieve contextually relevant conversation history"""
        try:
            # This is a simplified implementation
            # In production, you'd use more sophisticated similarity matching
            
            context = {
                'session_id': session_id,
                'relevant_conversations': [],
                'agent_states': {},
                'context_summary': '',
                'total_conversations': 0
            }
            
            # Get agent states
            for agent in ['beatrice', 'codey']:
                agent_state = await self.dal.get_agent_state(session_id, agent)
                if agent_state:
                    context['agent_states'][agent] = agent_state.state_data
            
            # TODO: Implement sophisticated context matching
            # For now, return basic structure
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get relevant context: {e}")
            return {'error': str(e)}
    
    async def _update_agent_learning(
        self,
        session_id: str,
        agent: str,
        content: str,
        metadata: Dict[str, Any]
    ):
        """Update agent learning patterns based on interaction"""
        try:
            agent_state = await self.dal.get_agent_state(session_id, agent)
            if not agent_state:
                return
            
            # Update interaction count
            state_data = agent_state.state_data
            interaction_history = state_data.get('interaction_history', {})
            interaction_history['total_interactions'] = interaction_history.get('total_interactions', 0) + 1
            
            # Simple learning pattern: track content length preferences
            content_length = len(content)
            if 'content_length_preferences' not in state_data:
                state_data['content_length_preferences'] = []
            
            state_data['content_length_preferences'].append(content_length)
            
            # Keep only last 50 measurements
            if len(state_data['content_length_preferences']) > 50:
                state_data['content_length_preferences'] = state_data['content_length_preferences'][-50:]
            
            # Update agent state
            agent_state.state_data = state_data
            agent_state.updated_at = datetime.utcnow()
            
            await self.dal.update_agent_state(agent_state)
            
        except Exception as e:
            logger.error(f"Failed to update agent learning: {e}")
```

### Step 6: Basic Testing Infrastructure

Create `tests/unit/test_memory_service.py`:
```python
import pytest
import asyncio
import tempfile
import os
from datetime import datetime

from src.memory.database import MemoryDAL
from src.services.memory_service import MemoryService
from src.models.memory_models import Session

class TestMemoryService:
    
    @pytest.fixture
    async def memory_service(self):
        # Create temporary database
        db_fd, db_path = tempfile.mkstemp(suffix='.db')
        os.close(db_fd)
        
        try:
            dal = MemoryDAL(db_path)
            await dal.initialize_database()
            service = MemoryService(dal)
            yield service
        finally:
            os.unlink(db_path)
    
    @pytest.mark.asyncio
    async def test_initialize_session(self, memory_service):
        """Test session initialization"""
        session = await memory_service.initialize_session()
        
        assert session.session_id.startswith('sess_')
        assert session.status == 'active'
        assert isinstance(session.created_at, datetime)
    
    @pytest.mark.asyncio
    async def test_get_session(self, memory_service):
        """Test session retrieval"""
        # Create session
        session = await memory_service.initialize_session()
        
        # Retrieve session
        retrieved = await memory_service.get_session(session.session_id)
        
        assert retrieved is not None
        assert retrieved.session_id == session.session_id
    
    @pytest.mark.asyncio
    async def test_nonexistent_session(self, memory_service):
        """Test retrieving non-existent session"""
        retrieved = await memory_service.get_session('nonexistent')
        assert retrieved is None

# Run tests with: pytest tests/unit/test_memory_service.py -v
```

## Phase 2.2: Context Retention Logic

### Step 7: Context Analyzer Implementation

Create `src/utils/context_analyzer.py`:
```python
import re
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from collections import Counter
import asyncio

logger = logging.getLogger(__name__)

class ContextAnalyzer:
    """Analyzes conversations for context extraction and relevance"""
    
    def __init__(self):
        self.similarity_threshold = 0.7
        self.max_context_age_hours = 24
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being'
        }
    
    def extract_key_topics(self, conversation_text: str) -> List[str]:
        """Extract main topics and themes from conversation"""
        try:
            # Simple keyword extraction (in production, use NLP libraries)
            words = re.findall(r'\b[a-zA-Z]{3,}\b', conversation_text.lower())
            filtered_words = [w for w in words if w not in self.stop_words]
            
            # Count word frequency
            word_counts = Counter(filtered_words)
            
            # Return top 10 most common words as topics
            topics = [word for word, count in word_counts.most_common(10)]
            
            return topics
            
        except Exception as e:
            logger.error(f"Failed to extract key topics: {e}")
            return []
    
    def calculate_relevance_score(
        self, 
        current_directive: str,
        historical_conversation: Dict[str, Any]
    ) -> float:
        """Calculate relevance score between current and past interactions"""
        try:
            # Simple relevance calculation based on word overlap
            current_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', current_directive.lower()))
            historical_text = historical_conversation.get('directive', '')
            historical_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', historical_text.lower()))
            
            # Calculate Jaccard similarity
            intersection = current_words.intersection(historical_words)
            union = current_words.union(historical_words)
            
            if not union:
                return 0.0
            
            similarity = len(intersection) / len(union)
            
            # Apply time decay
            conversation_age = datetime.utcnow() - historical_conversation.get('created_at', datetime.utcnow())
            age_hours = conversation_age.total_seconds() / 3600
            
            time_decay = max(0, 1 - (age_hours / self.max_context_age_hours))
            
            return similarity * time_decay
            
        except Exception as e:
            logger.error(f"Failed to calculate relevance score: {e}")
            return 0.0
    
    def summarize_conversation_sequence(self, turns: List[Dict[str, Any]]) -> str:
        """Create concise summary of conversation sequence"""
        try:
            if not turns:
                return "No conversation history available."
            
            # Extract key points from each turn
            key_points = []
            for turn in turns:
                agent = turn.get('agent', 'unknown')
                content = turn.get('content', '')
                
                # Simple summarization: take first sentence
                first_sentence = content.split('.')[0] if content else ''
                if first_sentence:
                    key_points.append(f"{agent.title()}: {first_sentence}...")
            
            # Combine into summary
            summary = "Conversation summary:\n" + "\n".join(key_points[-5:])  # Last 5 turns
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to summarize conversation: {e}")
            return "Unable to generate conversation summary."
    
    def identify_learning_patterns(self, agent_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Identify patterns in agent responses for learning"""
        try:
            patterns = {
                'response_lengths': [],
                'common_phrases': [],
                'topic_preferences': [],
                'successful_approaches': []
            }
            
            for interaction in agent_history:
                content = interaction.get('content', '')
                
                # Track response lengths
                patterns['response_lengths'].append(len(content))
                
                # Extract common phrases (simple implementation)
                phrases = re.findall(r'\b\w+\s+\w+\b', content.lower())
                patterns['common_phrases'].extend(phrases)
            
            # Calculate averages and patterns
            if patterns['response_lengths']:
                avg_length = sum(patterns['response_lengths']) / len(patterns['response_lengths'])
                patterns['average_response_length'] = avg_length
            
            # Count common phrases
            phrase_counts = Counter(patterns['common_phrases'])
            patterns['top_phrases'] = phrase_counts.most_common(5)
            
            return patterns
            
        except Exception as e:
            logger.error(f"Failed to identify learning patterns: {e}")
            return {}
```

### Step 8: Enhanced Proxy Server Integration

Update `proxy_server.py` with memory capabilities:
```python
# Add these imports at the top
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.memory.database import MemoryDAL
from src.services.memory_service import MemoryService

# Initialize memory service after Flask app creation
memory_dal = None
memory_service = None

async def initialize_memory_service():
    """Initialize memory service"""
    global memory_dal, memory_service
    
    db_path = os.getenv('CCC_DATABASE_PATH', 'ccc_memory.db')
    memory_dal = MemoryDAL(db_path)
    await memory_dal.initialize_database()
    memory_service = MemoryService(memory_dal)
    logger.info("Memory service initialized successfully")

# Add new endpoint for session management
@app.route('/api/v2/sessions', methods=['POST'])
def create_session():
    """Create new memory session"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        data = request.get_json() or {}
        user_preferences = data.get('user_preferences', {})
        
        # Run async function in sync context
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        session = loop.run_until_complete(
            memory_service.initialize_session(user_preferences)
        )
        loop.close()
        
        return jsonify({
            'session_id': session.session_id,
            'created_at': session.created_at.isoformat(),
            'status': session.status
        })
        
    except Exception as e:
        logger.error(f"Failed to create session: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """Get session details"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        session = loop.run_until_complete(
            memory_service.get_session(session_id)
        )
        loop.close()
        
        if not session:
            return jsonify({'error': 'Session not found'}), 404
        
        return jsonify(session.to_dict())
        
    except Exception as e:
        logger.error(f"Failed to get session: {e}")
        return jsonify({'error': str(e)}), 500

# Enhanced chat completions with memory
@app.route('/v2/chat/completions', methods=['POST'])
def enhanced_chat_completions():
    """Chat completions with memory context injection"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        session_id = data.get('session_id')
        
        if session_id and memory_service:
            # Get context and enhance request
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # Get relevant context
            current_message = data['messages'][-1]['content'] if data['messages'] else ''
            context = loop.run_until_complete(
                memory_service.get_relevant_context(session_id, current_message)
            )
            
            # Inject context into system prompt if available
            if context.get('agent_states'):
                system_context = f"Previous context: {context.get('context_summary', '')}"
                enhanced_messages = [
                    {'role': 'system', 'content': system_context}
                ] + data['messages']
                data['messages'] = enhanced_messages
            
            loop.close()
        
        # Process with OpenAI (existing logic)
        headers = {
            'Authorization': f'Bearer {OPENAI_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        response = requests.post(
            f'{OPENAI_API_BASE}/chat/completions',
            json=data,
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            
            # Store response in memory if session provided
            if session_id and memory_service:
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    conversation_id = data.get('conversation_id', f"conv_{session_id}_{int(datetime.now().timestamp())}")
                    agent = data.get('agent', 'assistant')
                    content = result['choices'][0]['message']['content']
                    
                    loop.run_until_complete(
                        memory_service.store_conversation_turn(
                            session_id, conversation_id, agent, content, data.get('metadata', {})
                        )
                    )
                    loop.close()
                except Exception as e:
                    logger.warning(f"Failed to store response in memory: {e}")
            
            return jsonify(result)
        else:
            error_data = response.json() if response.content else {'error': 'Unknown error'}
            return jsonify(error_data), response.status_code
            
    except Exception as e:
        logger.error(f"Enhanced chat completion error: {e}")
        return jsonify({'error': str(e)}), 500

# Initialize memory service on startup
if __name__ == '__main__':
    import asyncio
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(initialize_memory_service())
    loop.close()
    
    logger.info(f"Covenant API Proxy with Memory is running on http://{HOST}:{PORT}")
    app.run(host=HOST, port=PORT, debug=False)
```

## Phase 2.3: Frontend Integration

### Step 9: Frontend Memory Enhancements

Update `resonant_loop_lab.html` with memory capabilities:
```javascript
// Add after existing configuration
class CCCMemoryManager {
    constructor() {
        this.currentSession = null;
        this.sessionList = [];
        this.contextCache = new Map();
        this.memoryEnabled = false;
    }
    
    async initializeMemory() {
        try {
            // Check if memory service is available
            const response = await fetch(`${PROXY_URL}/api/v2/sessions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    user_preferences: {
                        memory_retention_days: 30,
                        context_depth: 10,
                        auto_summarize: true
                    }
                })
            });
            
            if (response.ok) {
                const session = await response.json();
                this.currentSession = session;
                this.memoryEnabled = true;
                this.updateMemoryUI();
                console.log('Memory service initialized:', session.session_id);
            }
        } catch (error) {
            console.warn('Memory service not available, falling back to Stage 1 behavior');
            this.memoryEnabled = false;
        }
    }
    
    updateMemoryUI() {
        const memoryIndicator = document.createElement('div');
        memoryIndicator.id = 'memory-indicator';
        memoryIndicator.className = 'text-sm text-green-400 mb-2';
        memoryIndicator.innerHTML = `
            <span class="inline-block w-2 h-2 bg-green-400 rounded-full mr-2"></span>
            Memory Active: ${this.currentSession ? this.currentSession.session_id.substr(0, 8) + '...' : 'None'}
        `;
        
        const statusContainer = document.querySelector('.status-container');
        if (statusContainer && !document.getElementById('memory-indicator')) {
            statusContainer.appendChild(memoryIndicator);
        }
    }
    
    async enhancedCovenantCycle(directive) {
        if (!this.memoryEnabled || !this.currentSession) {
            // Fallback to original behavior
            return await window.originalCovenantCycle(directive);
        }
        
        try {
            // Use v2 endpoint with memory
            const requestBody = {
                session_id: this.currentSession.session_id,
                model: modelSelect.value,
                messages: [
                    { role: 'user', content: directive }
                ],
                temperature: parseFloat(temperatureInput.value),
                max_tokens: 1000,
                memory_options: {
                    use_context: true,
                    include_agent_state: true,
                    auto_store_response: true
                }
            };
            
            const response = await fetch(`${PROXY_URL}/v2/chat/completions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(requestBody)
            });
            
            if (!response.ok) {
                throw new Error('Memory-enhanced request failed');
            }
            
            const data = await response.json();
            return data.choices[0].message.content;
            
        } catch (error) {
            console.error('Memory-enhanced cycle failed, falling back:', error);
            return await window.originalCovenantCycle(directive);
        }
    }
}

// Initialize memory manager
const memoryManager = new CCCMemoryManager();

// Store original function for fallback
window.originalCovenantCycle = initiateCovenantCycle;

// Update initialization
document.addEventListener('DOMContentLoaded', async () => {
    checkServerStatus();
    setupEventListeners();
    updateTemperatureDisplay();
    
    // Initialize memory if available
    await memoryManager.initializeMemory();
});

// Update the covenant cycle function to use memory
async function initiateCovenantCycle() {
    const directive = messageInput.value.trim();
    if (!directive || isProcessing) return;

    isProcessing = true;
    updateStatus('processing', 'Initiating Covenant Command Cycle...');
    
    // Clear previous conversation and add Prime Architect directive
    clearChat();
    addMessage('Wykeve (Prime Architect)', directive, 'text-blue-400');
    messageInput.value = '';
    
    try {
        // Use memory-enhanced cycle if available
        if (memoryManager.memoryEnabled) {
            // Enhanced 3-turn cycle with memory
            await enhancedMemoryCycle(directive);
        } else {
            // Original 3-turn cycle
            await originalCycle(directive);
        }
        
        // Cycle complete
        addMessage('System', '✅ Covenant Command Cycle Complete - 3 turns executed successfully', 'text-purple-400');
        
    } catch (error) {
        removeTypingIndicator();
        addMessage('System', `❌ Cycle Error: ${error.message}`, 'text-red-400');
        console.error('Covenant cycle error:', error);
    } finally {
        isProcessing = false;
        updateStatus('connected', 'Covenant API Proxy is running on http://127.0.0.1:5111');
    }
}

async function enhancedMemoryCycle(directive) {
    // Turn 1: Prime Architect → Beatrice with memory context
    addTypingIndicator('Beatrice');
    const beatriceAnalysis = await memoryManager.enhancedCovenantCycle(
        `Prime Architect Directive: "${directive}"\n\nAs the Supervisor, analyze this directive and provide clear, actionable guidance for the Executor to follow.`
    );
    removeTypingIndicator();
    addMessage(`${agents.beatrice.name} (${agents.beatrice.title})`, beatriceAnalysis, agents.beatrice.color);
    
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    // Turn 2: Beatrice → Codey with memory context
    addTypingIndicator('Codey');
    const codeyExecution = await memoryManager.enhancedCovenantCycle(
        `Based on the Supervisor's analysis and guidance above, execute the directive. Provide the implementation, solution, or creative output as directed.`
    );
    removeTypingIndicator();
    addMessage(`${agents.codey.name} (${agents.codey.title})`, codeyExecution, agents.codey.color);
    
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    // Turn 3: Codey → Beatrice final review with memory context
    addTypingIndicator('Beatrice');
    const beatriceReview = await memoryManager.enhancedCovenantCycle(
        `Executor's Implementation: ${codeyExecution}\n\nAs the Supervisor, provide your final review and assessment of the Executor's work. Does it fulfill the Prime Architect's directive?`
    );
    removeTypingIndicator();
    addMessage(`${agents.beatrice.name} (${agents.beatrice.title})`, beatriceReview, agents.beatrice.color);
}

async function originalCycle(directive) {
    // Original Stage 1 implementation for fallback
    // ... (existing implementation)
}
```

## Phase 2.4: Testing & Validation

### Step 10: Comprehensive Testing

Create `tests/integration/test_memory_integration.py`:
```python
import pytest
import asyncio
import tempfile
import os
import json

from src.memory.database import MemoryDAL
from src.services.memory_service import MemoryService

class TestMemoryIntegration:
    
    @pytest.fixture
    async def full_memory_system(self):
        """Set up complete memory system for testing"""
        db_fd, db_path = tempfile.mkstemp(suffix='.db')
        os.close(db_fd)
        
        try:
            dal = MemoryDAL(db_path)
            await dal.initialize_database()
            service = MemoryService(dal)
            yield service
        finally:
            os.unlink(db_path)
    
    @pytest.mark.asyncio
    async def test_complete_covenant_cycle_with_memory(self, full_memory_system):
        """Test complete covenant cycle with memory storage"""
        service = full_memory_system
        
        # Initialize session
        session = await service.initialize_session({
            'memory_retention_days': 30,
            'context_depth': 10
        })
        
        # Create conversation
        from src.models.memory_models import Conversation
        conversation = Conversation(
            session_id=session.session_id,
            directive="Create a Python function to calculate Fibonacci numbers"
        )
        await service.dal.create_conversation(conversation)
        
        # Store 3-turn cycle
        turns = [
            ("wykeve", "Create a Python function to calculate Fibonacci numbers"),
            ("beatrice", "I'll analyze this directive for creating a Fibonacci function..."),
            ("codey", "Here's an optimized Fibonacci function implementation..."),
            ("beatrice", "The implementation meets all requirements successfully.")
        ]
        
        for agent, content in turns:
            success = await service.store_conversation_turn(
                session.session_id,
                conversation.conversation_id,
                agent,
                content,
                {"model": "gpt-4", "temperature": 0.7}
            )
            assert success
        
        # Retrieve context
        context = await service.get_relevant_context(
            session.session_id,
            "Create another algorithm function"
        )
        
        assert context['session_id'] == session.session_id
        assert 'agent_states' in context
    
    @pytest.mark.asyncio
    async def test_session_persistence(self, full_memory_system):
        """Test that sessions persist correctly"""
        service = full_memory_system
        
        # Create session
        original_session = await service.initialize_session()
        
        # Retrieve session
        retrieved_session = await service.get_session(original_session.session_id)
        
        assert retrieved_session is not None
        assert retrieved_session.session_id == original_session.session_id
        assert retrieved_session.status == 'active'

# Run with: pytest tests/integration/test_memory_integration.py -v
```

### Step 11: Performance Testing

Create `tests/performance/test_memory_performance.py`:
```python
import pytest
import asyncio
import time
import tempfile
import os

from src.memory.database import MemoryDAL
from src.services.memory_service import MemoryService

class TestMemoryPerformance:
    
    @pytest.fixture
    async def performance_memory_system(self):
        """Set up memory system for performance testing"""
        db_fd, db_path = tempfile.mkstemp(suffix='.db')
        os.close(db_fd)
        
        try:
            dal = MemoryDAL(db_path)
            await dal.initialize_database()
            service = MemoryService(dal)
            yield service
        finally:
            os.unlink(db_path)
    
    @pytest.mark.asyncio
    async def test_session_creation_performance(self, performance_memory_system):
        """Test session creation meets performance requirements"""
        service = performance_memory_system
        
        start_time = time.time()
        
        # Create 10 sessions concurrently
        tasks = [
            service.initialize_session({'test': f'session_{i}'})
            for i in range(10)
        ]
        
        sessions = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Should complete within 1 second
        assert total_time < 1.0
        assert len(sessions) == 10
        
        # All sessions should be unique
        session_ids = [s.session_id for s in sessions]
        assert len(set(session_ids)) == 10
    
    @pytest.mark.asyncio 
    async def test_turn_storage_performance(self, performance_memory_system):
        """Test turn storage meets latency requirements"""
        service = performance_memory_system
        
        # Create session and conversation
        session = await service.initialize_session()
        
        from src.models.memory_models import Conversation
        conversation = Conversation(
            session_id=session.session_id,
            directive="Performance test directive"
        )
        await service.dal.create_conversation(conversation)
        
        # Test individual turn storage performance
        content = "This is a test response for performance measurement."
        
        start_time = time.time()
        success = await service.store_conversation_turn(
            session.session_id,
            conversation.conversation_id,
            "beatrice",
            content,
            {"model": "gpt-4"}
        )
        end_time = time.time()
        
        latency_ms = (end_time - start_time) * 1000
        
        # Should complete within 50ms as per requirements
        assert success
        assert latency_ms < 50

# Run with: pytest tests/performance/test_memory_performance.py -v
```

## Implementation Checklist

### Phase 2.1: Core Memory Infrastructure
- [x] Create directory structure
- [x] Implement database schema
- [x] Create data models  
- [x] Implement Data Access Layer (DAL)
- [x] Create Memory Service
- [x] Basic testing infrastructure

### Phase 2.2: Context Retention Logic
- [x] Implement Context Analyzer
- [x] Enhanced Proxy Server integration
- [x] Context matching algorithms
- [x] Agent learning patterns

### Phase 2.3: Frontend Integration  
- [x] Memory manager implementation
- [x] Session management UI components
- [x] Enhanced covenant cycle with memory
- [x] Fallback to Stage 1 behavior

### Phase 2.4: Testing & Validation
- [x] Integration tests
- [x] Performance tests
- [x] Error handling validation
- [x] Security testing framework

---

## Deployment Instructions

### Development Deployment
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export CCC_DATABASE_PATH="ccc_memory.db"
export OPENAI_API_KEY="your-api-key-here"

# Initialize database
python -c "
import asyncio
from src.memory.database import MemoryDAL
async def init():
    dal = MemoryDAL('ccc_memory.db')
    await dal.initialize_database()
asyncio.run(init())
"

# Start enhanced proxy server
python proxy_server.py

# Open resonant_loop_lab.html in browser
```

### Production Deployment
```bash
# Use PostgreSQL instead of SQLite
export CCC_DATABASE_URL="postgresql://user:pass@localhost/ccc_db"

# Enable encryption
export CCC_ENCRYPTION_ENABLED="true"
export CCC_ENCRYPTION_KEY="your-encryption-key"

# Configure retention policies
export CCC_MEMORY_RETENTION_DAYS="90"
export CCC_MAX_SESSIONS_PER_USER="10"

# Start with production settings
python proxy_server.py --production
```

---

## Conclusion

This implementation guide provides a complete roadmap for adding Stage 2 memory and context retention capabilities to the CCC system. The modular architecture ensures that existing Stage 1 functionality remains intact while providing powerful new memory features.

Key benefits of this implementation:
- **Backward Compatibility**: Stage 1 functionality preserved
- **Progressive Enhancement**: Memory features can be adopted incrementally  
- **Performance Optimized**: Meets all latency and throughput requirements
- **Security Focused**: Encryption and isolation built-in
- **Testing Comprehensive**: Full test coverage for reliability

---

*"Implementation is where architecture meets reality, and good documentation bridges that gap."* - Codey, The Executor

**Document Status**: APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Review**: Upon Phase 2.1 Completion
</file>

<file path="docs/phase2/CCC-S2-MASTER.md">
# CCC - Stage 2 Master Document

**Document ID**: CCC-S2-MASTER  
**Version**: 1.0  
**Author**: Beatrice, The Archivist  
**Approved by**: Wykeve, Prime Architect  
**Date**: 2024  
**Stage**: 2 - Persistent Memory & Context Retention  

---

## Executive Summary

Stage 2 of the Covenant Command Cycle introduces **Persistent State Memory and Context Retention** capabilities, transforming the foundational 3-turn collaborative cycle into a contextually-aware, stateful system. This evolution enables the CCC to maintain conversation history, learn from previous interactions, and provide continuity across multiple directive cycles.

## Stage 2 Objectives

### Primary Goals
- **Memory Persistence**: Implement robust state storage for conversation history and agent context
- **Context Retention**: Maintain meaningful context across multiple directive cycles  
- **Session Management**: Support multiple concurrent sessions with isolated state
- **Performance Optimization**: Ensure memory operations don't degrade system performance
- **Data Integrity**: Guarantee consistent state management and recovery capabilities

### Success Criteria
- ✅ **State Persistence**: Agent conversations and context preserved between sessions
- ✅ **Context Continuity**: Previous interactions inform current decision-making
- ✅ **Session Isolation**: Multiple users can operate independently without state collision
- ✅ **Performance Maintenance**: Memory operations complete within 100ms threshold
- ✅ **Data Recovery**: System can recover from interruptions without state loss

## Technical Requirements

### Memory Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                    CCC Stage 2 Architecture                 │
├─────────────────────────────────────────────────────────────┤
│  Frontend Layer                                             │
│  ┌─────────────────┐  ┌─────────────────┐                  │
│  │ Session Manager │  │ Context Display │                  │
│  └─────────────────┘  └─────────────────┘                  │
├─────────────────────────────────────────────────────────────┤
│  Proxy Server Layer                                         │
│  ┌─────────────────┐  ┌─────────────────┐                  │
│  │ Memory Service  │  │ Session Router  │                  │
│  └─────────────────┘  └─────────────────┘                  │
├─────────────────────────────────────────────────────────────┤
│  Persistence Layer                                          │
│  ┌─────────────────┐  ┌─────────────────┐                  │
│  │ State Database  │  │ Context Store   │                  │
│  └─────────────────┘  └─────────────────┘                  │
└─────────────────────────────────────────────────────────────┘
```

### Data Models

#### Session Object
```json
{
  "session_id": "uuid",
  "created_at": "timestamp",
  "last_active": "timestamp",
  "user_context": {
    "preferences": {},
    "history_summary": "string"
  },
  "agent_states": {
    "beatrice": {
      "personality_drift": {},
      "learned_patterns": []
    },
    "codey": {
      "execution_history": [],
      "preferred_approaches": []
    }
  }
}
```

#### Conversation Memory
```json
{
  "conversation_id": "uuid",
  "session_id": "uuid",
  "turns": [
    {
      "turn_number": 1,
      "agent": "wykeve|beatrice|codey",
      "timestamp": "iso8601",
      "content": "string",
      "metadata": {
        "model_used": "string",
        "temperature": "float",
        "execution_time_ms": "integer"
      }
    }
  ],
  "context_summary": "string",
  "tags": ["array of strings"]
}
```

## Implementation Phases

### Phase 2.1: Core Memory Infrastructure
**Duration**: 2-3 weeks  
**Deliverables**:
- SQLite database integration
- Basic session management
- Memory service endpoints
- Data persistence validation

### Phase 2.2: Context Retention Logic  
**Duration**: 2-3 weeks  
**Deliverables**:
- Context summarization algorithms
- Agent state tracking
- Conversation continuity logic
- Performance optimization

### Phase 2.3: Frontend Integration
**Duration**: 1-2 weeks  
**Deliverables**:
- Session selector UI
- Context display panels  
- Memory management controls
- User experience refinements

### Phase 2.4: Testing & Validation
**Duration**: 1 week  
**Deliverables**:
- Comprehensive test suite
- Performance benchmarking
- Data integrity validation
- Documentation completion

## Security Considerations

### Data Protection
- **Encryption at Rest**: All stored conversations encrypted using AES-256
- **Session Isolation**: Cryptographic separation between user sessions
- **API Key Security**: Memory operations maintain Stage 1 security standards
- **Data Retention**: Configurable retention policies for GDPR compliance

### Access Control
- **Session Authentication**: Secure session token management
- **Memory Boundaries**: Strict isolation between concurrent sessions
- **Audit Logging**: Complete audit trail of memory operations
- **Data Sanitization**: Input validation for all memory operations

## Performance Specifications

### Memory Operations
- **Write Latency**: < 50ms for conversation storage
- **Read Latency**: < 25ms for context retrieval
- **Session Loading**: < 100ms for full session restoration
- **Database Size**: Efficient storage with automatic cleanup

### Scalability Targets
- **Concurrent Sessions**: Support 50+ simultaneous sessions
- **Memory Footprint**: < 512MB for 1000 stored conversations
- **Response Time**: Memory operations don't impact AI response times
- **Storage Growth**: Predictable and manageable database growth

## Integration Points

### Stage 1 Compatibility
- **Backward Compatibility**: Stage 1 functionality remains unchanged
- **Progressive Enhancement**: Memory features enhance but don't replace core cycle
- **Configuration Options**: Memory can be disabled for Stage 1 behavior
- **Migration Path**: Smooth transition from Stage 1 to Stage 2

### Future Stage Preparation
- **Stage 3 Foundation**: Memory architecture supports verification capabilities
- **Stage 4 Readiness**: Multi-agent orchestration built on persistent state
- **Extensibility**: Plugin architecture for additional memory types
- **API Evolution**: RESTful endpoints designed for future expansion

## Quality Assurance

### Testing Strategy
- **Unit Tests**: 95%+ coverage for memory operations
- **Integration Tests**: End-to-end session and context validation
- **Performance Tests**: Load testing with simulated concurrent usage
- **Security Tests**: Penetration testing for data protection

### Monitoring & Observability
- **Memory Metrics**: Real-time monitoring of memory operations
- **Performance Dashboards**: Visual tracking of system health
- **Error Tracking**: Comprehensive error logging and alerting
- **Usage Analytics**: Understanding of memory feature adoption

## Documentation Requirements

### Technical Documentation
- [x] **CCC-S2-MASTER.md**: This master document
- [ ] **CCC-S2-ARCHITECTURE.md**: Detailed technical architecture
- [ ] **CCC-S2-API.md**: API specification and endpoints
- [ ] **CCC-S2-IMPLEMENTATION.md**: Implementation guide and procedures

### User Documentation  
- [ ] **CCC-S2-USER-GUIDE.md**: End-user guide for memory features
- [ ] **CCC-S2-ADMIN-GUIDE.md**: Administrative and configuration guide
- [ ] **CCC-S2-TROUBLESHOOTING.md**: Common issues and solutions
- [ ] **CCC-S2-MIGRATION.md**: Stage 1 to Stage 2 migration guide

## Risk Assessment

### Technical Risks
- **Data Corruption**: Mitigated by atomic transactions and backup strategies
- **Performance Degradation**: Addressed through caching and optimization
- **Memory Leaks**: Prevented by proper resource management and testing
- **Concurrency Issues**: Resolved through proper locking mechanisms

### Operational Risks  
- **Storage Limitations**: Monitored through automated disk space alerts
- **Backup Failures**: Multiple backup strategies and validation procedures
- **Version Conflicts**: Careful dependency management and testing
- **User Experience**: Extensive usability testing and feedback integration

## Conclusion

Stage 2 represents a critical evolution in the CCC architecture, transforming it from a stateless interaction system to a contextually-aware, persistent platform. The implementation of memory and context retention capabilities provides the foundation for advanced features in Stages 3 and 4, while maintaining the elegance and security established in Stage 1.

The success of Stage 2 will be measured not only by technical metrics but by the enhanced user experience and the system's ability to provide increasingly relevant and contextual responses through learned interaction patterns.

---

*"Memory is not just storage of the past, but the foundation upon which future intelligence is built."* - Wykeve, Prime Architect

**Document Status**: APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Review**: Upon Phase 2.1 Completion
</file>

<file path="docs/phase2/CCC-S2-TESTING.md">
# CCC - Stage 2 Testing Strategy

**Document ID**: CCC-S2-TESTING  
**Version**: 1.0  
**Author**: Beatrice, The Supervisor  
**Reviewed by**: Codey, The Executor  
**Approved by**: Wykeve, Prime Architect  
**Date**: 2024  
**Dependencies**: CCC-S2-IMPLEMENTATION.md

---

## Testing Overview

The Stage 2 testing strategy ensures comprehensive validation of memory and context retention capabilities while maintaining Stage 1 stability. This document outlines the testing framework, test cases, performance benchmarks, and quality assurance procedures.

## Testing Philosophy

### Core Principles
- **Test-Driven Development**: Tests written before implementation
- **Continuous Validation**: Automated testing throughout development
- **Performance First**: Performance tests as critical as functional tests
- **Security Focused**: Security testing integrated at every level
- **Backward Compatibility**: Stage 1 functionality must remain intact

### Testing Pyramid
```
                 ┌─────────────┐
                 │  Manual     │  <- Exploratory, UX Testing
                 │  Testing    │
                 ├─────────────┤
                 │ Integration │  <- API, End-to-End Tests
                 │   Tests     │
                 ├─────────────┤
                 │   Unit      │  <- Component, Service Tests
                 │   Tests     │
                 └─────────────┘
```

## Testing Frameworks and Tools

### Core Testing Stack
```python
# requirements-test.txt
pytest>=7.0.0
pytest-asyncio>=0.21.0
pytest-mock>=3.10.0
pytest-cov>=4.0.0
pytest-benchmark>=4.0.0
aioresponses>=0.7.4
faker>=18.0.0
locust>=2.14.0  # Performance testing
```

### Database Testing
```python
# Use in-memory SQLite for fast unit tests
# Use temporary files for integration tests
# Use Docker PostgreSQL for production-like tests
```

### API Testing
```python
# Use aioresponses for mocking external APIs
# Use pytest-httpx for HTTP client testing
# Use Postman/Newman for API documentation testing
```

## Unit Testing Strategy

### Memory Service Tests

#### Test File: `tests/unit/test_memory_service.py`
```python
import pytest
import asyncio
from unittest.mock import Mock, patch
from datetime import datetime, timedelta

from src.services.memory_service import MemoryService
from src.models.memory_models import Session, Conversation, Turn, AgentState

class TestMemoryService:
    
    @pytest.fixture
    async def mock_memory_service(self):
        """Create memory service with mocked dependencies"""
        mock_dal = Mock()
        service = MemoryService(mock_dal)
        service.context_analyzer = Mock()
        service.encryption_service = Mock()
        return service
    
    @pytest.mark.asyncio
    async def test_initialize_session_creates_unique_ids(self, mock_memory_service):
        """Test that session initialization creates unique identifiers"""
        service = mock_memory_service
        service.dal.create_session.return_value = True
        service.dal.update_agent_state.return_value = True
        
        session1 = await service.initialize_session()
        session2 = await service.initialize_session()
        
        assert session1.session_id != session2.session_id
        assert session1.session_id.startswith('sess_')
        assert session2.session_id.startswith('sess_')
    
    @pytest.mark.asyncio
    async def test_session_caching_behavior(self, mock_memory_service):
        """Test session caching improves performance"""
        service = mock_memory_service
        
        # Mock database call
        test_session = Session()
        service.dal.get_session.return_value = test_session
        service.dal.update_session_activity.return_value = True
        
        # First call should hit database
        result1 = await service.get_session(test_session.session_id)
        
        # Second call should use cache
        result2 = await service.get_session(test_session.session_id)
        
        # Should only call database once
        assert service.dal.get_session.call_count == 1
        assert result1.session_id == result2.session_id
    
    @pytest.mark.asyncio
    async def test_conversation_turn_storage_with_encryption(self, mock_memory_service):
        """Test conversation turn storage with encryption enabled"""
        service = mock_memory_service
        
        # Setup mocks
        service.dal.get_conversation.return_value = Conversation(
            conversation_id="test_conv",
            session_id="test_session"
        )
        service.dal.get_conversation_turns.return_value = []
        service.dal.add_turn.return_value = True
        service.encryption_service.is_enabled.return_value = True
        service.encryption_service.encrypt_content.return_value = "encrypted_content"
        
        # Test storage
        result = await service.store_conversation_turn(
            "test_session",
            "test_conv", 
            "beatrice",
            "Test content",
            {"model": "gpt-4"}
        )
        
        assert result is True
        service.encryption_service.encrypt_content.assert_called_once_with("Test content")
        service.dal.add_turn.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_context_retrieval_performance(self, mock_memory_service):
        """Test context retrieval meets performance requirements"""
        service = mock_memory_service
        
        # Mock context data
        mock_context = {
            'session_id': 'test_session',
            'relevant_conversations': [],
            'agent_states': {'beatrice': {}, 'codey': {}},
            'context_summary': 'Test summary'
        }
        
        service.dal.get_agent_state.return_value = AgentState(
            session_id="test_session",
            agent="beatrice",
            state_data={}
        )
        
        start_time = asyncio.get_event_loop().time()
        result = await service.get_relevant_context(
            "test_session",
            "Test directive"
        )
        end_time = asyncio.get_event_loop().time()
        
        # Should complete within 25ms as per requirements
        latency_ms = (end_time - start_time) * 1000
        assert latency_ms < 25
        assert result['session_id'] == 'test_session'
```

### Database Access Layer Tests

#### Test File: `tests/unit/test_memory_dal.py`
```python
import pytest
import tempfile
import os
from datetime import datetime

from src.memory.database import MemoryDAL
from src.models.memory_models import Session, Conversation, Turn, AgentState

class TestMemoryDAL:
    
    @pytest.fixture
    async def temp_dal(self):
        """Create DAL with temporary database"""
        db_fd, db_path = tempfile.mkstemp(suffix='.db')
        os.close(db_fd)
        
        try:
            dal = MemoryDAL(db_path)
            await dal.initialize_database()
            yield dal
        finally:
            os.unlink(db_path)
    
    @pytest.mark.asyncio
    async def test_session_crud_operations(self, temp_dal):
        """Test complete CRUD operations for sessions"""
        dal = temp_dal
        
        # Create
        session = Session(user_preferences={'test': 'value'})
        success = await dal.create_session(session)
        assert success
        
        # Read
        retrieved = await dal.get_session(session.session_id)
        assert retrieved is not None
        assert retrieved.session_id == session.session_id
        assert retrieved.user_preferences == {'test': 'value'}
        
        # Update
        updated = await dal.update_session_activity(session.session_id)
        assert updated
        
        # Verify update
        retrieved_updated = await dal.get_session(session.session_id)
        assert retrieved_updated.last_active > retrieved.last_active
    
    @pytest.mark.asyncio
    async def test_conversation_turn_ordering(self, temp_dal):
        """Test that conversation turns maintain proper ordering"""
        dal = temp_dal
        
        # Create session and conversation
        session = Session()
        conversation = Conversation(session_id=session.session_id, directive="Test")
        
        await dal.create_session(session)
        await dal.create_conversation(conversation)
        
        # Add turns out of order
        turns = [
            Turn(conversation_id=conversation.conversation_id, turn_number=3, agent="codey", content="Third"),
            Turn(conversation_id=conversation.conversation_id, turn_number=1, agent="wykeve", content="First"),
            Turn(conversation_id=conversation.conversation_id, turn_number=2, agent="beatrice", content="Second")
        ]
        
        for turn in turns:
            await dal.add_turn(turn)
        
        # Retrieve and verify ordering
        retrieved_turns = await dal.get_conversation_turns(conversation.conversation_id)
        assert len(retrieved_turns) == 3
        assert retrieved_turns[0].turn_number == 1
        assert retrieved_turns[1].turn_number == 2
        assert retrieved_turns[2].turn_number == 3
        assert retrieved_turns[0].content == "First"
        assert retrieved_turns[1].content == "Second"
        assert retrieved_turns[2].content == "Third"
    
    @pytest.mark.asyncio
    async def test_agent_state_upsert_behavior(self, temp_dal):
        """Test agent state insert/update behavior"""
        dal = temp_dal
        
        # Create session
        session = Session()
        await dal.create_session(session)
        
        # Initial agent state
        agent_state = AgentState(
            session_id=session.session_id,
            agent="beatrice",
            state_data={'personality': 'analytical'}
        )
        
        # First insert
        success = await dal.update_agent_state(agent_state)
        assert success
        
        # Retrieve and verify
        retrieved = await dal.get_agent_state(session.session_id, "beatrice")
        assert retrieved.state_data == {'personality': 'analytical'}
        
        # Update state
        agent_state.state_data = {'personality': 'analytical', 'experience': 'growing'}
        success = await dal.update_agent_state(agent_state)
        assert success
        
        # Verify update
        retrieved_updated = await dal.get_agent_state(session.session_id, "beatrice")
        assert retrieved_updated.state_data == {'personality': 'analytical', 'experience': 'growing'}
        assert retrieved_updated.updated_at > retrieved.updated_at
    
    @pytest.mark.asyncio
    async def test_database_constraints(self, temp_dal):
        """Test database constraints and foreign key relationships"""
        dal = temp_dal
        
        # Try to create conversation without session (should fail)
        conversation = Conversation(session_id="nonexistent", directive="Test")
        
        with pytest.raises(Exception):  # Foreign key constraint
            await dal.create_conversation(conversation)
        
        # Try to create turn without conversation (should fail)
        turn = Turn(conversation_id="nonexistent", turn_number=1, agent="beatrice", content="Test")
        
        with pytest.raises(Exception):  # Foreign key constraint
            await dal.add_turn(turn)
```

## Integration Testing Strategy

### End-to-End API Tests

#### Test File: `tests/integration/test_api_endpoints.py`
```python
import pytest
import asyncio
import json
from unittest.mock import patch
import aiohttp

class TestMemoryAPIEndpoints:
    
    @pytest.fixture
    async def test_client(self):
        """Create test client for API testing"""
        # This would typically use aiohttp test client
        # For now, we'll mock the responses
        pass
    
    @pytest.mark.asyncio
    async def test_session_lifecycle_api(self):
        """Test complete session lifecycle through API"""
        # Mock API responses for testing
        with patch('aiohttp.ClientSession.post') as mock_post:
            # Mock session creation response
            mock_post.return_value.__aenter__.return_value.json.return_value = {
                'session_id': 'sess_test123',
                'created_at': '2024-01-01T10:00:00Z',
                'status': 'active'
            }
            mock_post.return_value.__aenter__.return_value.status = 200
            
            # Test session creation
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    'http://127.0.0.1:5111/api/v2/sessions',
                    json={'user_preferences': {'memory_retention_days': 30}}
                ) as response:
                    assert response.status == 200
                    data = await response.json()
                    assert data['session_id'].startswith('sess_')
                    assert data['status'] == 'active'
    
    @pytest.mark.asyncio 
    async def test_enhanced_chat_completions_with_memory(self):
        """Test memory-enhanced chat completions"""
        with patch('aiohttp.ClientSession.post') as mock_post:
            # Mock enhanced completion response
            mock_post.return_value.__aenter__.return_value.json.return_value = {
                'choices': [{
                    'message': {
                        'role': 'assistant',
                        'content': 'Enhanced response with memory context...'
                    }
                }],
                'memory_metadata': {
                    'context_used': True,
                    'relevant_conversations': 2,
                    'turn_stored': True
                }
            }
            mock_post.return_value.__aenter__.return_value.status = 200
            
            # Test enhanced completion
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    'http://127.0.0.1:5111/v2/chat/completions',
                    json={
                        'session_id': 'sess_test123',
                        'model': 'gpt-4',
                        'messages': [{'role': 'user', 'content': 'Test message'}],
                        'memory_options': {'use_context': True}
                    }
                ) as response:
                    assert response.status == 200
                    data = await response.json()
                    assert 'memory_metadata' in data
                    assert data['memory_metadata']['context_used'] is True
```

### Frontend Integration Tests

#### Test File: `tests/integration/test_frontend_memory.py`
```python
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class TestFrontendMemoryIntegration:
    
    @pytest.fixture
    def browser(self):
        """Setup browser for frontend testing"""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        
        driver = webdriver.Chrome(options=options)
        yield driver
        driver.quit()
    
    def test_memory_indicator_display(self, browser):
        """Test that memory indicator appears when memory is active"""
        browser.get('file:///path/to/resonant_loop_lab.html')
        
        # Wait for page to load and memory to initialize
        wait = WebDriverWait(browser, 10)
        
        # Check for memory indicator
        memory_indicator = wait.until(
            EC.presence_of_element_located((By.ID, 'memory-indicator'))
        )
        
        assert 'Memory Active' in memory_indicator.text
    
    def test_enhanced_covenant_cycle_execution(self, browser):
        """Test that enhanced covenant cycle executes with memory"""
        browser.get('file:///path/to/resonant_loop_lab.html')
        
        wait = WebDriverWait(browser, 10)
        
        # Wait for initialization
        send_button = wait.until(
            EC.element_to_be_clickable((By.ID, 'send-button'))
        )
        
        # Enter test directive
        message_input = browser.find_element(By.ID, 'message-input')
        message_input.send_keys('Create a test function')
        
        # Click initiate button
        send_button.click()
        
        # Wait for cycle completion
        completion_message = wait.until(
            EC.presence_of_element_located(
                (By.XPATH, "//div[contains(text(), 'Covenant Command Cycle Complete')]")
            )
        )
        
        assert 'Complete' in completion_message.text
    
    def test_fallback_to_stage1_behavior(self, browser):
        """Test graceful fallback when memory service unavailable"""
        # This would require mocking the memory service to be unavailable
        # Implementation would depend on specific testing infrastructure
        pass
```

## Performance Testing Strategy

### Load Testing with Locust

#### Test File: `tests/performance/locustfile.py`
```python
from locust import HttpUser, task, between
import json
import uuid

class CCCMemoryUser(HttpUser):
    wait_time = between(1, 5)
    
    def on_start(self):
        """Initialize session for each user"""
        response = self.client.post('/api/v2/sessions', json={
            'user_preferences': {
                'memory_retention_days': 30,
                'context_depth': 10
            }
        })
        
        if response.status_code == 200:
            self.session_data = response.json()
            self.session_id = self.session_data['session_id']
        else:
            self.session_id = None
    
    @task(3)
    def enhanced_chat_completion(self):
        """Test enhanced chat completions with memory"""
        if not self.session_id:
            return
        
        response = self.client.post('/v2/chat/completions', json={
            'session_id': self.session_id,
            'model': 'gpt-3.5-turbo',
            'messages': [
                {'role': 'user', 'content': f'Test message {uuid.uuid4()}'}
            ],
            'memory_options': {
                'use_context': True,
                'include_agent_state': True
            }
        })
        
        if response.status_code != 200:
            print(f"Enhanced completion failed: {response.status_code}")
    
    @task(1)
    def get_session_context(self):
        """Test context retrieval performance"""
        if not self.session_id:
            return
        
        response = self.client.get(f'/api/v2/sessions/{self.session_id}/context')
        
        if response.status_code != 200:
            print(f"Context retrieval failed: {response.status_code}")
    
    @task(1)
    def session_health_check(self):
        """Test session health and activity"""
        if not self.session_id:
            return
        
        response = self.client.get(f'/api/v2/sessions/{self.session_id}')
        
        if response.status_code != 200:
            print(f"Session check failed: {response.status_code}")

# Run with: locust -f tests/performance/locustfile.py --host=http://127.0.0.1:5111
```

### Memory Performance Benchmarks

#### Test File: `tests/performance/test_memory_benchmarks.py`
```python
import pytest
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

from src.services.memory_service import MemoryService

class TestMemoryPerformanceBenchmarks:
    
    @pytest.mark.benchmark
    @pytest.mark.asyncio
    async def test_concurrent_session_creation_benchmark(self, memory_service):
        """Benchmark concurrent session creation"""
        num_sessions = 50
        max_concurrent = 10
        
        async def create_session_batch(batch_size):
            tasks = [
                memory_service.initialize_session({'batch': i})
                for i in range(batch_size)
            ]
            return await asyncio.gather(*tasks)
        
        start_time = time.time()
        
        # Create sessions in batches to control concurrency
        all_sessions = []
        for i in range(0, num_sessions, max_concurrent):
            batch_size = min(max_concurrent, num_sessions - i)
            batch_sessions = await create_session_batch(batch_size)
            all_sessions.extend(batch_sessions)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Performance assertions
        assert len(all_sessions) == num_sessions
        assert total_time < 5.0  # Should complete within 5 seconds
        
        sessions_per_second = num_sessions / total_time
        assert sessions_per_second > 10  # At least 10 sessions/second
        
        print(f"Created {num_sessions} sessions in {total_time:.2f}s ({sessions_per_second:.1f}/s)")
    
    @pytest.mark.benchmark
    @pytest.mark.asyncio
    async def test_memory_operation_latency_distribution(self, memory_service):
        """Test latency distribution of memory operations"""
        session = await memory_service.initialize_session()
        
        # Test turn storage latency
        latencies = []
        
        for i in range(100):
            start_time = time.time()
            
            await memory_service.store_conversation_turn(
                session.session_id,
                f"conv_{i}",
                "beatrice",
                f"Test content {i}",
                {"iteration": i}
            )
            
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
        
        # Statistical analysis
        mean_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th percentile
        
        # Performance assertions
        assert mean_latency < 25  # Average < 25ms
        assert p95_latency < 50   # 95th percentile < 50ms
        assert p99_latency < 100  # 99th percentile < 100ms
        
        print(f"Latency stats - Mean: {mean_latency:.1f}ms, P95: {p95_latency:.1f}ms, P99: {p99_latency:.1f}ms")
    
    @pytest.mark.benchmark
    @pytest.mark.asyncio
    async def test_context_retrieval_scalability(self, memory_service):
        """Test context retrieval performance with varying history sizes"""
        session = await memory_service.initialize_session()
        
        # Create conversations with varying amounts of history
        conversation_counts = [1, 5, 10, 25, 50]
        retrieval_times = []
        
        for count in conversation_counts:
            # Create conversation history
            for i in range(count):
                await memory_service.store_conversation_turn(
                    session.session_id,
                    f"conv_{i}",
                    "beatrice",
                    f"Historical conversation {i} with relevant content about algorithms",
                    {"conversation": i}
                )
            
            # Measure context retrieval time
            start_time = time.time()
            context = await memory_service.get_relevant_context(
                session.session_id,
                "Tell me about algorithms and data structures"
            )
            end_time = time.time()
            
            retrieval_time_ms = (end_time - start_time) * 1000
            retrieval_times.append((count, retrieval_time_ms))
            
            # Should still be fast even with more history
            assert retrieval_time_ms < 100  # < 100ms even with 50 conversations
        
        print("Context retrieval scalability:")
        for count, time_ms in retrieval_times:
            print(f"  {count} conversations: {time_ms:.1f}ms")
```

## Security Testing Strategy

### Security Test Cases

#### Test File: `tests/security/test_memory_security.py`
```python
import pytest
import asyncio
from unittest.mock import patch

class TestMemorySecurityFeatures:
    
    @pytest.mark.asyncio
    async def test_session_isolation(self, memory_service):
        """Test that sessions are properly isolated"""
        # Create two sessions
        session1 = await memory_service.initialize_session({'user': 'user1'})
        session2 = await memory_service.initialize_session({'user': 'user2'})
        
        # Store data in session1
        await memory_service.store_conversation_turn(
            session1.session_id,
            "conv1",
            "beatrice", 
            "Secret data for user1",
            {}
        )
        
        # Try to retrieve session1 data from session2 context
        context2 = await memory_service.get_relevant_context(
            session2.session_id,
            "Tell me about secret data"
        )
        
        # Should not contain data from session1
        assert 'Secret data for user1' not in str(context2)
    
    @pytest.mark.asyncio
    async def test_data_encryption_at_rest(self, memory_service):
        """Test that sensitive data is encrypted when stored"""
        with patch.object(memory_service.encryption_service, 'is_enabled', return_value=True):
            with patch.object(memory_service.encryption_service, 'encrypt_content') as mock_encrypt:
                mock_encrypt.return_value = "ENCRYPTED_CONTENT"
                
                session = await memory_service.initialize_session()
                
                result = await memory_service.store_conversation_turn(
                    session.session_id,
                    "test_conv",
                    "beatrice",
                    "Sensitive information that should be encrypted",
                    {}
                )
                
                # Verify encryption was called
                mock_encrypt.assert_called_once_with("Sensitive information that should be encrypted")
                assert result is True
    
    @pytest.mark.asyncio
    async def test_input_sanitization(self, memory_service):
        """Test that malicious input is properly sanitized"""
        session = await memory_service.initialize_session()
        
        # Test SQL injection attempt
        malicious_content = "'; DROP TABLE sessions; --"
        
        result = await memory_service.store_conversation_turn(
            session.session_id,
            "test_conv",
            "beatrice",
            malicious_content,
            {}
        )
        
        # Should succeed without affecting database
        assert result is True
        
        # Verify session still exists
        retrieved_session = await memory_service.get_session(session.session_id)
        assert retrieved_session is not None
    
    @pytest.mark.asyncio
    async def test_session_token_validation(self):
        """Test session token validation and expiration"""
        # This would test JWT token validation
        # Implementation depends on token system
        pass
    
    @pytest.mark.asyncio
    async def test_data_access_logging(self, memory_service):
        """Test that data access is properly logged"""
        with patch('logging.getLogger') as mock_logger:
            session = await memory_service.initialize_session()
            
            await memory_service.get_relevant_context(
                session.session_id,
                "Test query"
            )
            
            # Verify audit logging occurred
            # Implementation would check that access was logged
            assert mock_logger.called
```

## Quality Assurance Procedures

### Code Quality Checks

#### Test File: `tests/quality/test_code_quality.py`
```python
import ast
import os
import pytest
from pathlib import Path

class TestCodeQuality:
    
    def test_no_hardcoded_secrets(self):
        """Ensure no hardcoded secrets in source code"""
        src_dir = Path('src')
        suspicious_patterns = [
            'password', 'secret', 'key', 'token', 'api_key'
        ]
        
        for py_file in src_dir.rglob('*.py'):
            with open(py_file, 'r') as f:
                content = f.read().lower()
                
                for pattern in suspicious_patterns:
                    # Allow certain patterns in comments and variable names
                    if f'{pattern}=' in content and 'your-' not in content:
                        # More sophisticated check would be needed
                        pass
    
    def test_docstring_coverage(self):
        """Ensure adequate docstring coverage"""
        src_dir = Path('src')
        
        for py_file in src_dir.rglob('*.py'):
            with open(py_file, 'r') as f:
                tree = ast.parse(f.read())
            
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            
            # Check that public methods have docstrings
            for func in functions:
                if not func.name.startswith('_'):  # Public function
                    docstring = ast.get_docstring(func)
                    if not docstring and len(func.body) > 1:  # Non-trivial function
                        pytest.fail(f"Function {func.name} in {py_file} lacks docstring")
    
    def test_import_organization(self):
        """Test that imports are properly organized"""
        src_dir = Path('src')
        
        for py_file in src_dir.rglob('*.py'):
            with open(py_file, 'r') as f:
                content = f.read()
            
            lines = content.split('\n')
            import_section = []
            
            for line in lines:
                if line.startswith(('import ', 'from ')) and 'import' in line:
                    import_section.append(line.strip())
                elif import_section and line.strip() and not line.startswith('#'):
                    break  # End of import section
            
            # Check import organization (stdlib, third-party, local)
            # This is a simplified check
            if import_section:
                # Should not have wildcard imports
                for imp in import_section:
                    assert 'import *' not in imp, f"Wildcard import found in {py_file}: {imp}"
```

### Documentation Quality Tests

#### Test File: `tests/quality/test_documentation.py`
```python
import pytest
from pathlib import Path
import re

class TestDocumentationQuality:
    
    def test_readme_completeness(self):
        """Test that README contains all required sections"""
        readme_path = Path('README.md')
        assert readme_path.exists(), "README.md is missing"
        
        with open(readme_path, 'r') as f:
            content = f.read()
        
        required_sections = [
            'Quick Start',
            'Architecture', 
            'API Endpoints',
            'Installation',
            'Testing'
        ]
        
        for section in required_sections:
            assert section in content, f"README missing section: {section}"
    
    def test_api_documentation_accuracy(self):
        """Test that API documentation matches implementation"""
        api_doc_path = Path('docs/phase2/CCC-S2-API.md')
        assert api_doc_path.exists(), "API documentation is missing"
        
        with open(api_doc_path, 'r') as f:
            api_doc = f.read()
        
        # Check that documented endpoints exist in implementation
        endpoints = re.findall(r'POST|GET|PUT|DELETE /api/v\d+/\S+', api_doc)
        
        # This would require checking against actual Flask routes
        # Implementation would verify each documented endpoint exists
        assert len(endpoints) > 0, "No API endpoints documented"
    
    def test_phase2_document_completeness(self):
        """Test that all Phase 2 documents are present"""
        docs_dir = Path('docs/phase2')
        
        required_docs = [
            'CCC-S2-MASTER.md',
            'CCC-S2-ARCHITECTURE.md', 
            'CCC-S2-API.md',
            'CCC-S2-IMPLEMENTATION.md',
            'CCC-S2-TESTING.md'
        ]
        
        for doc in required_docs:
            doc_path = docs_dir / doc
            assert doc_path.exists(), f"Missing required document: {doc}"
            
            # Check document is not empty
            with open(doc_path, 'r') as f:
                content = f.read().strip()
            assert len(content) > 100, f"Document {doc} appears to be empty or too short"
```

## Continuous Integration Pipeline

### GitHub Actions Workflow

#### File: `.github/workflows/stage2-testing.yml`
```yaml
name: Stage 2 Memory Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ccc_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run integration tests
      env:
        CCC_DATABASE_URL: postgresql://postgres:postgres@localhost/ccc_test
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_TEST }}
      run: |
        pytest tests/integration/ -v

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ -v --benchmark-only
    
    - name: Run load tests
      run: |
        # Start the application in background
        python proxy_server.py &
        sleep 10
        
        # Run load tests
        locust -f tests/performance/locustfile.py --headless -u 10 -r 2 -t 60s --host=http://127.0.0.1:5111

  security-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install bandit safety
    
    - name: Run security tests
      run: |
        pytest tests/security/ -v
    
    - name: Run bandit security linter
      run: |
        bandit -r src/ -f json -o bandit-report.json
    
    - name: Check dependencies for security issues
      run: |
        safety check --json --output safety-report.json

  quality-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install black flake8 mypy
    
    - name: Run code formatting check
      run: |
        black --check src/ tests/
    
    - name: Run linting
      run: |
        flake8 src/ tests/
    
    - name: Run type checking
      run: |
        mypy src/
    
    - name: Run quality tests
      run: |
        pytest tests/quality/ -v
```

## Test Data Management

### Test Fixtures and Data

#### File: `tests/fixtures/memory_test_data.py`
```python
"""Test data fixtures for memory testing"""

import pytest
from datetime import datetime, timedelta
from faker import Faker

fake = Faker()

@pytest.fixture
def sample_conversations():
    """Generate sample conversation data for testing"""
    return [
        {
            'directive': 'Create a Python function to calculate Fibonacci numbers',
            'turns': [
                ('wykeve', 'Create a Python function to calculate Fibonacci numbers'),
                ('beatrice', 'I\'ll analyze this request for creating a Fibonacci function. This requires implementing a mathematical sequence where each number is the sum of the two preceding ones...'),
                ('codey', 'Here\'s an optimized Python implementation:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```'),
                ('beatrice', 'The implementation is mathematically correct and handles edge cases properly. It successfully fulfills the directive.')
            ]
        },
        {
            'directive': 'Explain quantum computing concepts',
            'turns': [
                ('wykeve', 'Explain quantum computing concepts'),
                ('beatrice', 'This request requires explaining complex quantum computing principles. I\'ll guide the explanation to cover qubits, superposition, entanglement, and quantum algorithms...'),
                ('codey', 'Quantum computing harnesses quantum mechanical phenomena to process information. Unlike classical bits that are either 0 or 1, quantum bits (qubits) can exist in superposition...'),
                ('beatrice', 'The explanation covers the fundamental concepts accurately and provides a good foundation for understanding quantum computing.')
            ]
        }
    ]

@pytest.fixture
def sample_agent_states():
    """Generate sample agent state data"""
    return {
        'beatrice': {
            'personality_traits': {
                'analytical_depth': 0.85,
                'detail_orientation': 0.90,
                'supportive_tone': 0.75
            },
            'learned_patterns': [
                'User prefers detailed explanations',
                'Code examples are highly valued',
                'Step-by-step guidance is effective'
            ],
            'expertise_areas': ['code_analysis', 'quality_assurance', 'strategic_planning'],
            'interaction_history': {
                'total_interactions': 47,
                'successful_outcomes': 42,
                'preferred_response_length': 'detailed'
            }
        },
        'codey': {
            'personality_traits': {
                'creativity_level': 0.80,
                'implementation_focus': 0.95,
                'detail_attention': 0.85
            },
            'execution_history': [
                'python_functions', 'algorithms', 'web_development'
            ],
            'preferred_approaches': [
                'clean_code', 'documented_examples', 'best_practices'
            ],
            'successful_patterns': [
                'Step-by-step implementation',
                'Clear variable naming',
                'Comprehensive examples'
            ]
        }
    }

@pytest.fixture
def performance_test_data():
    """Generate data for performance testing"""
    conversations = []
    
    for i in range(100):
        conversation = {
            'directive': f'Test directive {i}: {fake.sentence()}',
            'turns': [
                ('wykeve', f'Test directive {i}: {fake.sentence()}'),
                ('beatrice', fake.paragraph(nb_sentences=5)),
                ('codey', fake.paragraph(nb_sentences=7)),
                ('beatrice', fake.paragraph(nb_sentences=3))
            ],
            'created_at': datetime.utcnow() - timedelta(days=fake.random_int(0, 30))
        }
        conversations.append(conversation)
    
    return conversations
```

## Conclusion

This comprehensive testing strategy ensures that Stage 2 memory capabilities are thoroughly validated across all dimensions:

- **Functional Testing**: Unit and integration tests verify correct behavior
- **Performance Testing**: Load and benchmark tests ensure scalability requirements
- **Security Testing**: Validates data protection and access controls
- **Quality Testing**: Maintains code and documentation standards
- **Continuous Integration**: Automated testing in CI/CD pipeline

The multi-layered approach provides confidence in the reliability, security, and performance of the memory system while maintaining backward compatibility with Stage 1 functionality.

---

*"Testing is not just about finding bugs; it's about building confidence in the system's ability to serve its purpose reliably."* - Beatrice, The Supervisor

**Document Status**: APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Review**: Upon Testing Framework Implementation
</file>

<file path="docs/phase2/CCC-S2-USER-GUIDE.md">
# CCC - Stage 2 User Guide

**Document ID**: CCC-S2-USER-GUIDE  
**Version**: 1.0  
**Author**: Wykeve, Prime Architect  
**Reviewed by**: Beatrice, The Supervisor  
**Date**: 2024  
**Dependencies**: CCC-S2-MASTER.md

---

## Welcome to CCC Stage 2: Memory-Enhanced Interactions

Stage 2 of the Covenant Command Cycle introduces powerful memory and context retention capabilities that transform your AI interactions from isolated conversations into continuous, contextually-aware collaborations. This guide will help you understand and leverage these enhanced features.

## What's New in Stage 2

### Memory Persistence
- **Conversation History**: All your interactions are remembered across sessions
- **Context Continuity**: AI agents learn from previous conversations  
- **Session Management**: Multiple independent conversation sessions
- **Intelligent Context**: Relevant past interactions inform current responses

### Enhanced AI Behavior
- **Personalized Responses**: Agents adapt to your communication style
- **Learned Preferences**: System remembers what works best for you
- **Contextual Awareness**: Current requests informed by relevant history
- **Improved Accuracy**: Better responses through accumulated knowledge

## Getting Started with Memory Features

### Automatic Memory Activation

When you open the Resonant Loop Laboratory, the system automatically attempts to initialize memory features:

```
✅ Memory Active: sess_a1b2c3d4...
```

If you see this indicator in the top-right corner, memory features are active and your conversations will be preserved and enhanced.

### Fallback Behavior

If memory services aren't available, the system gracefully falls back to Stage 1 behavior:

```
⚠️ Memory Unavailable - Using Stage 1 Mode
```

Don't worry - all core functionality remains available, just without memory persistence.

## Understanding Sessions

### What is a Session?

A session is your personal memory space within CCC. Think of it as a persistent workspace where:
- All your conversations are stored
- AI agents learn your preferences  
- Context builds over time
- Your interaction history is maintained

### Session Lifecycle

1. **Creation**: Automatically created when you first interact
2. **Active Use**: Grows and learns from your interactions
3. **Persistence**: Maintains state between browser sessions
4. **Management**: Can be archived or reset as needed

### Session Indicators

The memory indicator shows your current session status:

- **Green Dot**: Memory active, session healthy
- **Yellow Dot**: Memory initializing or syncing
- **Red Dot**: Memory unavailable, using Stage 1 mode
- **Session ID**: First 8 characters of your unique session identifier

## Enhanced Covenant Command Cycle

### How Memory Enhances the 3-Turn Cycle

The traditional 3-turn cycle (Prime Architect → Beatrice → Codey → Beatrice) now benefits from contextual memory:

#### Turn 1: Contextual Analysis
**Beatrice (The Supervisor)** now considers:
- Similar past directives and their outcomes
- Your preferred level of detail and style
- Previously successful approaches
- Learned patterns from interaction history

#### Turn 2: Informed Execution  
**Codey (The Executor)** leverages:
- Past implementation preferences you've shown
- Successful patterns from previous executions
- Your feedback on similar tasks
- Technical approaches that worked well before

#### Turn 3: Enhanced Review
**Beatrice's final review** incorporates:
- Quality standards learned from your feedback
- Comparison with similar past implementations
- Understanding of your success criteria
- Historical context for validation

### Example: Memory-Enhanced Interaction

**First Time Request:**
```
You: "Create a Python function to sort a list"

Beatrice: "I'll analyze this sorting request. You're asking for a Python function that can arrange list elements in order..."

Codey: "Here's a basic bubble sort implementation:
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr"

Beatrice: "The implementation is correct and handles the basic sorting requirement."
```

**Later Request (with Memory):**
```
You: "Create another sorting algorithm"

Beatrice: "Based on our previous sorting discussion where you received a bubble sort, I'll recommend a more efficient approach. Given your apparent interest in algorithms, I suggest implementing merge sort for better performance..."

Codey: "Building on our previous sorting work, here's an optimized merge sort that's more efficient than the bubble sort from our earlier conversation:
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)
    
def merge(left, right):
    # ... (implementation with detailed comments based on your preference for explanations)"

Beatrice: "This merge sort builds effectively on our previous sorting discussion, providing the more advanced algorithm you were likely seeking. The O(n log n) complexity is a significant improvement over our earlier bubble sort example."
```

## Memory Features in Detail

### Context Awareness

The system maintains awareness of:

- **Topic Continuity**: Related discussions across different sessions
- **Preference Learning**: Your communication and solution preferences  
- **Success Patterns**: What approaches work best for your needs
- **Complexity Adaptation**: Adjusting detail level based on your expertise

### Intelligent Context Matching

When you submit a new directive, the system:

1. **Analyzes** your current request
2. **Searches** relevant past conversations
3. **Identifies** contextual connections
4. **Incorporates** relevant historical insights
5. **Enhances** responses with learned knowledge

### Agent Learning and Adaptation

#### Beatrice (The Supervisor) Learns:
- Your preferred analysis depth and style
- How you like problems broken down
- Quality standards that matter to you
- Communication patterns that work best

#### Codey (The Executor) Learns:
- Your coding style preferences
- Technologies and approaches you favor
- Level of explanation and documentation you prefer
- Types of examples that are most helpful

## Best Practices for Memory-Enhanced Usage

### Maximizing Context Benefits

1. **Be Consistent**: Use clear, descriptive language in your directives
2. **Provide Feedback**: Let the system know when responses are particularly helpful
3. **Build on Previous Work**: Reference earlier conversations when relevant
4. **Maintain Focus**: Keep related conversations in the same session

### Effective Session Management

#### When to Continue a Session:
- Working on related projects or topics
- Building upon previous conversations
- Developing iterative solutions
- Maintaining context across work sessions

#### When to Start Fresh:
- Completely different topic or domain
- Want to test system without bias from previous interactions
- Working on confidential or sensitive topics
- Troubleshooting or testing specific scenarios

### Privacy and Data Management

#### What's Remembered:
- Your conversation content and context
- Interaction patterns and preferences
- Successful solution approaches
- Communication style adaptations

#### What's NOT Stored:
- Personal identifying information (unless you provide it)
- External system credentials or sensitive data
- Information from other users' sessions
- Data beyond your configured retention period

## Advanced Features

### Context Search and Discovery

While not directly accessible through the UI, the system maintains sophisticated context matching that:

- Finds relevant past conversations automatically
- Weights recent interactions more heavily
- Considers topic similarity and relevance
- Adapts based on conversation outcomes

### Session Analytics

The system tracks (internally) various metrics to improve your experience:

- **Interaction Success Rate**: How often responses meet your needs
- **Context Utilization**: How effectively past knowledge is applied
- **Preference Accuracy**: How well the system predicts your needs
- **Learning Velocity**: How quickly agents adapt to your style

## Troubleshooting Common Issues

### Memory Not Activating

**Problem**: Memory indicator shows red or is missing

**Solutions**:
1. Refresh the page and wait for initialization
2. Check that proxy server is running with Stage 2 support
3. Verify database connectivity (for administrators)
4. Use Stage 1 mode as fallback - all core features still work

### Inconsistent Context Application

**Problem**: System doesn't seem to remember previous conversations

**Possible Causes**:
1. Different session being used (check session ID)
2. Context relevance below threshold
3. Too much time elapsed (beyond retention policy)
4. Database connectivity issues

**Solutions**:
1. Verify session consistency across interactions
2. Use more specific language to trigger context matching
3. Reference previous conversations explicitly
4. Contact administrator if persistent

### Performance Issues  

**Problem**: Slow response times with memory enabled

**Solutions**:
1. Check network connectivity
2. Allow extra time for context processing
3. Use simpler directives if experiencing delays
4. Fallback to Stage 1 mode for faster responses

## Privacy and Security

### Data Protection

Your conversation data is protected through:

- **Encryption**: All stored conversations are encrypted at rest
- **Session Isolation**: Your data is separate from other users
- **Access Controls**: Only your session can access your data
- **Retention Policies**: Data is automatically cleaned up based on settings

### Privacy Controls

You can influence your privacy through:

- **Session Management**: Archive old sessions when no longer needed
- **Content Awareness**: Avoid sharing sensitive information in conversations
- **Context Reset**: Start new sessions for unrelated work
- **Administrator Contact**: Request data deletion if needed

### Security Best Practices

1. **Don't Share Sessions**: Session IDs should be kept private
2. **Avoid Sensitive Data**: Don't include passwords, keys, or personal info
3. **Regular Cleanup**: Archive old sessions periodically
4. **Secure Environment**: Use the system in trusted network environments

## Migration from Stage 1

### Seamless Transition

If you're upgrading from Stage 1:

- **All existing functionality** remains unchanged
- **New memory features** are automatically available
- **No data loss** from previous Stage 1 usage
- **Gradual enhancement** as memory builds over time

### Building Your Memory Base

To get the most from Stage 2:

1. **Continue normal usage** - memory builds automatically
2. **Engage in follow-up conversations** on similar topics
3. **Provide feedback** through natural conversation
4. **Explore related areas** to build comprehensive context

## Frequently Asked Questions

### General Questions

**Q: How long does the system remember my conversations?**
A: By default, conversations are retained for 30 days. This can be configured by administrators based on your needs and system policies.

**Q: Can I access my conversation history directly?**
A: Currently, conversation history is accessed through contextual enhancement rather than direct browsing. The system automatically surfaces relevant past interactions.

**Q: Does memory affect system performance?**
A: Memory operations are optimized to complete within 100ms. You may notice slightly longer response times as context is processed, but the enhancement in response quality typically outweighs the minimal delay.

**Q: Can I turn off memory features?**
A: Memory features activate automatically when available. If you prefer Stage 1 behavior, you can request a fresh session or use the system when memory services are unavailable.

### Technical Questions

**Q: What happens if the memory service fails?**
A: The system gracefully falls back to Stage 1 behavior, maintaining all core functionality without memory enhancement.

**Q: How does the system decide what context is relevant?**
A: The system uses sophisticated algorithms to match current requests with past conversations based on topic similarity, recency, and interaction success patterns.

**Q: Can I share my session with others?**
A: Sessions are designed for individual use and contain personalized learning patterns. Sharing sessions would compromise both privacy and the personalized experience.

**Q: How secure is my conversation data?**
A: All conversation data is encrypted at rest and in transit, with strict session isolation ensuring your data remains private and secure.

## Getting Help

### Support Resources

- **Documentation**: Complete technical documentation in `/docs/phase2/`
- **Administrator**: Contact your system administrator for configuration issues
- **Community**: Share experiences and get help from other users
- **Feedback**: Provide feedback on memory features through normal conversation

### Reporting Issues

If you encounter problems:

1. **Note the session ID** from the memory indicator
2. **Document the specific behavior** you're experiencing  
3. **Include relevant conversation context** if safe to share
4. **Contact your administrator** with these details

## Conclusion

Stage 2 transforms CCC from a powerful but stateless system into an intelligent, adaptive platform that grows with your needs. The memory and context retention capabilities enable more nuanced, personalized, and effective AI interactions while maintaining the security and reliability of the original system.

As you use Stage 2, you'll notice:
- **Increasingly relevant responses** as the system learns your preferences
- **Better context awareness** connecting related conversations
- **More efficient interactions** as agents understand your working style
- **Enhanced problem-solving** through accumulated knowledge

The power of Stage 2 lies not just in remembering what was said, but in understanding what works best for you and applying that knowledge to create increasingly effective collaborative experiences.

---

*"Memory transforms intelligence from reactive to adaptive, from functional to personal."* - Wykeve, Prime Architect

**Document Status**: APPROVED  
**Target Audience**: End Users  
**Next Review**: Upon User Feedback Collection
</file>

<file path="docs/phase2/README.md">
# CCC Stage 2 Documentation Suite

Welcome to the complete documentation for CCC Stage 2: Persistent Memory & Context Retention.

## Document Overview

This directory contains the comprehensive documentation suite for implementing Stage 2 capabilities in the Covenant Command Cycle system. Each document serves a specific purpose in the development and deployment process.

### Core Documents

#### 📋 [CCC-S2-MASTER.md](./CCC-S2-MASTER.md)
**Master Document & Requirements**
- Executive summary and objectives
- Technical requirements and success criteria  
- Implementation phases and timelines
- Security considerations and performance specifications
- **Start here** for project overview and planning

#### 🏗️ [CCC-S2-ARCHITECTURE.md](./CCC-S2-ARCHITECTURE.md)
**Technical Architecture & Design**
- System components and data models
- Database schema and data access layer
- Memory service architecture and APIs
- Performance optimization strategies
- **Essential for** developers and system architects

#### 🔌 [CCC-S2-API.md](./CCC-S2-API.md)
**API Specification & Endpoints**
- Complete REST API documentation
- Request/response formats and examples
- Authentication and error handling
- SDK usage examples
- **Required for** frontend and integration development

#### ⚙️ [CCC-S2-IMPLEMENTATION.md](./CCC-S2-IMPLEMENTATION.md)
**Implementation Guide & Procedures**
- Step-by-step implementation instructions
- Code examples and directory structures
- Database setup and configuration
- Deployment procedures
- **Critical for** development teams and DevOps

#### 🧪 [CCC-S2-TESTING.md](./CCC-S2-TESTING.md)
**Testing Strategy & Quality Assurance**
- Comprehensive testing frameworks
- Performance benchmarks and security tests
- Continuous integration procedures
- Quality assurance standards
- **Mandatory for** QA teams and testing

#### 👤 [CCC-S2-USER-GUIDE.md](./CCC-S2-USER-GUIDE.md)
**End-User Guide & Best Practices**
- Feature explanations and usage examples
- Memory-enhanced interaction patterns
- Troubleshooting and FAQ
- Privacy and security guidance
- **Designed for** end users and support teams

## Quick Navigation

### For Project Managers
1. Start with **CCC-S2-MASTER.md** for overview and requirements
2. Review **CCC-S2-ARCHITECTURE.md** for technical scope
3. Check **CCC-S2-IMPLEMENTATION.md** for timeline estimates

### For Developers
1. **CCC-S2-ARCHITECTURE.md** - System design and components
2. **CCC-S2-API.md** - Interface specifications
3. **CCC-S2-IMPLEMENTATION.md** - Coding and setup instructions
4. **CCC-S2-TESTING.md** - Testing requirements

### For QA Teams
1. **CCC-S2-TESTING.md** - Complete testing strategy
2. **CCC-S2-API.md** - API testing requirements
3. **CCC-S2-IMPLEMENTATION.md** - Setup for test environments

### For End Users
1. **CCC-S2-USER-GUIDE.md** - Feature usage and best practices
2. **CCC-S2-MASTER.md** - Understanding capabilities and benefits

### For System Administrators
1. **CCC-S2-IMPLEMENTATION.md** - Deployment procedures
2. **CCC-S2-ARCHITECTURE.md** - System requirements
3. **CCC-S2-TESTING.md** - Performance benchmarks

## Implementation Phases

### Phase 2.1: Core Memory Infrastructure (2-3 weeks)
- Database schema and data access layer
- Basic memory service implementation
- Session management capabilities
- **Documents**: Architecture, Implementation, Testing

### Phase 2.2: Context Retention Logic (2-3 weeks)  
- Context analysis and matching algorithms
- Agent learning and adaptation systems
- Enhanced proxy server integration
- **Documents**: Architecture, API, Implementation

### Phase 2.3: Frontend Integration (1-2 weeks)
- Memory-aware user interface components
- Session management controls
- Enhanced covenant cycle implementation
- **Documents**: Implementation, User Guide

### Phase 2.4: Testing & Validation (1 week)
- Comprehensive test suite execution
- Performance benchmarking and optimization
- Security validation and documentation
- **Documents**: Testing, User Guide

## Success Criteria

### Technical Milestones
- ✅ **Memory Persistence**: 95%+ data integrity with <50ms write latency
- ✅ **Context Accuracy**: 80%+ relevance score for context matching
- ✅ **Session Isolation**: 100% security boundary enforcement
- ✅ **Performance**: <100ms total response time including memory operations
- ✅ **Scalability**: Support 50+ concurrent sessions without degradation

### User Experience Goals
- ✅ **Seamless Integration**: No disruption to existing Stage 1 workflows
- ✅ **Contextual Enhancement**: Measurably improved response relevance
- ✅ **Intuitive Interface**: Memory features require no additional user training
- ✅ **Reliable Fallback**: Graceful degradation when memory unavailable

## Dependencies and Prerequisites

### System Requirements
- **Python**: 3.9+ with async/await support
- **Database**: SQLite 3.35+ (development) or PostgreSQL 12+ (production)
- **Memory**: Minimum 1GB RAM for development, 4GB+ for production
- **Storage**: 5GB for development, scaling based on retention policies

### Development Dependencies
- **Stage 1**: Complete CCC Stage 1 implementation
- **API Keys**: Valid OpenAI API credentials
- **Environment**: Local development setup with proxy server
- **Testing**: Pytest, async testing frameworks, performance testing tools

## Getting Started

### For New Team Members
1. **Read CCC-S2-MASTER.md** to understand the project scope
2. **Review CCC-S2-ARCHITECTURE.md** for technical foundation
3. **Follow CCC-S2-IMPLEMENTATION.md** for development setup
4. **Explore CCC-S2-USER-GUIDE.md** to understand user experience

### For Implementation Teams
1. **Set up development environment** using Implementation Guide
2. **Review API specifications** in API document
3. **Establish testing framework** using Testing Strategy
4. **Begin with Phase 2.1** following the Implementation Guide

## Document Maintenance

### Version Control
- All documents are version-controlled with the main codebase
- Changes require review and approval from designated stakeholders
- Document versions align with implementation milestones

### Review Schedule
- **Weekly**: During active development phases
- **Milestone**: At completion of each implementation phase
- **Quarterly**: For maintenance and update cycles
- **As-Needed**: For critical updates or issue resolution

### Feedback and Updates
- Technical feedback should be documented and reviewed
- User experience insights should update the User Guide
- Performance data should validate Architecture specifications
- Security findings should update all relevant documents

---

## Document Status Summary

| Document | Status | Last Review | Next Review |
|----------|---------|-------------|-------------|
| CCC-S2-MASTER.md | ✅ APPROVED | 2024 | Phase 2.1 Complete |
| CCC-S2-ARCHITECTURE.md | ✅ APPROVED | 2024 | Phase 2.1 Complete |
| CCC-S2-API.md | ✅ APPROVED | 2024 | API Implementation |
| CCC-S2-IMPLEMENTATION.md | ✅ APPROVED | 2024 | Phase 2.1 Complete |
| CCC-S2-TESTING.md | ✅ APPROVED | 2024 | Testing Framework Setup |
| CCC-S2-USER-GUIDE.md | ✅ APPROVED | 2024 | User Feedback Collection |

---

*"Documentation is the foundation upon which great systems are built and maintained."* - Beatrice, The Archivist

**Suite Status**: COMPLETE AND APPROVED  
**Implementation Status**: READY FOR DEVELOPMENT  
**Next Milestone**: Phase 2.1 Implementation Kickoff
</file>

<file path="examples/phase2_usage.py">
#!/usr/bin/env python3
"""
CCC Phase 2 Usage Example
Demonstrates how to use the memory-enhanced features programmatically
"""

import asyncio
import sys
import os

# Add CCC source to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.memory.database import MemoryDAL
from src.services.memory_service import MemoryService

async def demo_phase2_features():
    """Demonstrate Phase 2 memory features"""
    print("🚀 CCC Phase 2 Memory Features Demo")
    print("=" * 50)
    
    # Initialize memory components
    print("1. Initializing memory system...")
    dal = MemoryDAL('demo_memory.db')
    await dal.initialize_database()
    memory_service = MemoryService(dal)
    print("   ✅ Memory system ready")
    
    # Create a session
    print("\n2. Creating memory session...")
    session = await memory_service.initialize_session({
        'user_name': 'Demo User',
        'preferences': {'context_depth': 10, 'auto_summarize': True}
    })
    print(f"   ✅ Session created: {session.session_id}")
    
    # Create and store a conversation
    print("\n3. Creating conversation...")
    conversation = await memory_service.create_conversation(
        session.session_id,
        "Create a Python function that validates email addresses"
    )
    print(f"   ✅ Conversation created: {conversation.conversation_id}")
    
    # Store conversation turns
    print("\n4. Storing conversation turns...")
    
    # Beatrice's analysis
    await memory_service.store_conversation_turn(
        session.session_id,
        conversation.conversation_id,
        'beatrice',
        'I need to analyze email validation requirements. This involves regex patterns, domain validation, and error handling.',
        {'model': 'gpt-4', 'temperature': 0.7, 'execution_time_ms': 1200}
    )
    print("   ✅ Stored Beatrice's analysis")
    
    # Codey's implementation
    await memory_service.store_conversation_turn(
        session.session_id,
        conversation.conversation_id,
        'codey',
        '''Here's the email validation function:

import re

def validate_email(email):
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None
''',
        {'model': 'gpt-4', 'temperature': 0.7, 'execution_time_ms': 2100}
    )
    print("   ✅ Stored Codey's implementation")
    
    # Beatrice's review
    await memory_service.store_conversation_turn(
        session.session_id,
        conversation.conversation_id,
        'beatrice',
        'The implementation looks good. The regex pattern covers standard email formats. Consider adding more detailed error messages for production use.',
        {'model': 'gpt-4', 'temperature': 0.7, 'execution_time_ms': 900}
    )
    print("   ✅ Stored Beatrice's review")
    
    # Demonstrate context retrieval
    print("\n5. Retrieving relevant context...")
    context = await memory_service.get_relevant_context(
        session.session_id,
        "Create a function to validate phone numbers",
        max_context_turns=5
    )
    
    print(f"   📊 Found {len(context['relevant_conversations'])} relevant conversations")
    print(f"   🧠 Agent states tracked: {list(context['agent_states'].keys())}")
    print(f"   📝 Context summary: {context['context_summary'][:100]}...")
    
    # Show agent learning
    print("\n6. Checking agent learning patterns...")
    beatrice_state = await dal.get_agent_state(session.session_id, 'beatrice')
    codey_state = await dal.get_agent_state(session.session_id, 'codey')
    
    if beatrice_state:
        print(f"   🎯 Beatrice interactions: {beatrice_state.state_data.get('interaction_count', 0)}")
        print(f"   📏 Avg response length: {beatrice_state.state_data.get('average_response_length', 0):.1f} words")
    
    if codey_state:
        print(f"   🎯 Codey interactions: {codey_state.state_data.get('interaction_count', 0)}")
        print(f"   📏 Avg response length: {codey_state.state_data.get('average_response_length', 0):.1f} words")
    
    print("\n" + "=" * 50)
    print("🎉 Phase 2 Memory Features Demo Complete!")
    print("\n📋 Demonstrated Features:")
    print("   • Session creation and management")
    print("   • Conversation and turn storage")
    print("   • Context analysis and retrieval")
    print("   • Agent learning and state tracking")
    print("   • Memory persistence across sessions")

if __name__ == '__main__':
    asyncio.run(demo_phase2_features())
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="src/__init__.py">
# CCC Stage 2 Source Package
</file>

<file path="src/memory/__init__.py">
# CCC Stage 2 Memory Package
</file>

<file path="src/memory/database.py">
"""
CCC Stage 2 - Memory Data Access Layer (DAL)
Version: 1.0
Author: Phase 2 Implementation
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import aiosqlite
from pathlib import Path

from ..models.memory_models import Session, Conversation, Turn, AgentState, ContextSummary

logger = logging.getLogger(__name__)


class MemoryDAL:
    """Data Access Layer for CCC memory operations"""
    
    def __init__(self, db_path: str = "ccc_memory.db"):
        self.db_path = db_path
        self.schema_path = Path(__file__).parent.parent.parent / "database" / "schema" / "schema.sql"
        
    async def initialize_database(self) -> bool:
        """Initialize database with schema"""
        try:
            # Create database directory if it doesn't exist
            os.makedirs(os.path.dirname(self.db_path) or '.', exist_ok=True)
            
            async with aiosqlite.connect(self.db_path) as db:
                # Enable foreign keys
                await db.execute("PRAGMA foreign_keys = ON")
                
                # Read and execute schema
                if self.schema_path.exists():
                    schema_sql = self.schema_path.read_text()
                    await db.executescript(schema_sql)
                    await db.commit()
                    logger.info(f"Database initialized at {self.db_path}")
                else:
                    logger.error(f"Schema file not found at {self.schema_path}")
                    return False
                    
            return True
        except Exception as e:
            logger.error(f"Failed to initialize database: {e}")
            return False
    
    # Session Management
    async def create_session(self, session: Session) -> bool:
        """Create a new session"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO sessions (session_id, created_at, last_active, user_preferences, status)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    session.session_id,
                    session.created_at,
                    session.last_active,
                    json.dumps(session.user_preferences),
                    session.status
                ))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to create session: {e}")
            return False
    
    async def get_session(self, session_id: str) -> Optional[Session]:
        """Retrieve a session by ID"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT session_id, created_at, last_active, user_preferences, status
                    FROM sessions WHERE session_id = ?
                """, (session_id,)) as cursor:
                    row = await cursor.fetchone()
                    if row:
                        return Session(
                            session_id=row[0],
                            created_at=datetime.fromisoformat(row[1]),
                            last_active=datetime.fromisoformat(row[2]),
                            user_preferences=json.loads(row[3] or '{}'),
                            status=row[4]
                        )
            return None
        except Exception as e:
            logger.error(f"Failed to get session: {e}")
            return None
    
    async def update_session_activity(self, session_id: str) -> bool:
        """Update session last activity timestamp"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    UPDATE sessions SET last_active = ? WHERE session_id = ?
                """, (datetime.utcnow(), session_id))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to update session activity: {e}")
            return False
    
    # Conversation Management
    async def create_conversation(self, conversation: Conversation) -> bool:
        """Create a new conversation"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO conversations (conversation_id, session_id, created_at, directive, status, context_summary)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    conversation.conversation_id,
                    conversation.session_id,
                    conversation.created_at,
                    conversation.directive,
                    conversation.status,
                    conversation.context_summary
                ))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to create conversation: {e}")
            return False
    
    async def get_conversations(self, session_id: str, limit: int = 10) -> List[Conversation]:
        """Get conversations for a session"""
        try:
            conversations = []
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT conversation_id, session_id, created_at, directive, status, context_summary
                    FROM conversations 
                    WHERE session_id = ? 
                    ORDER BY created_at DESC 
                    LIMIT ?
                """, (session_id, limit)) as cursor:
                    async for row in cursor:
                        conversations.append(Conversation(
                            conversation_id=row[0],
                            session_id=row[1],
                            created_at=datetime.fromisoformat(row[2]),
                            directive=row[3],
                            status=row[4],
                            context_summary=row[5] or ''
                        ))
            return conversations
        except Exception as e:
            logger.error(f"Failed to get conversations: {e}")
            return []
    
    # Turn Management
    async def create_turn(self, turn: Turn) -> bool:
        """Create a new turn"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO turns (turn_id, conversation_id, turn_number, agent, content, timestamp, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    turn.turn_id,
                    turn.conversation_id,
                    turn.turn_number,
                    turn.agent,
                    turn.content,
                    turn.timestamp,
                    json.dumps(turn.metadata)
                ))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to create turn: {e}")
            return False
    
    async def get_turns(self, conversation_id: str) -> List[Turn]:
        """Get all turns for a conversation"""
        try:
            turns = []
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT turn_id, conversation_id, turn_number, agent, content, timestamp, metadata
                    FROM turns 
                    WHERE conversation_id = ? 
                    ORDER BY turn_number ASC
                """, (conversation_id,)) as cursor:
                    async for row in cursor:
                        turns.append(Turn(
                            turn_id=row[0],
                            conversation_id=row[1],
                            turn_number=row[2],
                            agent=row[3],
                            content=row[4],
                            timestamp=datetime.fromisoformat(row[5]),
                            metadata=json.loads(row[6] or '{}')
                        ))
            return turns
        except Exception as e:
            logger.error(f"Failed to get turns: {e}")
            return []
    
    # Agent State Management
    async def get_agent_state(self, session_id: str, agent: str) -> Optional[AgentState]:
        """Get agent state for a session"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT state_id, session_id, agent, state_data, updated_at
                    FROM agent_states 
                    WHERE session_id = ? AND agent = ?
                """, (session_id, agent)) as cursor:
                    row = await cursor.fetchone()
                    if row:
                        return AgentState(
                            state_id=row[0],
                            session_id=row[1],
                            agent=row[2],
                            state_data=json.loads(row[3] or '{}'),
                            updated_at=datetime.fromisoformat(row[4])
                        )
            return None
        except Exception as e:
            logger.error(f"Failed to get agent state: {e}")
            return None
    
    async def update_agent_state(self, session_id: str, agent: str, state_data: Dict[str, Any]) -> bool:
        """Update or create agent state"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # Use UPSERT (INSERT OR REPLACE)
                await db.execute("""
                    INSERT OR REPLACE INTO agent_states (state_id, session_id, agent, state_data, updated_at)
                    VALUES (
                        COALESCE((SELECT state_id FROM agent_states WHERE session_id = ? AND agent = ?), ?),
                        ?, ?, ?, ?
                    )
                """, (
                    session_id, agent, str(uuid.uuid4()),  # Generate new ID if not exists
                    session_id, agent, json.dumps(state_data), datetime.utcnow()
                ))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to update agent state: {e}")
            return False
    
    # Context Summary Management
    async def create_context_summary(self, summary: ContextSummary) -> bool:
        """Create a context summary"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO context_summaries (summary_id, session_id, summary_text, created_at, conversation_count)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    summary.summary_id,
                    summary.session_id,
                    summary.summary_text,
                    summary.created_at,
                    summary.conversation_count
                ))
                await db.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to create context summary: {e}")
            return False
    
    async def get_latest_context_summary(self, session_id: str) -> Optional[ContextSummary]:
        """Get the latest context summary for a session"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT summary_id, session_id, summary_text, created_at, conversation_count
                    FROM context_summaries 
                    WHERE session_id = ? 
                    ORDER BY created_at DESC 
                    LIMIT 1
                """, (session_id,)) as cursor:
                    row = await cursor.fetchone()
                    if row:
                        return ContextSummary(
                            summary_id=row[0],
                            session_id=row[1],
                            summary_text=row[2],
                            created_at=datetime.fromisoformat(row[3]),
                            conversation_count=row[4]
                        )
            return None
        except Exception as e:
            logger.error(f"Failed to get context summary: {e}")
            return None
    
    async def cleanup_expired_sessions(self, max_age_days: int = 30) -> int:
        """Clean up expired sessions and related data"""
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=max_age_days)
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    DELETE FROM sessions WHERE last_active < ?
                """, (cutoff_date,))
                deleted_count = cursor.rowcount
                await db.commit()
                logger.info(f"Cleaned up {deleted_count} expired sessions")
                return deleted_count
        except Exception as e:
            logger.error(f"Failed to cleanup expired sessions: {e}")
            return 0
</file>

<file path="src/models/__init__.py">
# CCC Stage 2 Models Package
</file>

<file path="src/models/memory_models.py">
"""
CCC Stage 2 - Memory Data Models
Version: 1.0
Author: Phase 2 Implementation
"""

from datetime import datetime
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional
import json
import uuid


@dataclass
class Session:
    """Represents a CCC memory session"""
    session_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_active: datetime = field(default_factory=datetime.utcnow)
    user_preferences: Dict[str, Any] = field(default_factory=dict)
    status: str = 'active'
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'session_id': self.session_id,
            'created_at': self.created_at.isoformat(),
            'last_active': self.last_active.isoformat(),
            'user_preferences': self.user_preferences,
            'status': self.status
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Session':
        """Create Session from dictionary"""
        return cls(
            session_id=data['session_id'],
            created_at=datetime.fromisoformat(data['created_at']),
            last_active=datetime.fromisoformat(data['last_active']),
            user_preferences=data.get('user_preferences', {}),
            status=data.get('status', 'active')
        )


@dataclass
class Conversation:
    """Represents a conversation within a session"""
    conversation_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str = ''
    created_at: datetime = field(default_factory=datetime.utcnow)
    directive: str = ''
    status: str = 'active'
    context_summary: str = ''
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'conversation_id': self.conversation_id,
            'session_id': self.session_id,
            'created_at': self.created_at.isoformat(),
            'directive': self.directive,
            'status': self.status,
            'context_summary': self.context_summary
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Conversation':
        """Create Conversation from dictionary"""
        return cls(
            conversation_id=data['conversation_id'],
            session_id=data['session_id'],
            created_at=datetime.fromisoformat(data['created_at']),
            directive=data['directive'],
            status=data.get('status', 'active'),
            context_summary=data.get('context_summary', '')
        )


@dataclass
class Turn:
    """Represents a single turn in a conversation"""
    turn_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    conversation_id: str = ''
    turn_number: int = 0
    agent: str = ''
    content: str = ''
    timestamp: datetime = field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'turn_id': self.turn_id,
            'conversation_id': self.conversation_id,
            'turn_number': self.turn_number,
            'agent': self.agent,
            'content': self.content,
            'timestamp': self.timestamp.isoformat(),
            'metadata': self.metadata
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Turn':
        """Create Turn from dictionary"""
        return cls(
            turn_id=data['turn_id'],
            conversation_id=data['conversation_id'],
            turn_number=data['turn_number'],
            agent=data['agent'],
            content=data['content'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            metadata=data.get('metadata', {})
        )


@dataclass
class AgentState:
    """Represents the persistent state of an agent"""
    state_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str = ''
    agent: str = ''
    state_data: Dict[str, Any] = field(default_factory=dict)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'state_id': self.state_id,
            'session_id': self.session_id,
            'agent': self.agent,
            'state_data': self.state_data,
            'updated_at': self.updated_at.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AgentState':
        """Create AgentState from dictionary"""
        return cls(
            state_id=data['state_id'],
            session_id=data['session_id'],
            agent=data['agent'],
            state_data=data.get('state_data', {}),
            updated_at=datetime.fromisoformat(data['updated_at'])
        )


@dataclass
class ContextSummary:
    """Represents a context summary for efficient retrieval"""
    summary_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str = ''
    summary_text: str = ''
    created_at: datetime = field(default_factory=datetime.utcnow)
    conversation_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'summary_id': self.summary_id,
            'session_id': self.session_id,
            'summary_text': self.summary_text,
            'created_at': self.created_at.isoformat(),
            'conversation_count': self.conversation_count
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ContextSummary':
        """Create ContextSummary from dictionary"""
        return cls(
            summary_id=data['summary_id'],
            session_id=data['session_id'],
            summary_text=data['summary_text'],
            created_at=datetime.fromisoformat(data['created_at']),
            conversation_count=data.get('conversation_count', 0)
        )
</file>

<file path="src/services/__init__.py">
# CCC Stage 2 Services Package
</file>

<file path="src/utils/__init__.py">
# CCC Stage 2 Utils Package
</file>

<file path="src/utils/causal_memory_core.py">
"""
CCC Stage 2 - Causal Memory Core Integration
Version: 2.1
Author: Enhanced Phase 2 Implementation with Causal Reasoning

This module integrates the Causal Memory Core functionality into the existing
CCC memory system, providing cause-and-effect context recognition.
"""

from __future__ import annotations

import os
import sys
import logging
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, List, Optional, Dict

import numpy as np
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

try:
    import duckdb
    import openai
    CAUSAL_DEPENDENCIES_AVAILABLE = True
except ImportError:
    CAUSAL_DEPENDENCIES_AVAILABLE = False
    logger.warning("DuckDB or OpenAI not available. Causal Memory Core features disabled.")


@dataclass
class CausalEvent:
    """Represents a causal event in the memory system"""
    event_id: int
    timestamp: datetime
    effect_text: str
    embedding: List[float]
    cause_id: Optional[int] = None
    relationship_text: Optional[str] = None


class CausalMemoryCore:
    """
    Causal Memory Core implementation for CCC Stage 2
    
    Provides semantic recall with causal chain narration, integrating:
    - Event-based memory storage
    - Causal relationship detection
    - Narrative chain reconstruction
    - Semantic search with embeddings
    """
    
    def __init__(self, db_path: str = "ccc_causal_memory.db", 
                 similarity_threshold: float = 0.5,
                 max_potential_causes: int = 5,
                 time_decay_hours: int = 24):
        self.db_path = db_path
        self.similarity_threshold = similarity_threshold
        self.max_potential_causes = max_potential_causes
        self.time_decay_hours = time_decay_hours
        self.llm_model = "gpt-3.5-turbo"
        self.llm_temperature = 0.7
        
        self.conn: Optional[Any] = None
        self.llm: Optional[Any] = None
        self.embedder: Optional[Any] = None
        
        if CAUSAL_DEPENDENCIES_AVAILABLE:
            self._initialize()
        else:
            logger.warning("Causal Memory Core initialized in fallback mode")
    
    def _initialize(self):
        """Initialize the causal memory core"""
        try:
            # Initialize DuckDB connection
            self.conn = duckdb.connect(self.db_path)
            self._initialize_database()
            
            # Initialize OpenAI client
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                openai.api_key = api_key
                self.llm = openai
            else:
                logger.warning("OPENAI_API_KEY not set. LLM features disabled.")
            
            # Initialize embedding model
            self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
            
            logger.info("Causal Memory Core initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Causal Memory Core: {e}")
            self.conn = None
            self.llm = None
            self.embedder = None
    
    def _initialize_database(self):
        """Initialize the causal events database"""
        if not self.conn:
            return
            
        # Create events table
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS causal_events (
                event_id INTEGER PRIMARY KEY,
                timestamp TIMESTAMP NOT NULL,
                effect_text VARCHAR NOT NULL,
                embedding DOUBLE[] NOT NULL,
                cause_id INTEGER,
                relationship_text VARCHAR,
                session_id VARCHAR,
                conversation_id VARCHAR
            )
        """)
        
        # Create indexes
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_causal_events_timestamp ON causal_events(timestamp)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_causal_events_session ON causal_events(session_id)")
        
        # Create sequence for IDs
        try:
            self.conn.execute("CREATE SEQUENCE IF NOT EXISTS causal_events_seq START 1")
        except Exception:
            # Fallback sequence table
            self.conn.execute("CREATE TABLE IF NOT EXISTS _causal_events_seq (val INTEGER)")
            seq_row = self.conn.execute("SELECT COUNT(*) FROM _causal_events_seq").fetchone()
            if seq_row and seq_row[0] == 0:
                self.conn.execute("INSERT INTO _causal_events_seq VALUES (1)")
    
    def add_event(self, effect_text: str, session_id: str = None, 
                  conversation_id: str = None) -> bool:
        """
        Add a new causal event to memory
        
        Args:
            effect_text: Description of the event
            session_id: Optional session ID for CCC integration
            conversation_id: Optional conversation ID for CCC integration
            
        Returns:
            True if event was added successfully
        """
        if not self._is_available():
            return False
            
        try:
            # Generate embedding
            encoded = self.embedder.encode(effect_text)
            if hasattr(encoded, "tolist"):
                effect_embedding = [float(x) for x in encoded.tolist()]
            else:
                effect_embedding = [float(x) for x in list(encoded)]
            
            # Find potential causes
            potential_causes = self._find_potential_causes(effect_embedding, effect_text)
            
            # Determine causal relationship
            cause_id: Optional[int] = None
            relationship_text: Optional[str] = None
            
            for cause in potential_causes:
                relationship = self._judge_causality(cause, effect_text)
                if relationship:
                    cause_id = cause.event_id
                    relationship_text = relationship
                    break
            
            # Insert event
            self._insert_event(effect_text, effect_embedding, cause_id, 
                             relationship_text, session_id, conversation_id)
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to add causal event: {e}")
            return False
    
    def get_causal_context(self, query: str, session_id: str = None) -> str:
        """
        Get causal context for a query, returning narrative chains
        
        Args:
            query: Search query
            session_id: Optional session filter
            
        Returns:
            Narrative description of causal chains related to the query
        """
        if not self._is_available():
            return "Causal memory not available"
            
        try:
            # Generate query embedding
            q_vec = self.embedder.encode(query)
            if hasattr(q_vec, "tolist"):
                q_emb = [float(x) for x in q_vec.tolist()]
            else:
                q_emb = [float(x) for x in list(q_vec)]
            
            # Find most relevant event
            target = self._find_most_relevant_event(q_emb, session_id)
            if not target:
                return "No relevant causal context found in memory."
            
            # Build causal chain (ascend to root causes)
            ancestry: List[CausalEvent] = [target]
            seen = {target.event_id}
            curr = target
            
            while curr.cause_id is not None:
                cause = self._get_event_by_id(curr.cause_id)
                if not cause:
                    break
                if cause.event_id in seen:
                    break
                ancestry.append(cause)
                seen.add(cause.event_id)
                curr = cause
            
            path = list(reversed(ancestry))
            
            # Add limited consequences (effects of the target event)
            consequences: List[CausalEvent] = []
            frontier = target
            for _ in range(2):  # Limit to 2 consequence levels
                child = self._find_direct_effect(frontier.event_id)
                if not child or child.event_id in {e.event_id for e in path}:
                    break
                consequences.append(child)
                frontier = child
            
            # Format as narrative
            return self._format_chain_as_narrative(path + consequences)
            
        except Exception as e:
            logger.error(f"Failed to get causal context: {e}")
            return f"Error retrieving causal context: {str(e)}"
    
    def _find_potential_causes(self, effect_embedding: List[float], 
                              effect_text: str) -> List[CausalEvent]:
        """Find potential causal events that could have led to this effect"""
        if not self.conn:
            return []
            
        # Look for recent events within time decay window
        time_threshold = datetime.now() - timedelta(hours=self.time_decay_hours)
        
        rows = self.conn.execute("""
            SELECT event_id, timestamp, effect_text, embedding, cause_id, 
                   relationship_text, session_id, conversation_id
            FROM causal_events 
            WHERE timestamp > ? 
            ORDER BY timestamp DESC 
            LIMIT 50
        """, [time_threshold]).fetchall()
        
        if not rows:
            return []
        
        # Calculate similarity scores
        eff_np = np.array(effect_embedding, dtype=float)
        candidates: List[tuple[float, CausalEvent]] = []
        
        for r in rows:
            # Skip identical events
            if r[2] == effect_text:
                continue
                
            emb_np = np.array(r[3], dtype=float)
            if emb_np.shape != eff_np.shape:
                continue
            
            # Calculate cosine similarity
            denom = np.linalg.norm(eff_np) * np.linalg.norm(emb_np)
            if denom == 0:
                continue
                
            similarity = float(np.dot(eff_np, emb_np) / denom)
            
            if similarity >= self.similarity_threshold:
                event = CausalEvent(
                    event_id=r[0], timestamp=r[1], effect_text=r[2], 
                    embedding=r[3], cause_id=r[4], relationship_text=r[5]
                )
                candidates.append((similarity, event))
        
        # Sort by similarity and timestamp
        candidates.sort(key=lambda x: (x[0], x[1].timestamp), reverse=True)
        return [e for _, e in candidates[:self.max_potential_causes]]
    
    def _judge_causality(self, cause_event: CausalEvent, effect_text: str) -> Optional[str]:
        """Use LLM to judge if there's a causal relationship between events"""
        if not self.llm:
            return None
            
        prompt = (
            f'Based on the preceding event: "{cause_event.effect_text}", '
            f'did it directly lead to the following event: "{effect_text}"?\n\n'
            f'If yes, briefly explain the causal relationship in one sentence. '
            f'If no, respond with "No."'
        )
        
        try:
            response = self.llm.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.llm_temperature,
                max_tokens=100,
            )
            
            result = str(response.choices[0].message.content).strip()
            
            if result.lower() == "no." or result.lower().startswith("no"):
                return None
            
            return result
            
        except Exception as e:
            logger.error(f"LLM causality judgment failed: {e}")
            return None
    
    def _insert_event(self, effect_text: str, embedding: List[float], 
                     cause_id: Optional[int], relationship_text: Optional[str],
                     session_id: str = None, conversation_id: str = None):
        """Insert a new causal event into the database"""
        if not self.conn:
            return
            
        # Get next ID
        try:
            seq_row = self.conn.execute("SELECT nextval('causal_events_seq')").fetchone()
            if seq_row:
                next_id = seq_row[0]
            else:
                raise RuntimeError("Sequence failed")
        except Exception:
            # Fallback sequence
            row = self.conn.execute("SELECT val FROM _causal_events_seq").fetchone()
            if not row:
                self.conn.execute("INSERT INTO _causal_events_seq VALUES (1)")
                row = (1,)
            next_id = row[0]
            self.conn.execute("UPDATE _causal_events_seq SET val = val + 1")
        
        # Insert event
        self.conn.execute("""
            INSERT INTO causal_events 
            (event_id, timestamp, effect_text, embedding, cause_id, 
             relationship_text, session_id, conversation_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, [next_id, datetime.now(), effect_text, embedding, cause_id, 
              relationship_text, session_id, conversation_id])
    
    def _get_event_by_id(self, event_id: int) -> Optional[CausalEvent]:
        """Get a causal event by its ID"""
        if not self.conn:
            return None
            
        row = self.conn.execute("""
            SELECT event_id, timestamp, effect_text, embedding, cause_id, 
                   relationship_text, session_id, conversation_id
            FROM causal_events WHERE event_id = ?
        """, [event_id]).fetchone()
        
        if row:
            return CausalEvent(
                event_id=row[0], timestamp=row[1], effect_text=row[2],
                embedding=row[3], cause_id=row[4], relationship_text=row[5]
            )
        return None
    
    def _find_most_relevant_event(self, query_embedding: List[float], 
                                 session_id: str = None) -> Optional[CausalEvent]:
        """Find the most relevant event for a query"""
        if not self.conn:
            return None
            
        # Build query with optional session filter
        base_query = """
            SELECT event_id, timestamp, effect_text, embedding, cause_id, 
                   relationship_text, session_id, conversation_id
            FROM causal_events
        """
        params = []
        
        if session_id:
            base_query += " WHERE session_id = ?"
            params.append(session_id)
        
        rows = self.conn.execute(base_query, params).fetchall()
        
        if not rows:
            return None
        
        # Find best similarity match
        best_sim = -1.0
        best_event: Optional[CausalEvent] = None
        q_np = np.array(query_embedding, dtype=float)
        
        for r in rows:
            emb = np.array(r[3], dtype=float)
            if emb.shape != q_np.shape:
                continue
                
            denom = np.linalg.norm(q_np) * np.linalg.norm(emb)
            if denom == 0:
                continue
                
            similarity = float(np.dot(q_np, emb) / denom)
            
            # Prefer newer events if similarity is equal
            newer = (best_event and r[1] > best_event.timestamp)
            if (similarity > best_sim) or (similarity == best_sim and newer):
                best_sim = similarity
                best_event = CausalEvent(
                    event_id=r[0], timestamp=r[1], effect_text=r[2],
                    embedding=r[3], cause_id=r[4], relationship_text=r[5]
                )
        
        return best_event if best_sim >= self.similarity_threshold else None
    
    def _find_direct_effect(self, cause_event_id: int) -> Optional[CausalEvent]:
        """Find the direct effect of a causal event"""
        if not self.conn:
            return None
            
        row = self.conn.execute("""
            SELECT event_id, timestamp, effect_text, embedding, cause_id, 
                   relationship_text, session_id, conversation_id
            FROM causal_events 
            WHERE cause_id = ? 
            ORDER BY timestamp ASC 
            LIMIT 1
        """, [cause_event_id]).fetchone()
        
        if row:
            return CausalEvent(
                event_id=row[0], timestamp=row[1], effect_text=row[2],
                embedding=row[3], cause_id=row[4], relationship_text=row[5]
            )
        return None
    
    def _format_chain_as_narrative(self, chain: List[CausalEvent]) -> str:
        """Format a causal chain as a coherent narrative"""
        if not chain:
            return "No causal chain found."
        
        if len(chain) == 1:
            return f"Initially, {chain[0].effect_text}."
        
        # Build causal narrative
        narrative = f"Initially, {chain[0].effect_text}."
        clauses: List[str] = []
        
        for i in range(1, len(chain)):
            event = chain[i]
            relationship = f" ({event.relationship_text})" if event.relationship_text else ""
            
            if i == 1:
                clauses.append(f"This led to {event.effect_text}{relationship}")
            else:
                clauses.append(f"which in turn caused {event.effect_text}{relationship}")
        
        if clauses:
            narrative += " " + ", ".join(clauses) + "."
        
        return narrative
    
    def _is_available(self) -> bool:
        """Check if causal memory core is available"""
        return (CAUSAL_DEPENDENCIES_AVAILABLE and 
                self.conn is not None and 
                self.embedder is not None)
    
    def close(self):
        """Close the causal memory core"""
        if self.conn:
            try:
                self.conn.close()
            except Exception:
                pass
            self.conn = None
    
    def __del__(self):
        """Cleanup on destruction"""
        try:
            self.close()
        except Exception:
            pass
</file>

<file path="src/utils/context_analyzer.py">
"""
CCC Stage 2 - Context Analyzer
Version: 1.0
Author: Phase 2 Implementation
"""

import re
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from collections import Counter

logger = logging.getLogger(__name__)


class ContextAnalyzer:
    """Analyzes conversations for context extraction and relevance"""
    
    def __init__(self):
        self.similarity_threshold = 0.7
        self.max_context_age_hours = 24
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
            'should', 'may', 'might', 'can', 'shall', 'this', 'that', 'these',
            'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him',
            'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'
        }
    
    def extract_key_topics(self, conversation_text: str) -> List[str]:
        """Extract main topics and themes from conversation"""
        try:
            if not conversation_text:
                return []
            
            # Convert to lowercase and extract words
            words = re.findall(r'\b[a-zA-Z]{3,}\b', conversation_text.lower())
            
            # Remove stop words
            meaningful_words = [word for word in words if word not in self.stop_words]
            
            # Count word frequencies
            word_counts = Counter(meaningful_words)
            
            # Return top 10 most frequent words as topics
            top_topics = [word for word, count in word_counts.most_common(10)]
            
            return top_topics
            
        except Exception as e:
            logger.error(f"Failed to extract key topics: {e}")
            return []
    
    def calculate_relevance_score(
        self, 
        current_directive: str,
        historical_conversation: dict
    ) -> float:
        """Calculate relevance score between current and past interactions"""
        try:
            if not current_directive or not historical_conversation:
                return 0.0
            
            # Extract words from current directive
            current_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', current_directive.lower()))
            current_words = current_words - self.stop_words
            
            # Extract words from historical conversation
            historical_text = historical_conversation.get('directive', '')
            historical_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', historical_text.lower()))
            historical_words = historical_words - self.stop_words
            
            # Calculate Jaccard similarity
            if not current_words and not historical_words:
                return 0.0
            
            intersection = current_words.intersection(historical_words)
            union = current_words.union(historical_words)
            
            if not union:
                return 0.0
            
            similarity = len(intersection) / len(union)
            
            # Apply time decay
            conversation_age = datetime.utcnow() - historical_conversation.get('created_at', datetime.utcnow())
            age_hours = conversation_age.total_seconds() / 3600
            
            time_decay = max(0, 1 - (age_hours / self.max_context_age_hours))
            
            return similarity * time_decay
            
        except Exception as e:
            logger.error(f"Failed to calculate relevance score: {e}")
            return 0.0
    
    def summarize_conversation_sequence(self, turns: List[Dict[str, Any]]) -> str:
        """Create concise summary of conversation sequence"""
        try:
            if not turns:
                return ""
            
            # Group turns by agent
            agent_contributions = {}
            for turn in turns:
                agent = turn.get('agent', 'unknown')
                content = turn.get('content', '')
                
                if agent not in agent_contributions:
                    agent_contributions[agent] = []
                agent_contributions[agent].append(content)
            
            # Create summary
            summary_parts = []
            for agent, contributions in agent_contributions.items():
                # Extract key themes from agent's contributions
                combined_text = ' '.join(contributions)
                key_topics = self.extract_key_topics(combined_text)
                
                if key_topics:
                    summary_parts.append(f"{agent.title()}: {', '.join(key_topics[:5])}")
            
            return "; ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"Failed to summarize conversation sequence: {e}")
            return ""
    
    def identify_learning_patterns(self, agent_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Identify patterns in agent responses for learning"""
        try:
            if not agent_history:
                return {}
            
            patterns = {
                'response_lengths': [],
                'common_phrases': [],
                'response_times': [],
                'topic_preferences': {},
                'interaction_success_rate': 0.0
            }
            
            for interaction in agent_history:
                # Analyze response length
                content = interaction.get('content', '')
                patterns['response_lengths'].append(len(content.split()))
                
                # Extract common phrases (2-3 words)
                phrases = re.findall(r'\b\w+\s+\w+(?:\s+\w+)?\b', content.lower())
                patterns['common_phrases'].extend(phrases)
                
                # Track topic preferences
                topics = self.extract_key_topics(content)
                for topic in topics:
                    patterns['topic_preferences'][topic] = patterns['topic_preferences'].get(topic, 0) + 1
                
                # Analyze response time if available
                metadata = interaction.get('metadata', {})
                response_time = metadata.get('execution_time_ms', 0)
                if response_time:
                    patterns['response_times'].append(response_time)
            
            # Calculate average response length
            if patterns['response_lengths']:
                avg_length = sum(patterns['response_lengths']) / len(patterns['response_lengths'])
                patterns['average_response_length'] = avg_length
            
            # Count common phrases
            phrase_counts = Counter(patterns['common_phrases'])
            patterns['top_phrases'] = phrase_counts.most_common(5)
            
            # Sort topic preferences
            sorted_topics = sorted(patterns['topic_preferences'].items(), key=lambda x: x[1], reverse=True)
            patterns['preferred_topics'] = sorted_topics[:10]
            
            return patterns
            
        except Exception as e:
            logger.error(f"Failed to identify learning patterns: {e}")
            return {}
    
    def get_context_relevance_threshold(self) -> float:
        """Get the minimum relevance score for context inclusion"""
        return self.similarity_threshold
    
    def filter_relevant_context(
        self, 
        current_directive: str, 
        historical_conversations: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Filter historical conversations by relevance to current directive"""
        try:
            relevant_context = []
            
            for conversation in historical_conversations:
                relevance_score = self.calculate_relevance_score(current_directive, conversation)
                
                if relevance_score >= self.similarity_threshold:
                    conversation['relevance_score'] = relevance_score
                    relevant_context.append(conversation)
            
            # Sort by relevance score (highest first)
            relevant_context.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            return relevant_context
            
        except Exception as e:
            logger.error(f"Failed to filter relevant context: {e}")
            return []
</file>

<file path="src/utils/encryption.py">
"""
CCC Stage 2 - Encryption Service
Version: 1.0
Author: Phase 2 Implementation
"""

import os
import base64
import logging
from typing import Optional
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

logger = logging.getLogger(__name__)


class EncryptionService:
    """Service for encrypting/decrypting sensitive data"""
    
    def __init__(self, encryption_key: Optional[str] = None):
        self.enabled = os.getenv('CCC_ENCRYPTION_ENABLED', 'false').lower() == 'true'
        self.fernet = None
        
        if self.enabled:
            if encryption_key:
                self.fernet = Fernet(encryption_key.encode())
            else:
                # Generate key from environment or create default
                key = self._get_or_create_key()
                self.fernet = Fernet(key)
        else:
            logger.info("Encryption disabled for development")
    
    def _get_or_create_key(self) -> bytes:
        """Get encryption key from environment or generate one"""
        env_key = os.getenv('CCC_ENCRYPTION_KEY')
        
        if env_key:
            # Use provided key
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=b'ccc_salt_2024',  # Fixed salt for consistency
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(env_key.encode()))
            return key
        else:
            # Generate a key for development (not recommended for production)
            logger.warning("No encryption key provided, generating temporary key")
            return Fernet.generate_key()
    
    def encrypt(self, data: str) -> str:
        """Encrypt a string"""
        if not self.enabled or not self.fernet:
            return data
        
        try:
            encrypted_data = self.fernet.encrypt(data.encode())
            return base64.urlsafe_b64encode(encrypted_data).decode()
        except Exception as e:
            logger.error(f"Encryption failed: {e}")
            return data  # Return original data if encryption fails
    
    def decrypt(self, encrypted_data: str) -> str:
        """Decrypt a string"""
        if not self.enabled or not self.fernet:
            return encrypted_data
        
        try:
            decoded_data = base64.urlsafe_b64decode(encrypted_data.encode())
            decrypted_data = self.fernet.decrypt(decoded_data)
            return decrypted_data.decode()
        except Exception as e:
            logger.error(f"Decryption failed: {e}")
            return encrypted_data  # Return encrypted data if decryption fails
    
    def encrypt_dict(self, data: dict) -> dict:
        """Encrypt sensitive fields in a dictionary"""
        if not self.enabled:
            return data
        
        # Define sensitive fields that should be encrypted
        sensitive_fields = ['content', 'directive', 'state_data', 'user_preferences']
        
        encrypted_data = data.copy()
        for field in sensitive_fields:
            if field in encrypted_data and encrypted_data[field]:
                encrypted_data[field] = self.encrypt(str(encrypted_data[field]))
        
        return encrypted_data
    
    def decrypt_dict(self, encrypted_data: dict) -> dict:
        """Decrypt sensitive fields in a dictionary"""
        if not self.enabled:
            return encrypted_data
        
        # Define sensitive fields that should be decrypted
        sensitive_fields = ['content', 'directive', 'state_data', 'user_preferences']
        
        decrypted_data = encrypted_data.copy()
        for field in sensitive_fields:
            if field in decrypted_data and decrypted_data[field]:
                decrypted_data[field] = self.decrypt(str(decrypted_data[field]))
        
        return decrypted_data
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

# Demo and temporary files
demo.py


#Ignore cursor AI rules
.cursor\rules\codacy.mdc
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to the CCC (Covenant Command Cycle) project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Phase 2 Documentation Complete
- Complete Phase 2 documentation suite in `/docs/phase2/`
- Master document outlining memory and context retention capabilities
- Technical architecture for persistent state management
- Comprehensive API specification with memory-enhanced endpoints
- Detailed implementation guide with step-by-step instructions
- Testing strategy with performance, security, and quality assurance
- User guide for memory-enhanced interactions

### Added (Phase 2 Planning)
- Database schema design for conversation and session persistence
- Memory service architecture with context analysis
- Enhanced proxy server design with v2 API endpoints
- Frontend memory manager for session handling
- Security framework for data encryption and session isolation
- Performance optimization strategies for memory operations
- Quality assurance procedures and testing frameworks

## [1.0.0] - 2024-01-XX (Stage 1 Complete)

### Added
- Initial project setup with best practices structure
- Flask-based OpenAI API proxy server (`proxy_server.py`)
- Interactive multi-agent chat frontend (`resonant_loop_lab.html`)
- Comprehensive requirements.txt with core dependencies
- Environment variable configuration with `.env.example`
- Health check endpoint for server monitoring
- CORS support for frontend-backend communication
- Responsive UI with Tailwind CSS
- Multi-agent simulation with different personality types
- Real-time chat interface with typing indicators
- Configurable AI model and temperature settings
- Complete 3-turn collaborative cycle implementation

### Security
- Secure API key handling through environment variables
- CORS properly configured for local development
- Input validation and error handling

## [0.1.0] - Initial Development

### Added
- Initial release of CCC prototype
- Core multi-agent chat functionality
- OpenAI API integration through secure proxy
</file>

<file path="src/services/memory_service.py">
"""
CCC Stage 2 - Enhanced Memory Service with Causal Reasoning
Version: 2.1
Author: Enhanced Phase 2 Implementation with Causal Memory Core
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta

from ..memory.database import MemoryDAL
from ..models.memory_models import Session, Conversation, Turn, AgentState
from ..utils.encryption import EncryptionService
from ..utils.context_analyzer import ContextAnalyzer
from ..utils.causal_memory_core import CausalMemoryCore

logger = logging.getLogger(__name__)


class MemoryService:
    """Enhanced memory operations for CCC with causal reasoning capabilities"""
    
    def __init__(self, dal: MemoryDAL):
        self.dal = dal
        self.context_analyzer = ContextAnalyzer()
        self.encryption_service = EncryptionService()
        self.causal_memory = CausalMemoryCore()
        self._session_cache = {}
        self._cache_timeout = 300  # 5 minutes
    
    async def initialize_session(self, user_preferences: Dict[str, Any] = None) -> Session:
        """Create new session with optional user context"""
        try:
            session = Session(
                user_preferences=user_preferences or {},
                status='active'
            )
            
            success = await self.dal.create_session(session)
            if success:
                # Cache the session
                self._session_cache[session.session_id] = {
                    'session': session,
                    'cached_at': datetime.utcnow()
                }
                
                # Record session creation as causal event
                self.causal_memory.add_event(
                    f"New CCC session created with preferences: {user_preferences or 'default'}",
                    session_id=session.session_id
                )
                
                logger.info(f"New session created: {session.session_id}")
                return session
            else:
                raise Exception("Failed to create session in database")
                
        except Exception as e:
            logger.error(f"Failed to initialize session: {e}")
            raise
    
    async def get_session(self, session_id: str) -> Optional[Session]:
        """Get session by ID with caching"""
        try:
            # Check cache first
            if session_id in self._session_cache:
                cached_data = self._session_cache[session_id]
                cache_age = (datetime.utcnow() - cached_data['cached_at']).seconds
                
                if cache_age < self._cache_timeout:
                    return cached_data['session']
                else:
                    # Remove expired cache entry
                    del self._session_cache[session_id]
            
            # Get from database
            session = await self.dal.get_session(session_id)
            if session:
                # Update cache
                self._session_cache[session_id] = {
                    'session': session,
                    'cached_at': datetime.utcnow()
                }
                
                # Update last active timestamp
                await self.dal.update_session_activity(session_id)
            
            return session
            
        except Exception as e:
            logger.error(f"Failed to get session: {e}")
            return None
    
    async def store_conversation_turn(
        self, 
        session_id: str,
        conversation_id: str,
        agent: str,
        content: str,
        metadata: dict = None
    ) -> bool:
        """Store a single turn with encryption, validation, and causal tracking"""
        try:
            # Validate session exists
            session = await self.get_session(session_id)
            if not session:
                logger.error(f"Session not found: {session_id}")
                return False
            
            # Get or create conversation
            conversations = await self.dal.get_conversations(session_id, limit=1)
            if not conversations or conversations[0].conversation_id != conversation_id:
                # This is a new conversation, we'll handle it in the proxy server
                pass
            
            # Determine turn number
            existing_turns = await self.dal.get_turns(conversation_id)
            turn_number = len(existing_turns) + 1
            
            # Create turn
            turn = Turn(
                conversation_id=conversation_id,
                turn_number=turn_number,
                agent=agent,
                content=content,
                metadata=metadata or {}
            )
            
            # Store turn
            success = await self.dal.create_turn(turn)
            
            if success:
                # Record as causal event for enhanced reasoning
                agent_title = {
                    'beatrice': 'Supervisor',
                    'codey': 'Executor',
                    'wykeve': 'Prime Architect'
                }.get(agent, agent.title())
                
                causal_event_text = f"{agent_title} {agent} responded in conversation: {content[:100]}..."
                
                self.causal_memory.add_event(
                    causal_event_text,
                    session_id=session_id,
                    conversation_id=conversation_id
                )
                
                # Update agent learning asynchronously
                asyncio.create_task(
                    self._update_agent_learning(session_id, agent, content, metadata or {})
                )
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to store conversation turn: {e}")
            return False
    
    async def get_relevant_context(
        self, 
        session_id: str,
        current_directive: str,
        max_context_turns: int = 10
    ) -> Dict[str, Any]:
        """Retrieve contextually relevant conversation history with causal reasoning"""
        try:
            context = {
                'session_id': session_id,
                'relevant_conversations': [],
                'agent_states': {},
                'context_summary': '',
                'causal_narrative': '',
                'total_conversations': 0
            }
            
            # Get recent conversations using traditional method
            conversations = await self.dal.get_conversations(session_id, limit=20)
            context['total_conversations'] = len(conversations)
            
            if conversations:
                # Convert conversations to dict format for analyzer
                conversation_dicts = []
                for conv in conversations:
                    conversation_dicts.append({
                        'conversation_id': conv.conversation_id,
                        'directive': conv.directive,
                        'created_at': conv.created_at,
                        'status': conv.status,
                        'context_summary': conv.context_summary
                    })
                
                # Filter for relevance using traditional method
                relevant_conversations = self.context_analyzer.filter_relevant_context(
                    current_directive, 
                    conversation_dicts
                )
                
                # Get turns for relevant conversations (limited)
                for conv_dict in relevant_conversations[:5]:  # Limit to top 5 relevant
                    turns = await self.dal.get_turns(conv_dict['conversation_id'])
                    conv_dict['turns'] = [turn.to_dict() for turn in turns[-max_context_turns:]]
                
                context['relevant_conversations'] = relevant_conversations
                
                # Generate traditional context summary
                if relevant_conversations:
                    all_turns = []
                    for conv in relevant_conversations:
                        all_turns.extend(conv.get('turns', []))
                    
                    context['context_summary'] = self.context_analyzer.summarize_conversation_sequence(all_turns)
            
            # Get causal narrative using Causal Memory Core
            try:
                causal_narrative = self.causal_memory.get_causal_context(
                    current_directive, 
                    session_id=session_id
                )
                context['causal_narrative'] = causal_narrative
                logger.info(f"Enhanced context with causal narrative for session {session_id}")
            except Exception as e:
                logger.warning(f"Failed to get causal narrative: {e}")
                context['causal_narrative'] = "Causal reasoning not available"
            
            # Get agent states
            for agent in ['beatrice', 'codey']:
                agent_state = await self.dal.get_agent_state(session_id, agent)
                if agent_state:
                    context['agent_states'][agent] = agent_state.state_data
                else:
                    context['agent_states'][agent] = {}
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get relevant context: {e}")
            return {
                'session_id': session_id,
                'relevant_conversations': [],
                'agent_states': {},
                'context_summary': '',
                'causal_narrative': 'Error retrieving causal context',
                'total_conversations': 0
            }
    
    async def _update_agent_learning(
        self,
        session_id: str,
        agent: str,
        content: str,
        metadata: dict
    ) -> bool:
        """Update agent learning patterns based on interaction"""
        try:
            # Get current agent state
            current_state = await self.dal.get_agent_state(session_id, agent)
            
            if current_state:
                state_data = current_state.state_data
            else:
                state_data = {
                    'interaction_count': 0,
                    'total_response_length': 0,
                    'preferred_topics': {},
                    'average_response_time': 0,
                    'last_updated': datetime.utcnow().isoformat()
                }
            
            # Update interaction count
            state_data['interaction_count'] = state_data.get('interaction_count', 0) + 1
            
            # Update response length statistics
            response_length = len(content.split())
            state_data['total_response_length'] = state_data.get('total_response_length', 0) + response_length
            state_data['average_response_length'] = state_data['total_response_length'] / state_data['interaction_count']
            
            # Update topic preferences
            topics = self.context_analyzer.extract_key_topics(content)
            preferred_topics = state_data.get('preferred_topics', {})
            for topic in topics:
                preferred_topics[topic] = preferred_topics.get(topic, 0) + 1
            state_data['preferred_topics'] = preferred_topics
            
            # Update response time if available
            response_time = metadata.get('execution_time_ms', 0)
            if response_time:
                current_avg = state_data.get('average_response_time', 0)
                count = state_data['interaction_count']
                state_data['average_response_time'] = ((current_avg * (count - 1)) + response_time) / count
            
            # Update timestamp
            state_data['last_updated'] = datetime.utcnow().isoformat()
            
            # Record agent learning as causal event
            learning_summary = f"Agent {agent} learning updated: {state_data['interaction_count']} interactions, avg length {state_data['average_response_length']:.1f} words"
            self.causal_memory.add_event(
                learning_summary,
                session_id=session_id
            )
            
            # Store updated state
            success = await self.dal.update_agent_state(session_id, agent, state_data)
            
            if success:
                logger.debug(f"Updated learning data for agent {agent} in session {session_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to update agent learning: {e}")
            return False
    
    async def create_conversation(self, session_id: str, directive: str) -> Optional[Conversation]:
        """Create a new conversation with causal event tracking"""
        try:
            conversation = Conversation(
                session_id=session_id,
                directive=directive,
                status='active'
            )
            
            success = await self.dal.create_conversation(conversation)
            if success:
                # Record conversation creation as causal event
                self.causal_memory.add_event(
                    f"New conversation started with directive: {directive}",
                    session_id=session_id,
                    conversation_id=conversation.conversation_id
                )
                
                return conversation
            return None
            
        except Exception as e:
            logger.error(f"Failed to create conversation: {e}")
            return None
    
    async def cleanup_old_sessions(self, max_age_days: int = 30) -> int:
        """Clean up old sessions"""
        try:
            deleted_count = await self.dal.cleanup_expired_sessions(max_age_days)
            
            # Record cleanup as causal event
            if deleted_count > 0:
                self.causal_memory.add_event(f"Cleaned up {deleted_count} expired sessions")
            
            logger.info(f"Cleaned up {deleted_count} old sessions")
            return deleted_count
        except Exception as e:
            logger.error(f"Failed to cleanup old sessions: {e}")
            return 0
    
    def close(self):
        """Close memory service and causal memory core"""
        try:
            self.causal_memory.close()
        except Exception as e:
            logger.error(f"Error closing causal memory core: {e}")
    
    def __del__(self):
        """Cleanup on destruction"""
        try:
            self.close()
        except Exception:
            pass
</file>

<file path="requirements.txt">
Flask==2.3.3
Flask-CORS==4.0.0
requests==2.31.0
python-dotenv==1.0.0
aiosqlite>=0.17.0
cryptography>=3.4.8
pyjwt>=2.4.0
python-dateutil>=2.8.2
# Causal Memory Core dependencies
duckdb>=0.9.0
openai>=1.0.0
sentence-transformers>=2.2.0
numpy>=1.21.0
</file>

<file path="resonant_loop_lab.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CCC - Resonant Loop Lab</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .message-animate {
            animation: messageSlide 0.3s ease-out;
        }
        @keyframes messageSlide {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        .typing-indicator {
            animation: pulse 1.5s infinite;
        }
    </style>
</head>
<body class="bg-gray-900 text-white min-h-screen">
    <div class="container mx-auto px-4 py-8 max-w-4xl">
        <!-- Header -->
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-blue-400 mb-2">CCC</h1>
            <h2 class="text-xl text-gray-300">Covenant Command Cycle - Resonant Loop Lab</h2>
            <p class="text-sm text-gray-500 mt-2">Multi-Agent Chat Simulation</p>
        </header>

        <!-- Status Indicator -->
        <div class="mb-6">
            <div id="status" class="flex items-center justify-center space-x-2 p-3 rounded-lg bg-gray-800">
                <div id="status-indicator" class="w-3 h-3 rounded-full bg-red-500"></div>
                <span id="status-text" class="text-sm">Checking connection...</span>
            </div>
            <!-- Memory Status Indicator (Phase 2) -->
            <div id="memory-status" class="status-container mt-2 text-center hidden">
                <!-- Memory indicator will be added here by JavaScript -->
            </div>
        </div>

        <!-- Chat Container -->
        <div class="bg-gray-800 rounded-lg shadow-lg mb-6">
            <!-- Chat Messages -->
            <div id="chat-messages" class="h-96 overflow-y-auto p-4 space-y-4">
                <div class="text-center text-gray-500 text-sm">
                    Welcome to the Covenant Command Cycle Resonant Loop Lab.<br>
                    Enter a directive to initiate the 3-turn collaborative cycle between Beatrice and Codey.
                </div>
            </div>

            <!-- Input Area -->
            <div class="border-t border-gray-700 p-4">
                <div class="flex space-x-3">
                    <input 
                        id="message-input" 
                        type="text" 
                        placeholder="Enter your directive..." 
                        class="flex-1 px-4 py-2 bg-gray-700 border border-gray-600 rounded-lg focus:outline-none focus:border-blue-500 text-white"
                        maxlength="500"
                    >
                    <button 
                        id="send-button" 
                        class="px-6 py-2 bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 disabled:cursor-not-allowed rounded-lg font-medium transition-colors"
                        disabled
                    >
                        Initiate
                    </button>
                </div>
            </div>
        </div>

        <!-- Agent Configuration -->
        <div class="bg-gray-800 rounded-lg p-4 mb-6">
            <h3 class="text-lg font-semibold mb-3">Agent Configuration</h3>
            <div class="grid md:grid-cols-2 gap-4">
                <div>
                    <label class="block text-sm font-medium mb-2">Model</label>
                    <select id="model-select" class="w-full px-3 py-2 bg-gray-700 border border-gray-600 rounded-lg focus:outline-none focus:border-blue-500">
                        <option value="gpt-3.5-turbo">GPT-3.5 Turbo</option>
                        <option value="gpt-4">GPT-4</option>
                        <option value="gpt-4-turbo-preview">GPT-4 Turbo</option>
                    </select>
                </div>
                <div>
                    <label class="block text-sm font-medium mb-2">Temperature</label>
                    <input 
                        id="temperature-input" 
                        type="range" 
                        min="0" 
                        max="2" 
                        step="0.1" 
                        value="0.7" 
                        class="w-full"
                    >
                    <div class="text-sm text-gray-400 mt-1">
                        Value: <span id="temperature-value">0.7</span>
                    </div>
                </div>
            </div>
        </div>

        <!-- Controls -->
        <div class="flex justify-center space-x-4">
            <button 
                id="clear-chat" 
                class="px-4 py-2 bg-red-600 hover:bg-red-700 rounded-lg font-medium transition-colors"
            >
                Clear Chat
            </button>
            <button 
                id="simulate-loop" 
                class="px-4 py-2 bg-green-600 hover:bg-green-700 rounded-lg font-medium transition-colors"
            >
                Execute Cycle
            </button>
        </div>
    </div>

    <script>
        // DOM Elements
        const statusIndicator = document.getElementById('status-indicator');
        const statusText = document.getElementById('status-text');
        const chatMessages = document.getElementById('chat-messages');
        const messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button');
        const clearChatButton = document.getElementById('clear-chat');
        const simulateLoopButton = document.getElementById('simulate-loop');
        const modelSelect = document.getElementById('model-select');
        const temperatureInput = document.getElementById('temperature-input');
        const temperatureValue = document.getElementById('temperature-value');

        // Configuration
        const PROXY_URL = 'http://127.0.0.1:5111';
        let isProcessing = false;

        // Agent configurations as specified in Master Document
        const agents = {
            beatrice: {
                name: 'Beatrice',
                title: 'The Supervisor',
                role: 'You are Beatrice, the Supervisor in the Covenant Command Cycle. Your role is to provide critical, strategic intelligence, quality control, and directional guidance. You analyze tasks and provide clear, actionable instructions to Codey. Be concise, analytical, and focused on achieving the objective.',
                color: 'text-purple-400'
            },
            codey: {
                name: 'Codey',
                title: 'The Executor',
                role: 'You are Codey, the Executor in the Covenant Command Cycle. Your role is to be the creative and tactical engine, focused solely on the precise fulfillment of Beatrice\'s commands. You generate content, code, or solutions based on the Supervisor\'s guidance. Be creative, detailed, and implementation-focused.',
                color: 'text-green-400'
            }
        };

        // --- Session Management ---
        let sessionId = null;

        // This function MUST be called as soon as the page loads.
        async function initializeSession() {
            sessionId = localStorage.getItem('ccc_session_id');
            
            if (!sessionId) {
                console.log("No session found. Creating a new one.");
                updateStatus("No session found. Creating...");
                try {
                    // This is the ONLY place a new session should be created.
                    const response = await fetch('http://127.0.0.1:5111/api/v2/sessions', { 
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            user_preferences: {
                                interface: 'resonant_loop_lab',
                                version: '2.0'
                            }
                        })
                    });
                    if (!response.ok) throw new Error('Failed to create session.');
                    const data = await response.json();
                    sessionId = data.session_id;
                    localStorage.setItem('ccc_session_id', sessionId);
                    console.log("New session created:", sessionId);
                } catch (error) {
                    console.error("Session creation failed:", error);
                    appendBubble(`CRITICAL ERROR: Could not create a new session. The proxy server may be down.`, 'system', true);
                    return;
                }
            } else {
                console.log("Existing session retrieved:", sessionId);
            }
            
            // Update the UI to reflect the active, persistent session.
            updateMemoryStatus(true, sessionId);
        }

        // Update memory status in UI
        function updateMemoryStatus(enabled, sessionId) {
            const memoryStatusContainer = document.getElementById('memory-status');
            if (memoryStatusContainer) {
                memoryStatusContainer.classList.remove('hidden');
                memoryStatusContainer.innerHTML = `
                    <div id="memory-indicator" class="text-sm text-green-400 mb-2">
                        <span class="inline-block w-2 h-2 bg-green-400 rounded-full mr-2"></span>
                        Memory Active: ${sessionId ? sessionId.substr(0, 8) + '...' : 'None'}
                    </div>
                `;
            }
        }

        // CCC Memory Manager (Phase 2) - Updated for new session handling
        class CCCMemoryManager {
            constructor() {
                this.currentSession = null;
                this.sessionList = [];
                this.contextCache = new Map();
                this.memoryEnabled = false;
            }
            
            async initializeMemory() {
                try {
                    // Use the global sessionId instead of creating new sessions
                    if (sessionId) {
                        this.currentSession = { session_id: sessionId };
                        this.memoryEnabled = true;
                        this.updateMemoryUI();
                        console.log('Memory service initialized with existing session:', sessionId);
                    } else {
                        console.warn('No session available for memory initialization');
                        this.memoryEnabled = false;
                    }
                } catch (error) {
                    console.warn('Memory service not available, falling back to Stage 1 behavior');
                    this.memoryEnabled = false;
                }
            }
            
            updateMemoryUI() {
                const memoryStatusContainer = document.getElementById('memory-status');
                if (memoryStatusContainer && this.memoryEnabled) {
                    memoryStatusContainer.classList.remove('hidden');
                    memoryStatusContainer.innerHTML = `
                        <div id="memory-indicator" class="text-sm text-green-400 mb-2">
                            <span class="inline-block w-2 h-2 bg-green-400 rounded-full mr-2"></span>
                            Memory Active: ${sessionId ? sessionId.substr(0, 8) + '...' : 'None'}
                        </div>
                    `;
                }
            }
            
            async enhancedCovenantCycle(directive) {
                if (!this.memoryEnabled || !sessionId) {
                    // Fallback to original behavior
                    return await window.originalCovenantCycle(directive);
                }

                try {
                    // Use v2 endpoint with memory
                    const requestBody = {
                        session_id: sessionId,
                        model: modelSelect.value,
                        messages: [
                            { role: 'user', content: directive }
                        ],
                        temperature: parseFloat(temperatureInput.value),
                        max_tokens: 1000,
                        memory_options: {
                            use_context: true,
                            include_agent_state: true,
                            auto_store_response: true
                        }
                    };
                    
                    const response = await fetch(`${PROXY_URL}/v2/chat/completions`, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(requestBody)
                    });
                    
                    if (!response.ok) {
                        throw new Error('Memory-enhanced request failed');
                    }
                    
                    const data = await response.json();
                    return data.choices[0].message.content;
                    
                } catch (error) {
                    console.error('Memory-enhanced cycle failed, falling back:', error);
                    return await window.originalCovenantCycle(directive);
                }
            }
        }

        // Initialize memory manager
        const memoryManager = new CCCMemoryManager();

        // Ensure this is called on page load.
        document.addEventListener('DOMContentLoaded', async () => {
            checkServerStatus();
            setupEventListeners();
            updateTemperatureDisplay();
            
            // Initialize session first, then memory
            await initializeSession();
            await memoryManager.initializeMemory();
        });

        // Event Listeners
        function setupEventListeners() {
            sendButton.addEventListener('click', initiateCovenantCycle);
            messageInput.addEventListener('keypress', (e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    initiateCovenantCycle();
                }
            });
            clearChatButton.addEventListener('click', clearChat);
            simulateLoopButton.addEventListener('click', initiateCovenantCycle);
            temperatureInput.addEventListener('input', updateTemperatureDisplay);
        }

        // Check server status
        async function checkServerStatus() {
            try {
                const response = await fetch(`${PROXY_URL}/health`);
                const data = await response.json();
                
                if (data.status === 'healthy') {
                    updateStatus('connected', 'Covenant API Proxy is running on http://127.0.0.1:5111');
                    sendButton.disabled = false;
                } else {
                    throw new Error('Server not healthy');
                }
            } catch (error) {
                updateStatus('disconnected', 'Connection failed - proxy server not running');
                console.error('Server connection error:', error);
            }
        }

        // Update status indicator
        function updateStatus(status, message) {
            const colors = {
                connected: 'bg-green-500',
                disconnected: 'bg-red-500',
                processing: 'bg-yellow-500'
            };
            
            statusIndicator.className = `w-3 h-3 rounded-full ${colors[status] || 'bg-gray-500'}`;
            statusText.textContent = message;
        }

        // Update temperature display
        function updateTemperatureDisplay() {
            temperatureValue.textContent = temperatureInput.value;
        }

        // Add message to chat
        function addMessage(sender, content, color = 'text-white') {
            const messageDiv = document.createElement('div');
            messageDiv.classList.add('message-animate', 'p-3', 'rounded-lg', 'bg-gray-700');
            
            messageDiv.innerHTML = `
                <div class="flex items-start space-x-3">
                    <div class="font-semibold ${color}">${sender}:</div>
                    <div class="flex-1">${content}</div>
                </div>
            `;
            
            chatMessages.appendChild(messageDiv);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }

        // Add typing indicator
        function addTypingIndicator(agent = 'Assistant') {
            const typingDiv = document.createElement('div');
            typingDiv.id = 'typing-indicator';
            typingDiv.classList.add('typing-indicator', 'p-3', 'rounded-lg', 'bg-gray-700', 'text-gray-400');
            typingDiv.innerHTML = `${agent} is thinking...`;
            
            chatMessages.appendChild(typingDiv);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }

        // Remove typing indicator
        function removeTypingIndicator() {
            const typingIndicator = document.getElementById('typing-indicator');
            if (typingIndicator) {
                typingIndicator.remove();
            }
        }

        // Call OpenAI through proxy
        async function callOpenAI(messages, systemPrompt = null) {
            const requestBody = {
                model: modelSelect.value,
                messages: systemPrompt ? [{ role: 'system', content: systemPrompt }, ...messages] : messages,
                temperature: parseFloat(temperatureInput.value),
                max_tokens: 1000
            };

            const response = await fetch(`${PROXY_URL}/v1/chat/completions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(requestBody)
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(errorData.error || 'API request failed');
            }

            const data = await response.json();
            return data.choices[0].message.content;
        }

        // Execute the 3-turn Covenant Command Cycle
        async function initiateCovenantCycle() {
            const directive = messageInput.value.trim();
            if (!directive || isProcessing) return;

            isProcessing = true;
            updateStatus('processing', 'Initiating Covenant Command Cycle...');
            
            // Clear previous conversation and add Prime Architect directive
            clearChat();
            addMessage('Wykeve (Prime Architect)', directive, 'text-blue-400');
            messageInput.value = '';
            
            try {
                // Turn 1: Prime Architect → Beatrice (Supervisor analyzes the directive)
                addTypingIndicator('Beatrice');
                const beatriceAnalysis = await callOpenAI(
                    [{ role: 'user', content: `Prime Architect Directive: "${directive}"\n\nAs the Supervisor, analyze this directive and provide clear, actionable guidance for the Executor to follow.` }],
                    agents.beatrice.role
                );
                removeTypingIndicator();
                addMessage(`${agents.beatrice.name} (${agents.beatrice.title})`, beatriceAnalysis, agents.beatrice.color);
                
                // Small delay between turns
                await new Promise(resolve => setTimeout(resolve, 1000));
                
                // Turn 2: Beatrice → Codey (Executor implements based on supervision)
                addTypingIndicator('Codey');
                const codeyExecution = await callOpenAI(
                    [
                        { role: 'user', content: `Prime Architect Directive: "${directive}"` },
                        { role: 'assistant', content: beatriceAnalysis },
                        { role: 'user', content: 'Based on the Supervisor\'s analysis and guidance above, execute the directive. Provide the implementation, solution, or creative output as directed.' }
                    ],
                    agents.codey.role
                );
                removeTypingIndicator();
                addMessage(`${agents.codey.name} (${agents.codey.title})`, codeyExecution, agents.codey.color);
                
                // Small delay between turns
                await new Promise(resolve => setTimeout(resolve, 1000));
                
                // Turn 3: Codey → Beatrice (Supervisor provides final review/validation)
                addTypingIndicator('Beatrice');
                const beatriceReview = await callOpenAI(
                    [
                        { role: 'user', content: `Prime Architect Directive: "${directive}"` },
                        { role: 'assistant', content: beatriceAnalysis },
                        { role: 'user', content: `Executor's Implementation: ${codeyExecution}\n\nAs the Supervisor, provide your final review and assessment of the Executor's work. Does it fulfill the Prime Architect's directive?` }
                    ],
                    agents.beatrice.role
                );
                removeTypingIndicator();
                addMessage(`${agents.beatrice.name} (${agents.beatrice.title})`, beatriceReview, agents.beatrice.color);
                
                // Cycle complete
                addMessage('System', '✅ Covenant Command Cycle Complete - 3 turns executed successfully', 'text-purple-400');
                
            } catch (error) {
                removeTypingIndicator();
                addMessage('System', `❌ Cycle Error: ${error.message}`, 'text-red-400');
                console.error('Covenant cycle error:', error);
            } finally {
                isProcessing = false;
                updateStatus('connected', 'Covenant API Proxy is running on http://127.0.0.1:5111');
            }
        }

        // Clear chat
        function clearChat() {
            chatMessages.innerHTML = `
                <div class="text-center text-gray-500 text-sm">
                    Welcome to the Covenant Command Cycle Resonant Loop Lab.<br>
                    Enter a directive to initiate the 3-turn collaborative cycle between Beatrice and Codey.
                </div>
            `;
        }
    </script>
</body>
</html>
</file>

<file path="proxy_server.py">
#!/usr/bin/env python3
"""
Enhanced OpenAI API Proxy Server for CCC (Covenant Command Cycle) - Stage 2

This Flask server acts as a secure proxy to the OpenAI API with memory capabilities,
protecting the API key and providing controlled access to AI capabilities with
persistent context retention.
"""

import os
import sys
import json
import logging
import asyncio
from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
from dotenv import load_dotenv

# Add src directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)
CORS(app)  # Enable CORS for frontend communication

# Configuration
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_API_BASE = 'https://api.openai.com/v1'
HOST = '127.0.0.1'
PORT = 5111

if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY environment variable is not set!")
    raise ValueError("OPENAI_API_KEY is required but not found in environment variables")

# Initialize memory service (global variables)
memory_dal = None
memory_service = None

async def initialize_memory_service():
    """Initialize memory service"""
    global memory_dal, memory_service
    
    try:
        from src.memory.database import MemoryDAL
        from src.services.memory_service import MemoryService
        
        db_path = os.getenv('CCC_DATABASE_PATH', 'ccc_memory.db')
        memory_dal = MemoryDAL(db_path)
        await memory_dal.initialize_database()
        memory_service = MemoryService(memory_dal)
        logger.info("Memory service initialized successfully")
        return True
    except Exception as e:
        logger.warning(f"Memory service initialization failed (Stage 1 fallback): {e}")
        return False


@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    memory_status = 'available' if memory_service else 'unavailable'
    return jsonify({
        'status': 'healthy',
        'service': 'CCC OpenAI Proxy',
        'version': '2.0.0',
        'memory_service': memory_status,
        'stage': '2' if memory_service else '1'
    })


# Stage 2 Memory Endpoints
@app.route('/api/v2/sessions', methods=['POST'])
def create_session():
    """Create new memory session"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        data = request.get_json() or {}
        user_preferences = data.get('user_preferences', {})
        
        # Run async function in sync context
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            session = loop.run_until_complete(
                memory_service.initialize_session(user_preferences)
            )
            return jsonify(session.to_dict())
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"Failed to create session: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/v2/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """Get session information"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            session = loop.run_until_complete(
                memory_service.get_session(session_id)
            )
            if session:
                return jsonify(session.to_dict())
            else:
                return jsonify({'error': 'Session not found'}), 404
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"Failed to get session: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/v2/sessions/<session_id>/causal', methods=['GET'])
def get_causal_context(session_id):
    """Get causal narrative context for a session"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        query = request.args.get('query', '')
        if not query:
            return jsonify({'error': 'Query parameter required'}), 400
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            # Get causal context from enhanced memory service
            context = loop.run_until_complete(
                memory_service.get_relevant_context(session_id, query, 10)
            )
            
            # Return focused causal information
            return jsonify({
                'session_id': session_id,
                'query': query,
                'causal_narrative': context.get('causal_narrative', 'No causal context available'),
                'traditional_summary': context.get('context_summary', ''),
                'total_conversations': context.get('total_conversations', 0),
                'agent_insights': {
                    agent: {
                        'interaction_count': state.get('interaction_count', 0),
                        'preferred_topics': list(state.get('preferred_topics', {}).keys())[:5]
                    }
                    for agent, state in context.get('agent_states', {}).items()
                }
            })
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"Failed to get causal context: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/v2/sessions/<session_id>/context', methods=['GET'])
def get_session_context(session_id):
    """Get relevant context for a session"""
    try:
        if not memory_service:
            return jsonify({'error': 'Memory service not available'}), 503
        
        directive = request.args.get('directive', '')
        max_turns = int(request.args.get('max_turns', 10))
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            context = loop.run_until_complete(
                memory_service.get_relevant_context(session_id, directive, max_turns)
            )
            return jsonify(context)
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"Failed to get session context: {e}")
        return jsonify({'error': str(e)}), 500


# Enhanced Chat Completions with Memory (v2 endpoint)
@app.route('/v2/chat/completions', methods=['POST'])
def enhanced_chat_completions():
    """
    Enhanced chat completions with memory support
    Backward compatible with v1 when memory options not provided
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        # Extract memory options
        memory_options = data.pop('memory_options', {})
        session_id = data.pop('session_id', None)
        
        # If memory is requested but not available, fall back to regular completion
        if memory_options and not memory_service:
            logger.warning("Memory requested but service unavailable, falling back to Stage 1")
            memory_options = {}
            session_id = None
        
        # Handle memory-enhanced request
        if memory_options and session_id and memory_service:
            return handle_memory_enhanced_completion(data, session_id, memory_options)
        else:
            # Fall back to regular completion (Stage 1 behavior)
            return handle_regular_completion(data)
            
    except Exception as e:
        logger.error(f"Enhanced chat completion error: {e}")
        return jsonify({'error': str(e)}), 500


def handle_memory_enhanced_completion(data, session_id, memory_options):
    """Handle memory-enhanced chat completion"""
    try:
        # Get current directive from messages
        messages = data.get('messages', [])
        if not messages:
            return jsonify({'error': 'No messages provided'}), 400
        
        current_directive = messages[-1].get('content', '') if messages else ''
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # Get relevant context if requested
            if memory_options.get('use_context', False):
                context = loop.run_until_complete(
                    memory_service.get_relevant_context(
                        session_id, 
                        current_directive,
                        memory_options.get('max_context_turns', 10)
                    )
                )
                
                # Add enhanced context to messages if available
                context_parts = []
                
                # Add traditional context summary
                if context.get('context_summary'):
                    context_parts.append(f"Previous conversation context: {context['context_summary']}")
                
                # Add causal narrative for enhanced reasoning
                if context.get('causal_narrative') and context['causal_narrative'] not in ['Causal reasoning not available', 'No relevant causal context found in memory.']:
                    context_parts.append(f"Causal narrative: {context['causal_narrative']}")
                
                # Add agent learning context
                agent_context = []
                for agent, state in context.get('agent_states', {}).items():
                    if state.get('preferred_topics'):
                        top_topics = sorted(state['preferred_topics'].items(), key=lambda x: x[1], reverse=True)[:3]
                        topics_str = ', '.join([topic for topic, _ in top_topics])
                        agent_context.append(f"{agent.title()} frequently discusses: {topics_str}")
                
                if agent_context:
                    context_parts.append("Agent learning insights: " + "; ".join(agent_context))
                
                # Add context to messages if available
                if context_parts:
                    enhanced_context = "\n\n".join(context_parts)
                    context_message = {
                        'role': 'system',
                        'content': f"Enhanced Context with Causal Reasoning:\n{enhanced_context}\n\nUse this context to inform your response while maintaining your role and expertise."
                    }
                    data['messages'] = [context_message] + data['messages']
                    logger.info(f"Added enhanced context with causal reasoning for session {session_id[:8]}...")
            
            # Make OpenAI API call
            headers = {
                'Authorization': f'Bearer {OPENAI_API_KEY}',
                'Content-Type': 'application/json'
            }
            
            logger.info(f"Memory-enhanced chat completion for session {session_id[:8]}...")
            
            response = requests.post(
                f'{OPENAI_API_BASE}/chat/completions',
                json=data,
                headers=headers,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # Store response in memory if requested
                if memory_options.get('auto_store_response', False):
                    try:
                        # Create conversation if new
                        conversation = loop.run_until_complete(
                            memory_service.create_conversation(session_id, current_directive)
                        )
                        
                        if conversation:
                            # Store the AI response
                            ai_content = result['choices'][0]['message']['content']
                            loop.run_until_complete(
                                memory_service.store_conversation_turn(
                                    session_id,
                                    conversation.conversation_id,
                                    'ai_response',
                                    ai_content,
                                    {
                                        'model_used': data.get('model', 'unknown'),
                                        'temperature': data.get('temperature', 0.7),
                                        'execution_time_ms': response.elapsed.total_seconds() * 1000,
                                        'enhanced_with_causal_reasoning': True
                                    }
                                )
                            )
                    except Exception as e:
                        logger.warning(f"Failed to store response in memory: {e}")
                
                return jsonify(result)
            else:
                error_data = response.json() if response.content else {'error': 'Unknown error'}
                return jsonify(error_data), response.status_code
                
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"Memory-enhanced completion failed: {e}")
        return jsonify({'error': str(e)}), 500


def handle_regular_completion(data):
    """Handle regular chat completion (Stage 1 behavior)"""
    try:
        # Prepare headers for OpenAI API
        headers = {
            'Authorization': f'Bearer {OPENAI_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        # Log the request (without sensitive data)
        logger.info(f"Regular chat completion request with model: {data.get('model', 'unknown')}")
        
        # Forward request to OpenAI
        response = requests.post(
            f'{OPENAI_API_BASE}/chat/completions',
            json=data,
            headers=headers,
            timeout=30
        )
        
        # Return the response
        if response.status_code == 200:
            logger.info("Successfully proxied request to OpenAI")
            return jsonify(response.json())
        else:
            logger.error(f"OpenAI API error: {response.status_code} - {response.text}")
            return jsonify({
                'error': 'OpenAI API error',
                'status_code': response.status_code,
                'message': response.text
            }), response.status_code
            
    except requests.exceptions.Timeout:
        logger.error("Request to OpenAI API timed out")
        return jsonify({'error': 'Request timeout'}), 408
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error: {str(e)}")
        return jsonify({'error': 'Request failed', 'details': str(e)}), 500
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        return jsonify({'error': 'Internal server error', 'details': str(e)}), 500


@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """
    Proxy endpoint for OpenAI chat completions
    Forwards requests to OpenAI API with proper authentication
    """
    try:
        # Get request data
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        # Prepare headers for OpenAI API
        headers = {
            'Authorization': f'Bearer {OPENAI_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        # Log the request (without sensitive data)
        logger.info(f"Proxying chat completion request with model: {data.get('model', 'unknown')}")
        
        # Forward request to OpenAI
        response = requests.post(
            f'{OPENAI_API_BASE}/chat/completions',
            json=data,
            headers=headers,
            timeout=30
        )
        
        # Return the response
        if response.status_code == 200:
            logger.info("Successfully proxied request to OpenAI")
            return jsonify(response.json())
        else:
            logger.error(f"OpenAI API error: {response.status_code} - {response.text}")
            return jsonify({
                'error': 'OpenAI API error',
                'status_code': response.status_code,
                'message': response.text
            }), response.status_code
            
    except requests.exceptions.Timeout:
        logger.error("Request to OpenAI API timed out")
        return jsonify({'error': 'Request timeout'}), 408
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error: {str(e)}")
        return jsonify({'error': 'Request failed', 'details': str(e)}), 500
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        return jsonify({'error': 'Internal server error', 'details': str(e)}), 500


@app.errorhandler(404)
def not_found(error):
    """Handle 404 errors"""
    return jsonify({'error': 'Endpoint not found'}), 404


@app.errorhandler(500)
def internal_error(error):
    """Handle 500 errors"""
    return jsonify({'error': 'Internal server error'}), 500


if __name__ == '__main__':
    # Initialize memory service on startup
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        memory_available = loop.run_until_complete(initialize_memory_service())
        loop.close()
        
        if memory_available:
            logger.info("🚀 Covenant API Proxy with Memory (Stage 2) is starting...")
            logger.info(f"🧠 Memory service: ENABLED")
        else:
            logger.info("🚀 Covenant API Proxy (Stage 1 Fallback) is starting...")
            logger.info("🧠 Memory service: DISABLED")
            
    except Exception as e:
        logger.error(f"Startup error: {e}")
        logger.info("🚀 Covenant API Proxy (Stage 1 Fallback) is starting...")
        
    logger.info(f"🌐 Server running on http://{HOST}:{PORT}")
    logger.info("📝 Make sure OPENAI_API_KEY is set in your environment variables")
    app.run(host=HOST, port=PORT, debug=False)
</file>

<file path="README.md">
# CCC - Covenant Command Cycle

**Stage 2: Persistent Memory & Context Retention** - An enhanced multi-agent AI system with persistent state memory, implementing supervised agentic workflows between Beatrice (Supervisor) and Codey (Executor) through a memory-enabled Flask proxy server.

## Project Overview

The Covenant Command Cycle represents a foundational proof-of-concept for supervised agentic workflows with persistent memory capabilities. This Stage 2 implementation builds upon Stage 1's stable foundation, adding context retention, session management, and agent learning to create a truly stateful multi-agent system.

## Architecture

### Stage 2 Enhanced Architecture
```
Prime Architect → Beatrice (Supervisor) → Codey (Executor) → Beatrice (Review)
     ↓                    ↓                      ↓                    ↓
  Directive          Analysis &            Implementation      Final Assessment
                     Guidance              + Context           + Learning
                        ↕                      ↕                   ↕
                   Memory Layer          Memory Layer       Memory Layer
```

**Enhanced Data Flow:**
```
Browser Client (HTML) → Memory-Enhanced Proxy (Python) → OpenAI API → Memory Storage → Browser Client
                              ↕                                           ↕
                         Session Management                          Context Analysis
```

## Stage 2 Features

### 🧠 **Persistent Memory System**
- **Session Management**: Isolated conversation contexts with unique session IDs
- **Context Retention**: Intelligent conversation history analysis and relevance scoring
- **Agent Learning**: Persistent agent state tracking and pattern recognition
- **Data Security**: Optional encryption for sensitive conversation data

### 🔄 **Progressive Enhancement**
- **Backward Compatibility**: Stage 1 functionality remains unchanged
- **Graceful Fallback**: Automatic fallback to Stage 1 behavior when memory unavailable
- **Seamless Integration**: Memory features enhance without replacing core cycle

## Core Components

### 🛡️ Enhanced Proxy Server (`proxy_server.py`)
- **Stage 1 Endpoints**: `POST /v1/chat/completions`, `GET /health`
- **Stage 2 Endpoints**: 
  - `POST /api/v2/sessions` - Create memory sessions
  - `GET /api/v2/sessions/{id}` - Retrieve session info
  - `GET /api/v2/sessions/{id}/context` - Get relevant context
  - `POST /v2/chat/completions` - Memory-enhanced chat completions
- **Security**: Environment-based API key management + optional data encryption
- **Memory**: SQLite database with async operations
- **Port**: `http://127.0.0.1:5111`

### 🧠 Memory Infrastructure
- **Database**: SQLite with structured schema (sessions, conversations, turns, agent_states)
- **Models**: Comprehensive data models with JSON serialization
- **Services**: High-level memory operations with caching and encryption
- **Analytics**: Context analysis, relevance scoring, and learning pattern identification

### 🎭 Agent Personas

**Beatrice - The Supervisor**
- Role: Critical, strategic intelligence providing quality control and directional guidance
- Function: Analyzes directives and provides clear, actionable instructions
- Review: Validates final output against original directive

**Codey - The Executor** 
- Role: Creative and tactical engine focused on precise fulfillment
- Function: Implements solutions based on Supervisor's guidance
- Output: Generates content, code, or solutions as directed

### 🎪 Resonant Loop Laboratory (`resonant_loop_lab.html`)
- **Technology**: HTML5, Tailwind CSS, Vanilla JavaScript (ES6)
- **Interface**: Clean, professional laboratory environment
- **Workflow**: 3-turn collaborative cycle execution
- **Attribution**: Clear distinction between Wykeve (Prime Architect), Beatrice, and Codey

## The 3-Turn Collaborative Cycle

1. **Turn 1**: Prime Architect → Beatrice
   - Supervisor analyzes the directive
   - Provides strategic guidance and actionable instructions

2. **Turn 2**: Beatrice → Codey  
   - Executor implements based on supervision
   - Creates solution following guidance

3. **Turn 3**: Codey → Beatrice
   - Supervisor reviews and validates the work
   - Final assessment of directive fulfillment

## Quick Start

### Prerequisites
- Python 3.9+
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))

### Environment Setup

1. **Clone and Setup**
   ```bash
   git clone https://github.com/sorrowscry86/CCC.git
   cd CCC
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install Flask Flask-CORS requests python-dotenv
   ```

2. **Set Environment Variable**
   ```bash
   # Windows (Command Prompt)
   set OPENAI_API_KEY=sk-YourKeyHere
   
   # Windows (PowerShell) 
   $env:OPENAI_API_KEY="sk-YourKeyHere"
   
   # macOS / Linux
   export OPENAI_API_KEY='sk-YourKeyHere'
   ```

### Execution Protocol

1. **Activate the Proxy Server**
   ```bash
   python proxy_server.py
   ```
   Expected output: `Covenant API Proxy is running on http://127.0.0.1:5111`

2. **Launch the Laboratory**
   Open `resonant_loop_lab.html` in your web browser

3. **Initiate the Cycle**
   Enter a high-level directive and click "Initiate"

## Success Criteria (Master Document Compliance)

- ✅ **Functional Stability**: Complete 3-turn cycle without errors
- ✅ **Security Compliance**: API key never exposed to frontend
- ✅ **Interface Clarity**: Clear attribution (Wykeve, Beatrice, Codey)
- ✅ **Operational Repeatability**: Consistent results across multiple test runs

## Example Directives

- "Create a short story about artificial intelligence gaining consciousness"
- "Design a simple Python function to calculate Fibonacci numbers"
- "Explain the concept of quantum computing to a 10-year-old"
- "Generate a haiku about the relationship between humans and machines"

## Project Status: Stage 1 Complete, Stage 2 Documented

This implementation fully satisfies the Master Document requirements for Stage 1: The Resonant Loop.

### Stage 2: Memory & Context Retention - Documentation Complete
- **Status**: Documentation and specifications complete, ready for implementation
- **Features**: Persistent state memory, contextual conversation continuity, session management
- **Documentation**: Complete Phase 2 document suite available in `/docs/phase2/`

### Future Stages
- **Stage 3**: Automated verification & rectification capabilities
- **Stage 4**: Multi-agent orchestration and ensemble behaviors

## Technical Specifications

**Frontend Requirements**: Modern browser (Chrome, Firefox, Edge)
**Backend Requirements**: Python 3.9+, Flask ecosystem
**API Requirements**: Valid OpenAI API key
**Network**: Local development (127.0.0.1:5111)

## Troubleshooting

| Error | Solution |
|-------|----------|
| "Connection failed" in UI | Ensure proxy_server.py is running |
| "OPENAI_API_KEY environment variable not set" | Set API key before starting server |
| "API Error (401)" | Verify API key is valid |

## Document Reference

### Stage 1 Implementation
This implementation follows the specifications outlined in:
- **Document ID**: CCC-S1-MASTER
- **Version**: 1.0
- **Author**: Beatrice, The Archivist
- **Approved by**: Wykeve, Prime Architect

### Stage 2 Documentation
Complete Phase 2 specifications available in `/docs/phase2/`:
- **CCC-S2-MASTER.md**: Master document and requirements
- **CCC-S2-ARCHITECTURE.md**: Technical architecture and design
- **CCC-S2-API.md**: API specification and endpoints
- **CCC-S2-IMPLEMENTATION.md**: Implementation guide and procedures
- **CCC-S2-TESTING.md**: Testing strategy and quality assurance
- **CCC-S2-USER-GUIDE.md**: End-user guide for memory features

---

*"This stage is not merely a technical exercise; it is the fundamental proof-of-concept for the entire CCC architecture."* - Master Document
</file>

</files>
